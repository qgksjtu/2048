{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from game2048.game import Game\n",
    "from game2048.displays import Display, IPythonDisplay\n",
    "from game2048.agents import Agent, RandomAgent, ExpectiMaxAgent\n",
    "import json\n",
    "import numpy as np\n",
    "import random,math\n",
    "import keras\n",
    "from keras.models import Sequential,load_model,model_from_json,Input,Model\n",
    "from keras.optimizers import SGD,RMSprop\n",
    "from keras.layers import SimpleRNN,BatchNormalization,Dense, Dropout, Flatten, MaxPooling3D, MaxPooling2D ,Activation ,Concatenate ,Conv3D,Conv2D,concatenate\n",
    "from keras.utils import to_categorical\n",
    "from keras.utils import np_utils\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64.0"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43 87 74 101 89 78 84 129\n",
      "85 172 157 186 171 172 159 228\n",
      "128 260 257 273 260 250 249 311\n",
      "161 353 344 365 324 345 343 430\n",
      "205 430 445 477 371 474 379 528\n",
      "257 495 544 592 412 585 440 621\n",
      "303 585 615 678 489 694 509 701\n",
      "347 670 709 753 563 780 616 762\n",
      "394 756 803 851 635 882 705 824\n",
      "431 835 881 942 717 974 790 908\n",
      "487 907 966 1031 791 1106 884 959\n",
      "530 990 1063 1114 868 1199 968 1037\n",
      "565 1077 1156 1228 930 1276 1068 1132\n",
      "606 1164 1238 1316 1023 1369 1150 1220\n",
      "653 1255 1324 1400 1105 1461 1257 1287\n",
      "688 1337 1406 1490 1198 1552 1341 1356\n",
      "739 1417 1491 1575 1286 1630 1428 1451\n",
      "782 1498 1591 1652 1381 1706 1511 1541\n",
      "829 1600 1654 1747 1460 1782 1625 1622\n",
      "875 1692 1748 1826 1548 1871 1704 1702\n",
      "931 1765 1855 1898 1630 1971 1797 1780\n",
      "968 1863 1915 1994 1726 2036 1890 1860\n",
      "1002 1958 2014 2068 1817 2134 1970 1985\n",
      "1051 2036 2112 2146 1916 2216 2059 2076\n",
      "1098 2135 2183 2244 1998 2313 2132 2166\n",
      "1146 2217 2284 2327 2079 2417 2206 2243\n",
      "1177 2304 2364 2420 2176 2494 2283 2332\n",
      "1219 2393 2473 2482 2256 2583 2368 2412\n",
      "1252 2498 2545 2566 2339 2676 2438 2508\n",
      "1308 2564 2636 2652 2424 2762 2529 2593\n",
      "1349 2658 2702 2752 2516 2827 2642 2651\n",
      "1390 2758 2773 2856 2594 2920 2727 2740\n",
      "1428 2845 2860 2959 2681 3003 2810 2817\n",
      "1467 2937 2935 3076 2749 3095 2894 2909\n",
      "1513 3016 3021 3164 2824 3193 2973 3000\n",
      "1564 3097 3111 3247 2910 3278 3071 3081\n",
      "1609 3203 3166 3341 2992 3389 3126 3172\n",
      "1653 3290 3244 3445 3066 3476 3213 3251\n",
      "1683 3389 3337 3535 3123 3593 3276 3340\n",
      "1727 3479 3426 3625 3197 3674 3365 3427\n",
      "1777 3567 3515 3746 3232 3768 3450 3519\n",
      "1803 3674 3599 3882 3262 3852 3544 3596\n",
      "1847 3757 3686 3971 3350 3925 3641 3683\n",
      "1888 3835 3812 4024 3435 4001 3736 3778\n",
      "1941 3924 3883 4122 3521 4102 3808 3865\n",
      "1972 4011 3969 4198 3609 4186 3898 3951\n",
      "2001 4113 4048 4295 3689 4262 3983 4042\n",
      "2042 4191 4153 4369 3785 4338 4070 4128\n",
      "2079 4269 4242 4369 3785 4338 4070 4128\n",
      "2135 4349 4329 4447 3863 4429 4146 4218\n",
      "2177 4450 4407 4535 3946 4517 4225 4307\n",
      "2223 4538 4493 4621 4037 4599 4312 4400\n",
      "2264 4640 4565 4714 4118 4694 4413 4473\n",
      "2296 4725 4653 4809 4203 4771 4544 4506\n",
      "2336 4817 4725 4911 4284 4854 4624 4601\n",
      "2402 4897 4796 4987 4370 4958 4724 4706\n",
      "2460 4983 4873 5090 4442 5050 4805 4796\n",
      "2516 5052 4955 5189 4529 5122 4890 4887\n",
      "2553 5140 5040 5281 4609 5221 4961 4983\n",
      "2601 5220 5128 5374 4694 5299 5047 5077\n",
      "2648 5301 5225 5461 4810 5347 5137 5157\n",
      "2682 5396 5301 5552 4886 5465 5196 5231\n",
      "2716 5496 5381 5640 4968 5556 5280 5317\n",
      "2752 5602 5441 5721 5064 5637 5365 5404\n",
      "2783 5692 5537 5812 5125 5717 5456 5491\n",
      "2836 5782 5618 5904 5198 5812 5544 5583\n",
      "2883 5868 5704 5984 5291 5890 5629 5672\n",
      "2925 5962 5786 6081 5364 5994 5705 5800\n",
      "2981 6048 5858 6184 5425 6084 5786 5884\n",
      "3020 6140 5933 6281 5496 6167 5873 5970\n",
      "3071 6218 6024 6370 5573 6269 5947 6062\n",
      "3102 6301 6155 6435 5639 6392 5986 6148\n",
      "3159 6381 6236 6522 5731 6497 6043 6232\n",
      "3211 6469 6313 6615 5820 6571 6131 6322\n",
      "3254 6570 6389 6711 5903 6675 6211 6403\n",
      "3300 6646 6468 6797 5986 6760 6296 6483\n",
      "3345 6738 6549 6884 6080 6829 6390 6566\n",
      "3387 6814 6654 6945 6151 6935 6469 6655\n",
      "3427 6916 6740 7028 6238 7029 6581 6750\n",
      "3496 6982 6826 7116 6319 7116 6658 6847\n",
      "3537 7069 6920 7191 6404 7199 6748 6929\n",
      "3580 7178 7007 7269 6482 7292 6829 7020\n",
      "3611 7271 7097 7399 6526 7377 6922 7115\n",
      "3646 7348 7189 7489 6601 7468 7001 7211\n",
      "3691 7438 7291 7565 6677 7560 7082 7299\n",
      "3729 7528 7361 7657 6764 7637 7167 7384\n",
      "3776 7616 7446 7731 6858 7760 7222 7506\n",
      "3826 7689 7540 7835 6924 7853 7321 7599\n",
      "3871 7773 7635 7918 7003 7954 7399 7688\n",
      "3914 7865 7711 8002 7098 8030 7481 7775\n",
      "3967 7941 7794 8093 7185 8107 7573 7864\n",
      "4010 8033 7876 8178 7270 8206 7648 7954\n",
      "4065 8114 7951 8273 7360 8284 7735 8042\n",
      "4104 8205 8025 8369 7448 8368 7841 8104\n",
      "4160 8278 8118 8467 7528 8456 7923 8193\n",
      "4210 8372 8199 8542 7611 8540 8002 8290\n",
      "4254 8467 8278 8642 7690 8631 8091 8375\n",
      "4293 8551 8382 8714 7770 8723 8181 8462\n",
      "4339 8642 8472 8792 7860 8848 8220 8551\n",
      "4391 8711 8555 8884 7931 8932 8305 8651\n",
      "4440 8803 8627 8964 8017 9017 8394 8743\n",
      "4478 8895 8700 9064 8090 9092 8487 8827\n",
      "4525 8975 8802 9138 8185 9171 8565 8920\n",
      "4563 9071 8880 9228 8271 9259 8653 8999\n",
      "4615 9158 8997 9290 8373 9328 8726 9085\n",
      "4661 9247 9083 9368 8448 9424 8806 9169\n",
      "4708 9341 9165 9456 8539 9511 8881 9257\n",
      "4744 9440 9230 9553 8621 9607 8995 9298\n",
      "4793 9526 9326 9629 8704 9691 9076 9390\n",
      "4905 9538 9409 9717 8776 9789 9156 9481\n",
      "4947 9621 9500 9795 8858 9883 9233 9574\n",
      "4981 9730 9586 9888 8925 9974 9363 9623\n",
      "5026 9835 9658 9967 9008 10071 9451 9690\n",
      "5075 9922 9743 10038 9098 10161 9536 9776\n",
      "5123 10010 9840 10122 9202 10231 9632 9865\n",
      "5158 10104 9925 10203 9281 10314 9725 9966\n",
      "5203 10191 10020 10290 9356 10426 9792 10054\n",
      "5256 10272 10115 10370 9439 10520 9873 10145\n",
      "5303 10355 10195 10472 9515 10608 9961 10229\n",
      "5343 10451 10275 10564 9594 10690 10052 10321\n",
      "5384 10548 10338 10655 9681 10768 10143 10404\n",
      "5431 10617 10431 10740 9750 10861 10221 10498\n",
      "5463 10714 10517 10824 9833 10939 10315 10579\n",
      "5526 10789 10594 10915 9915 11033 10393 10668\n",
      "5570 10877 10684 11005 9992 11113 10478 10750\n",
      "5616 10971 10757 11095 10072 11209 10581 10836\n",
      "5646 11069 10831 11192 10150 11302 10662 11012\n",
      "5687 11160 10911 11281 10227 11396 10747 11092\n",
      "5726 11256 11006 11352 10305 11489 10822 11185\n",
      "5759 11358 11091 11440 10378 11579 10912 11275\n",
      "5812 11449 11179 11531 10454 11665 10998 11371\n",
      "5852 11535 11263 11628 10528 11774 11084 11480\n",
      "5895 11653 11334 11701 10611 11860 11166 11571\n",
      "5940 11748 11417 11784 10689 11944 11271 11654\n",
      "5989 11827 11499 11879 10769 12028 11351 11742\n",
      "6029 11916 11584 11973 10850 12114 11429 11825\n",
      "6068 11999 11670 12060 10935 12193 11515 11920\n",
      "6111 12089 11754 12139 11018 12323 11559 12007\n",
      "6163 12178 11833 12224 11096 12409 11631 12098\n",
      "6201 12265 11964 12273 11171 12500 11714 12184\n",
      "6246 12346 12052 12359 11248 12585 11798 12275\n",
      "6288 12442 12171 12421 11319 12674 11877 12358\n",
      "6326 12533 12256 12523 11391 12756 11973 12446\n",
      "6375 12623 12355 12588 11477 12836 12055 12534\n",
      "6404 12717 12439 12694 11552 12918 12147 12619\n",
      "6446 12832 12495 12792 11622 13007 12238 12707\n",
      "6497 12920 12568 12883 11702 13109 12300 12816\n",
      "6539 13005 12656 12960 11789 13216 12374 12909\n",
      "6574 13096 12772 13013 11874 13306 12464 12982\n",
      "6624 13174 12867 13107 11946 13385 12549 13065\n",
      "6677 13254 12951 13221 12009 13468 12632 13164\n",
      "6723 13345 13025 13315 12089 13559 12711 13247\n",
      "6762 13433 13135 13386 12200 13600 12808 13332\n",
      "6787 13531 13217 13467 12280 13678 12902 13418\n",
      "6831 13622 13343 13510 12361 13774 12976 13510\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-38-a49946478736>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     99\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m             \u001b[0mgame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmove\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m         \u001b[0mu\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx1_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx2_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx3_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx4_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx5_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx6_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx7_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx8_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx1_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx2_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx3_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx4_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx5_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx6_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx7_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx8_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/2048-api/game2048/agents.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m         \u001b[0mdirection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msearch_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mboard\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdirection\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/2048-api/game2048/expectimax/_ext.py\u001b[0m in \u001b[0;36mboard_to_move\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    108\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m         \u001b[0mret\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 110\u001b[0;31m     \u001b[0mmove\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mm_to_move\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    111\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmove\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/2048-api/game2048/expectimax/_ext.py\u001b[0m in \u001b[0;36mm_to_move\u001b[0;34m(m)\u001b[0m\n\u001b[1;32m     98\u001b[0m     \u001b[0mmine\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mldru\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m     '''\n\u001b[0;32m--> 100\u001b[0;31m     \u001b[0mmove\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfind_best_move\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    101\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmove\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/2048-api/game2048/expectimax/_ext.py\u001b[0m in \u001b[0;36mfind_best_move\u001b[0;34m(m)\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m         scores = pool.map(score_toplevel_move, [\n\u001b[0;32m---> 83\u001b[0;31m                           (board, move) for move in range(4)])\n\u001b[0m\u001b[1;32m     84\u001b[0m         \u001b[0mbestmove\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbestscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbestscore\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/multiprocessing/pool.py\u001b[0m in \u001b[0;36mmap\u001b[0;34m(self, func, iterable, chunksize)\u001b[0m\n\u001b[1;32m    264\u001b[0m         \u001b[0;32min\u001b[0m \u001b[0ma\u001b[0m \u001b[0mlist\u001b[0m \u001b[0mthat\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mreturned\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m         '''\n\u001b[0;32m--> 266\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_map_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmapstar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunksize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    267\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstarmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunksize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/multiprocessing/pool.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    636\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    637\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 638\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    639\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mready\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    640\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/multiprocessing/pool.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    633\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    634\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 635\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_event\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    636\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    637\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    549\u001b[0m             \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flag\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    550\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 551\u001b[0;31m                 \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cond\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    552\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    553\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    293\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 295\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    296\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if 1:\n",
    "    item=64000\n",
    "    \n",
    "    x1_train=[]\n",
    "    y1_train=[]\n",
    "    \n",
    "    x2_train=[]\n",
    "    y2_train=[]\n",
    "    \n",
    "    x3_train=[]\n",
    "    y3_train=[]\n",
    "    \n",
    "    x4_train=[]\n",
    "    y4_train=[]\n",
    "    \n",
    "    x5_train=[]\n",
    "    y5_train=[]\n",
    "    \n",
    "    x6_train=[]\n",
    "    y6_train=[]\n",
    "    \n",
    "    x7_train=[]\n",
    "    y7_train=[]\n",
    "    \n",
    "    x8_train=[]\n",
    "    y8_train=[]\n",
    "    \n",
    "    while 1:\n",
    "        game = Game(4, score_to_win=2048, random=False)\n",
    "        agent = ExpectiMaxAgent(game, display=display)\n",
    "        \n",
    "        \n",
    "        \n",
    "        while game.end==False:\n",
    "            tmp=[]\n",
    "            tmp=(game.board)\n",
    "            tmp[tmp==0]=1\n",
    "            \n",
    "            \n",
    "            \n",
    "            a=tmp.reshape(16)\n",
    "            b=np.sort(a)\n",
    "            a1=b[-1]\n",
    "            a2=b[-2]\n",
    "            a3=b[-3]\n",
    "            tmp=np.log2(tmp)\n",
    "            tmp=tmp.tolist()\n",
    "            if a1<128:\n",
    "                game.move(agent.step())\n",
    "                continue\n",
    "            elif a1<256:\n",
    "                if len(x1_train)>item:\n",
    "                    game.move(agent.step())\n",
    "                    continue\n",
    "                x1_train.append(tmp)\n",
    "                y1_train.append(agent.step())\n",
    "            elif a1<512:\n",
    "                if len(x2_train)>item:\n",
    "                    game.move(agent.step())\n",
    "                    continue\n",
    "                x2_train.append(tmp)\n",
    "                y2_train.append(agent.step())\n",
    "            elif a1<1024 and a2<256:\n",
    "                if len(x3_train)>item:\n",
    "                    game.move(agent.step())\n",
    "                    continue\n",
    "                x3_train.append(tmp)\n",
    "                y3_train.append(agent.step())\n",
    "            elif a1<1024:\n",
    "                if len(x4_train)>item:\n",
    "                    game.move(agent.step())\n",
    "                    continue\n",
    "                x4_train.append(tmp)\n",
    "                y4_train.append(agent.step())\n",
    "            elif a1<2048 and a2<256:\n",
    "                if len(x5_train)>item:\n",
    "                    game.move(agent.step())\n",
    "                    continue\n",
    "                x5_train.append(tmp)\n",
    "                y5_train.append(agent.step())\n",
    "            elif a1<2048 and a2<512:\n",
    "                if len(x6_train)>item:\n",
    "                    game.move(agent.step())\n",
    "                    continue\n",
    "                x6_train.append(tmp)\n",
    "                y6_train.append(agent.step())\n",
    "            elif a1<2048 and a3<256:\n",
    "                if len(x7_train)>item:\n",
    "                    game.move(agent.step())\n",
    "                    continue\n",
    "                x7_train.append(tmp)\n",
    "                y7_train.append(agent.step())\n",
    "            elif a1<2048:\n",
    "                if len(x8_train)>item:\n",
    "                    game.move(agent.step())\n",
    "                    continue\n",
    "                x8_train.append(tmp)\n",
    "                y8_train.append(agent.step())\n",
    "            else:\n",
    "                break\n",
    "            game.move(agent.step())\n",
    "        u=min(len(x1_train),len(x2_train),len(x3_train),len(x4_train),len(x5_train),len(x6_train),len(x7_train),len(x8_train))\n",
    "        print(len(x1_train),len(x2_train),len(x3_train),len(x4_train),len(x5_train),len(x6_train),len(x7_train),len(x8_train))\n",
    "        if u>=item:\n",
    "            break\n",
    "\n",
    "                \n",
    "                \n",
    "            \n",
    "            \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 1:\n",
    "    x1_train=np.array(x1_train,dtype=float)\n",
    "    y1_train=np.array(y1_train,dtype=float)\n",
    "    x2_train=np.array(x2_train,dtype=float)\n",
    "    y2_train=np.array(y2_train,dtype=float)\n",
    "    x3_train=np.array(x3_train,dtype=float)\n",
    "    y3_train=np.array(y3_train,dtype=float)\n",
    "    x4_train=np.array(x4_train,dtype=float)\n",
    "    y4_train=np.array(y4_train,dtype=float)\n",
    "    x5_train=np.array(x5_train,dtype=float)\n",
    "    y5_train=np.array(y5_train,dtype=float)\n",
    "    x6_train=np.array(x6_train,dtype=float)\n",
    "    y6_train=np.array(y6_train,dtype=float)\n",
    "    x7_train=np.array(x7_train,dtype=float)\n",
    "    y7_train=np.array(y7_train,dtype=float)\n",
    "    x8_train=np.array(x8_train,dtype=float)\n",
    "    y8_train=np.array(y8_train,dtype=float)\n",
    "    \n",
    "    p = np.random.permutation(range(len(x1_train)))\n",
    "    x1_train,y1_train = x1_train[p],y1_train[p]\n",
    "    x1_train=x1_train.astype('float32')\n",
    "    x1_train=to_categorical(x1_train,8)\n",
    "    y1_train=to_categorical(y1_train)\n",
    "    np.save('x_256.npy',x1_train)\n",
    "    np.save('y_256.npy',y1_train)\n",
    "    \n",
    "    p = np.random.permutation(range(len(x2_train)))\n",
    "    x2_train,y2_train = x2_train[p],y2_train[p]\n",
    "    x2_train=x2_train.astype('float32')\n",
    "    x2_train=to_categorical(x2_train,9)\n",
    "    y2_train=to_categorical(y2_train)\n",
    "    np.save('x_512.npy',x2_train)\n",
    "    np.save('y_512.npy',y2_train)\n",
    "    \n",
    "    p = np.random.permutation(range(len(x3_train)))\n",
    "    x3_train,y3_train = x3_train[p],y3_train[p]\n",
    "    x3_train=x3_train.astype('float32')\n",
    "    x3_train=to_categorical(x3_train,10)\n",
    "    y3_train=to_categorical(y3_train)\n",
    "    np.save('x_1024_1.npy',x3_train)\n",
    "    np.save('y_1024_1.npy',y3_train)\n",
    "\n",
    "    p = np.random.permutation(range(len(x4_train)))\n",
    "    x4_train,y4_train = x4_train[p],y4_train[p]\n",
    "    x4_train=x4_train.astype('float32')\n",
    "    x4_train=to_categorical(x4_train,10)\n",
    "    y4_train=to_categorical(y4_train)\n",
    "    np.save('x_1024_2.npy',x4_train)\n",
    "    np.save('y_1024_2.npy',y4_train)\n",
    "    \n",
    "    p = np.random.permutation(range(len(x5_train)))\n",
    "    x5_train,y5_train = x5_train[p],y5_train[p]\n",
    "    x5_train=x5_train.astype('float32')\n",
    "    x5_train=to_categorical(x5_train,11)\n",
    "    y5_train=to_categorical(y5_train)\n",
    "    np.save('x_2048_1.npy',x5_train)\n",
    "    np.save('y_2048_1.npy',y5_train)\n",
    "    \n",
    "    p = np.random.permutation(range(len(x6_train)))\n",
    "    x6_train,y6_train = x6_train[p],y6_train[p]\n",
    "    x6_train=x6_train.astype('float32')\n",
    "    x6_train=to_categorical(x6_train,11)\n",
    "    y6_train=to_categorical(y6_train)\n",
    "    np.save('x_2048_2.npy',x6_train)\n",
    "    np.save('y_2048_2.npy',y6_train)\n",
    "    \n",
    "    p = np.random.permutation(range(len(x7_train)))\n",
    "    x7_train,y7_train = x7_train[p],y7_train[p]\n",
    "    x7_train=x7_train.astype('float32')\n",
    "    x7_train=to_categorical(x7_train,11)\n",
    "    y7_train=to_categorical(y7_train)\n",
    "    np.save('x_2048_3.npy',x7_train)\n",
    "    np.save('y_2048_3.npy',y7_train)\n",
    "\n",
    "    p = np.random.permutation(range(len(x8_train)))\n",
    "    x8_train,y8_train = x8_train[p],y8_train[p]\n",
    "    x8_train=x8_train.astype('float32')\n",
    "    x8_train=to_categorical(x8_train,11)\n",
    "    y8_train=to_categorical(y8_train)\n",
    "    np.save('x_2048_4.npy',x8_train)\n",
    "    np.save('y_2048_4.npy',y8_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def first_train(s,cand):\n",
    "    x_train=np.load('x_'+s+'.npy')\n",
    "    y_train=np.load('y_'+s+'.npy')\n",
    "    display = IPythonDisplay()\n",
    "    model = Sequential()\n",
    "    Filters=256\n",
    "    inputs=Input((4,4,cand))\n",
    "    conv=inputs\n",
    "    conv1=Conv2D(Filters,kernel_size=(4,1),kernel_initializer='he_uniform',padding='Same')(conv)\n",
    "\n",
    "    conv2=Conv2D(Filters,kernel_size=(1,4),kernel_initializer='he_uniform',padding='Same')(conv)\n",
    "\n",
    "    conv3=Conv2D(Filters,kernel_size=(1,1),kernel_initializer='he_uniform',padding='Same')(conv)\n",
    "\n",
    "    conv4=Conv2D(Filters,kernel_size=(2,2),kernel_initializer='he_uniform',padding='Same')(conv)\n",
    "\n",
    "    conv5=Conv2D(Filters,kernel_size=(3,3),kernel_initializer='he_uniform',padding='Same')(conv)\n",
    "\n",
    "    conv6=Conv2D(Filters,kernel_size=(4,4),kernel_initializer='he_uniform',padding='Same')(conv)\n",
    "\n",
    "    hidden=concatenate([Flatten()(conv1),Flatten()(conv2),Flatten()(conv3),Flatten()(conv4),Flatten()(conv5),Flatten()(conv6)])\n",
    "    x=BatchNormalization()(hidden)\n",
    "    x=Activation('relu')(hidden)\n",
    "    x=Dense(2048,kernel_initializer='he_uniform')(x)\n",
    "    x=BatchNormalization()(x)\n",
    "    x=Activation('relu')(x)\n",
    "    x=Dropout(0.2)(x)\n",
    "    x=Dense(512,kernel_initializer='he_uniform')(x)\n",
    "    x=BatchNormalization()(x)\n",
    "    x=Activation('relu')(x)\n",
    "    x=Dropout(0.2)(x)\n",
    "    x=Dense(128,kernel_initializer='he_uniform')(x)\n",
    "    x=BatchNormalization()(x)\n",
    "    x=Activation('relu')(x)\n",
    "    x=Dropout(0.2)(x)\n",
    "\n",
    "    outputs=Dense(4,activation='softmax')(x)\n",
    "    model=Model(inputs,outputs)\n",
    "    model.summary()\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=RMSprop(lr = 0.001, decay=0.0),metrics=['accuracy'])\n",
    "    model.fit(x=x_train,y=y_train,epochs=20,batch_size=512,verbose=1,shuffle=True)\n",
    "    model.save('2048_'+s+'.h5')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def move(model,tm,cand):\n",
    "    \n",
    "    tm=np.array(tm,dtype=float)\n",
    "    tmp0=tm.astype('float32')\n",
    "       \n",
    "    tmp=tmp0\n",
    "        \n",
    "        \n",
    "    tmp=to_categorical(tmp0,cand)\n",
    "        \n",
    "    tmp = tmp.reshape(1,4,4,cand)\n",
    "    tmp_list=[]\n",
    "    tmp_list.append(tmp)\n",
    "    tmp_list=max(model.predict(tmp_list,batch_size=128))\n",
    "    tmp_pre=tmp_list.tolist()\n",
    "    direction=(tmp_pre.index(max(tmp_pre)))\n",
    "    return direction\n",
    "\n",
    "def second_train(s,cand,s1,e1,s2,e2,s3,e3):\n",
    "    model=load_model('2048_'+s+'.h5')\n",
    "    item=6400\n",
    "    m=0\n",
    "    n=0\n",
    "    x_train=[]\n",
    "    y_train=[]\n",
    "    while len(x_train)<item:\n",
    "        \n",
    "        game = Game(4, score_to_win=2048, random=False)\n",
    "        agent = ExpectiMaxAgent(game, display=display)\n",
    "        m=m+1\n",
    "        \n",
    "        \n",
    "        while game.end==False:\n",
    "            tmp=[]\n",
    "            tmp=(game.board)\n",
    "            tmp[tmp==0]=1\n",
    "            \n",
    "            \n",
    "            \n",
    "            a=tmp.reshape(16)\n",
    "            b=np.sort(a)\n",
    "            a1=b[-1]\n",
    "            a2=b[-2]\n",
    "            a3=b[-3]\n",
    "            tmp=np.log2(tmp)\n",
    "            \n",
    "            \n",
    "            if a1>=s1 and a1<e1 and a2>=s2 and a2<e2 and a3>=s3 and a3<e3:\n",
    "                choice=move(model,tmp,cand)\n",
    "                tmp=tmp.tolist()\n",
    "                x_train.append(tmp)\n",
    "                y_train.append(agent.step())\n",
    "                game.move(choice)\n",
    "            elif a1<s1 or (a1>=s1 and a1<e1 and a2<s2) or (a1>=s1 and a1<e1 and a2>=s2 and a2<e2 and a3<s3):\n",
    "                game.move(agent.step())\n",
    "                continue\n",
    "            else:\n",
    "                n=n+1\n",
    "                break\n",
    "    print(n,m)\n",
    "            \n",
    "    x_train=np.array(x_train,dtype=float)\n",
    "    y_train=np.array(y_train,dtype=float)\n",
    "    p = np.random.permutation(range(len(x_train)))\n",
    "    x_train,y_train = x_train[p],y_train[p]\n",
    "    x_train=x_train.astype('float32')\n",
    "    x_train=to_categorical(x_train,cand)\n",
    "    y_train=to_categorical(y_train)\n",
    "    #model.fit(x=x_train,y=y_train,epochs=1,batch_size=6400,verbose=1,shuffle=True)\n",
    "    model.train_on_batch(x=x_train,y=y_train)\n",
    "    model.save('2048_'+s+'.h5')\n",
    "def train_num(s,x_train,y_train,cand,model):\n",
    "    x_train=np.array(x_train,dtype=float)\n",
    "    y_train=np.array(y_train,dtype=float)\n",
    "    p = np.random.permutation(range(len(x_train)))\n",
    "    x_train,y_train = x_train[p],y_train[p]\n",
    "    x_train=x_train.astype('float32')\n",
    "    x_train=to_categorical(x_train,cand)\n",
    "    y_train=to_categorical(y_train)\n",
    "    #model.fit(x=x_train,y=y_train,epochs=1,batch_size=6400,verbose=1,shuffle=True)\n",
    "    model.train_on_batch(x=x_train,y=y_train)\n",
    "    model.save('2048_'+s+'.h5')\n",
    "    return model\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def third_train():\n",
    "    model1 = load_model('2048_128.h5')\n",
    "    model2 = load_model('2048_256.h5')\n",
    "    model3 = load_model('2048_512.h5')\n",
    "    model4 = load_model('2048_1024_1.h5')\n",
    "    model5 = load_model('2048_1024_2.h5')\n",
    "    x1_train=[]\n",
    "    y1_train=[]\n",
    "    \n",
    "    x2_train=[]\n",
    "    y2_train=[]\n",
    "    \n",
    "    x3_train=[]\n",
    "    y3_train=[]\n",
    "    \n",
    "    x4_train=[]\n",
    "    y4_train=[]\n",
    "    \n",
    "    x5_train=[]\n",
    "    y5_train=[]\n",
    "    n1=0\n",
    "    n2=0\n",
    "    n3=0\n",
    "    n4=0\n",
    "    n5=0\n",
    "    n6=0\n",
    "    m=0\n",
    "    while 1:\n",
    "\n",
    "        game = Game(4, score_to_win=2048, random=False)\n",
    "        agent = ExpectiMaxAgent(game, display=display)\n",
    "        m=m+1\n",
    "        while game.end==False:\n",
    "            tmp=[]\n",
    "            tmp=(game.board)\n",
    "            tmp[tmp==0]=1\n",
    "            a=tmp.reshape(16)\n",
    "            b=np.sort(a)\n",
    "            a1=b[-1]\n",
    "            a2=b[-2]\n",
    "            a3=b[-3]\n",
    "            tmp=np.log2(tmp)\n",
    "            tmp=tmp.tolist()\n",
    "            if a1<128:\n",
    "                x1_train.append(tmp)\n",
    "                y1_train.append(agent.step())\n",
    "                choice=move(model1,tmp,7)\n",
    "                \n",
    "                game.move(choice)\n",
    "                continue\n",
    "            elif a1<256:\n",
    "                x2_train.append(tmp)\n",
    "                y2_train.append(agent.step())\n",
    "                choice=move(model2,tmp,8)\n",
    "                \n",
    "                game.move(choice)\n",
    "                continue\n",
    "            elif a1<512:\n",
    "                x3_train.append(tmp)\n",
    "                y3_train.append(agent.step())\n",
    "                choice=move(model3,tmp,9)\n",
    "                \n",
    "                game.move(choice)\n",
    "                continue\n",
    "            elif a1<1024 and a2<256:\n",
    "                x4_train.append(tmp)\n",
    "                y4_train.append(agent.step())\n",
    "                choice=move(model4,tmp,10)\n",
    "               \n",
    "                game.move(choice)\n",
    "                continue\n",
    "            elif a1<1024:\n",
    "                x5_train.append(tmp)\n",
    "                y5_train.append(agent.step())\n",
    "                choice=move(model5,tmp,10)\n",
    "                \n",
    "                game.move(choice)\n",
    "                continue\n",
    "            else:\n",
    "                n6=n6+1\n",
    "                break\n",
    "            game.move(agent.step())\n",
    "        if a1<128:\n",
    "            n1=n1+1\n",
    "        elif a1<256:\n",
    "            n2=n2+1\n",
    "        elif a1<512:\n",
    "            n3=n3+1\n",
    "        elif a2<256:\n",
    "            n4=n4+1\n",
    "        elif a1<1024:\n",
    "            n5=n5+1\n",
    "        if len(x1_train)>2000:\n",
    "            \n",
    "            model1=train_num('128',x1_train,y1_train,7,model1)\n",
    "            x1_train=[]\n",
    "            y1_train=[]\n",
    "            \n",
    "        if len(x2_train)>2000:\n",
    "            \n",
    "            model2=train_num('256',x2_train,y2_train,8,model2)\n",
    "            x2_train=[]\n",
    "            y2_train=[]\n",
    "            \n",
    "        if len(x3_train)>2000:\n",
    "            \n",
    "            model3=train_num('512',x3_train,y3_train,9,model3)\n",
    "            x3_train=[]\n",
    "            y3_train=[]\n",
    "            \n",
    "        if len(x4_train)>2000:\n",
    "           \n",
    "            model4=train_num('1024_1',x4_train,y4_train,10,model4)\n",
    "            x4_train=[]\n",
    "            y4_train=[]\n",
    "            \n",
    "        if len(x5_train)>2000:\n",
    "            \n",
    "            model5=train_num('1024_2',x5_train,y5_train,10,model5)\n",
    "            x5_train=[]\n",
    "            y5_train=[]\n",
    "        if m>25:\n",
    "            if n1*64+n2*128+n3*256+n4*512+n5*512+n6*1024>(m+1)*500:\n",
    "                break\n",
    "            print(n1,n2,n3,n4,n5,n6,m,(n1*64+n2*128+n3*256+n4*512+n5*512+n6*1024)/(m+1))\n",
    "            \n",
    "            n1=0\n",
    "            n2=0\n",
    "            n3=0\n",
    "            n4=0\n",
    "            n5=0\n",
    "            n6=0\n",
    "            m=0\n",
    "        \n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded expectmax lib for 2048: /home/ubuntu/2048-api/game2048/expectimax/bin/2048.so\n",
      "0 6 9 8 3 1 26 360.2962962962963\n",
      "4 6 6 7 3 0 26 284.44444444444446\n",
      "1 4 4 14 3 1 26 419.55555555555554\n",
      "1 1 14 7 3 0 26 329.48148148148147\n",
      "1 8 3 11 3 0 26 334.22222222222223\n",
      "3 1 7 10 5 0 26 362.6666666666667\n",
      "1 3 4 14 4 1 26 433.77777777777777\n",
      "3 1 3 12 7 0 26 400.5925925925926\n",
      "1 4 5 10 6 0 26 372.14814814814815\n",
      "1 4 10 6 5 1 26 362.6666666666667\n",
      "5 4 4 8 5 0 26 315.25925925925924\n",
      "2 2 6 10 6 1 26 412.44444444444446\n",
      "3 6 3 11 3 0 26 329.48148148148147\n",
      "4 3 4 12 3 1 26 384.0\n",
      "1 1 9 8 7 0 26 376.8888888888889\n",
      "1 2 11 7 5 0 26 343.7037037037037\n",
      "2 3 7 11 3 0 26 350.81481481481484\n",
      "0 3 13 8 2 0 26 327.1111111111111\n",
      "3 4 2 10 7 0 26 367.4074074074074\n",
      "3 2 8 10 3 1 26 376.8888888888889\n",
      "0 5 9 7 5 0 26 336.5925925925926\n",
      "0 3 7 10 6 0 26 384.0\n",
      "0 6 5 13 2 0 26 360.2962962962963\n",
      "2 4 9 10 1 0 26 317.6296296296296\n",
      "0 5 10 9 2 0 26 327.1111111111111\n"
     ]
    }
   ],
   "source": [
    "third_train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "516.9230769230769\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'first_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-ea920530cdba>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfirst_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'256'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mfirst_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'512'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m9\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mfirst_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'1024_1'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mfirst_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'1024_2'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mfirst_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'2048_1'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m11\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'first_train' is not defined"
     ]
    }
   ],
   "source": [
    "first_train('256',8)\n",
    "first_train('512',9)\n",
    "first_train('1024_1',10)\n",
    "first_train('1024_2',10)\n",
    "first_train('2048_1',11)\n",
    "first_train('2048_2',11)\n",
    "first_train('2048_3',11)\n",
    "first_train('2048_4',11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "second_train('256',8,128,256,0,256,0,256)\n",
    "second_train('512',9,256,512,0,512,0,512)\n",
    "second_train('1024_1',10,512,1024,0,256,0,256)\n",
    "second_train('1024_2',10,512,1024,256,513,0,512)\n",
    "second_train('2048_1',11,1024,2048,0,256,0,256)\n",
    "second_train('2048_2',11,1024,2048,256,512,0,257)\n",
    "second_train('2048_3',11,1024,2048,512,1024,0,256)\n",
    "second_train('2048_4',11,1024,2048,512,1024,256,513)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded expectmax lib for 2048: /home/ubuntu/2048-api/game2048/expectimax/bin/2048.so\n",
      "85 105\n",
      "Epoch 1/1\n",
      "6402/6402 [==============================] - 7s 1ms/step - loss: 1.0135 - acc: 0.5133\n",
      "81 103\n",
      "Epoch 1/1\n",
      "6442/6442 [==============================] - 7s 1ms/step - loss: 1.0069 - acc: 0.5379\n",
      "81 106\n",
      "Epoch 1/1\n",
      "6406/6406 [==============================] - 8s 1ms/step - loss: 1.0260 - acc: 0.5116\n"
     ]
    }
   ],
   "source": [
    "while 1:\n",
    "    second_train('256',8,128,256,0,256,0,256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 4, 4, 8)      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 4, 4, 256)    8448        input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 4, 4, 256)    8448        input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 4, 4, 256)    2304        input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 4, 4, 256)    8448        input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 4, 4, 256)    18688       input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, 4, 4, 256)    33024       input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 4096)         0           conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "flatten_2 (Flatten)             (None, 4096)         0           conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "flatten_3 (Flatten)             (None, 4096)         0           conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "flatten_4 (Flatten)             (None, 4096)         0           conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "flatten_5 (Flatten)             (None, 4096)         0           conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "flatten_6 (Flatten)             (None, 4096)         0           conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 24576)        0           flatten_1[0][0]                  \n",
      "                                                                 flatten_2[0][0]                  \n",
      "                                                                 flatten_3[0][0]                  \n",
      "                                                                 flatten_4[0][0]                  \n",
      "                                                                 flatten_5[0][0]                  \n",
      "                                                                 flatten_6[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 24576)        0           concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 2048)         50333696    activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 2048)         8192        dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 2048)         0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 2048)         0           activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 512)          1049088     dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 512)          2048        dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 512)          0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 512)          0           activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 128)          65664       dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 128)          512         dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 128)          0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 128)          0           activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 4)            516         dropout_3[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 51,539,076\n",
      "Trainable params: 51,533,700\n",
      "Non-trainable params: 5,376\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/20\n",
      "64001/64001 [==============================] - 41s 643us/step - loss: 1.1514 - acc: 0.4273\n",
      "Epoch 2/20\n",
      "64001/64001 [==============================] - 38s 591us/step - loss: 1.0393 - acc: 0.4794\n",
      "Epoch 3/20\n",
      "64001/64001 [==============================] - 38s 591us/step - loss: 0.9760 - acc: 0.5230\n",
      "Epoch 4/20\n",
      "64001/64001 [==============================] - 38s 592us/step - loss: 0.9193 - acc: 0.5585\n",
      "Epoch 5/20\n",
      "64001/64001 [==============================] - 38s 590us/step - loss: 0.8529 - acc: 0.5991\n",
      "Epoch 6/20\n",
      "64001/64001 [==============================] - 38s 591us/step - loss: 0.7853 - acc: 0.6377\n",
      "Epoch 7/20\n",
      "64001/64001 [==============================] - 38s 590us/step - loss: 0.7229 - acc: 0.6711\n",
      "Epoch 8/20\n",
      "64001/64001 [==============================] - 38s 590us/step - loss: 0.6634 - acc: 0.7009\n",
      "Epoch 9/20\n",
      "64001/64001 [==============================] - 38s 591us/step - loss: 0.6040 - acc: 0.7358\n",
      "Epoch 10/20\n",
      "64001/64001 [==============================] - 38s 591us/step - loss: 0.5453 - acc: 0.7623\n",
      "Epoch 11/20\n",
      "64001/64001 [==============================] - 38s 591us/step - loss: 0.4921 - acc: 0.7893\n",
      "Epoch 12/20\n",
      "64001/64001 [==============================] - 38s 590us/step - loss: 0.4385 - acc: 0.8156\n",
      "Epoch 13/20\n",
      "64001/64001 [==============================] - 38s 591us/step - loss: 0.3909 - acc: 0.8408\n",
      "Epoch 14/20\n",
      "64001/64001 [==============================] - 38s 590us/step - loss: 0.3434 - acc: 0.8611\n",
      "Epoch 15/20\n",
      "64001/64001 [==============================] - 38s 591us/step - loss: 0.3031 - acc: 0.8807\n",
      "Epoch 16/20\n",
      "64001/64001 [==============================] - 38s 589us/step - loss: 0.2699 - acc: 0.8940\n",
      "Epoch 17/20\n",
      "64001/64001 [==============================] - 38s 593us/step - loss: 0.2438 - acc: 0.9054\n",
      "Epoch 18/20\n",
      "64001/64001 [==============================] - 38s 591us/step - loss: 0.2183 - acc: 0.9168\n",
      "Epoch 19/20\n",
      "64001/64001 [==============================] - 38s 590us/step - loss: 0.1977 - acc: 0.9257\n",
      "Epoch 20/20\n",
      "64001/64001 [==============================] - 38s 591us/step - loss: 0.1823 - acc: 0.9333\n"
     ]
    }
   ],
   "source": [
    "first_train('256',8)           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded expectmax lib for 2048: /home/ubuntu/2048-api/game2048/expectimax/bin/2048.so\n",
      "76\n",
      "164\n",
      "250\n",
      "330\n",
      "417\n",
      "508\n",
      "593\n",
      "675\n",
      "761\n",
      "830\n",
      "928\n",
      "1018\n",
      "1101\n",
      "1190\n",
      "1271\n",
      "1347\n",
      "1427\n",
      "1520\n",
      "1624\n",
      "1704\n",
      "1772\n",
      "1847\n",
      "1938\n",
      "2031\n",
      "2146\n",
      "2253\n",
      "2348\n",
      "2423\n",
      "2535\n",
      "2623\n",
      "2715\n",
      "2817\n",
      "2906\n",
      "2990\n",
      "3090\n",
      "3183\n",
      "3280\n",
      "3363\n",
      "3448\n",
      "3530\n",
      "3627\n",
      "3703\n",
      "3798\n",
      "3894\n",
      "3985\n",
      "4071\n",
      "4145\n",
      "4236\n",
      "4326\n",
      "4416\n",
      "4495\n",
      "4582\n",
      "4703\n",
      "4790\n",
      "4897\n",
      "4977\n",
      "5067\n",
      "5157\n",
      "5242\n",
      "5331\n",
      "5405\n",
      "5502\n",
      "5601\n",
      "5693\n",
      "5783\n",
      "5855\n",
      "5944\n",
      "6023\n",
      "6110\n",
      "6214\n",
      "6314\n",
      "6424\n",
      "6518\n",
      "6598\n",
      "6671\n",
      "6761\n",
      "6849\n",
      "6948\n",
      "7038\n",
      "7129\n",
      "7219\n",
      "7300\n",
      "7394\n",
      "7482\n",
      "7562\n",
      "7650\n",
      "7740\n",
      "7829\n",
      "7900\n",
      "7986\n",
      "8074\n",
      "8175\n",
      "8258\n",
      "8341\n",
      "8424\n",
      "8511\n",
      "8583\n",
      "8663\n",
      "8772\n",
      "8855\n",
      "8927\n",
      "9020\n",
      "9108\n",
      "9198\n",
      "9285\n",
      "9378\n",
      "9470\n",
      "9555\n",
      "9636\n",
      "9722\n",
      "9808\n",
      "9891\n",
      "9982\n",
      "10092\n",
      "10178\n",
      "10268\n",
      "10358\n",
      "10451\n",
      "10543\n",
      "10636\n",
      "10720\n",
      "10804\n",
      "10894\n",
      "10976\n",
      "11056\n",
      "11169\n",
      "11251\n",
      "11336\n",
      "11420\n",
      "11513\n",
      "11604\n",
      "11684\n",
      "11766\n",
      "11852\n",
      "11923\n",
      "12027\n",
      "12112\n",
      "12199\n",
      "12293\n",
      "12375\n",
      "12462\n",
      "12550\n",
      "12642\n",
      "12732\n",
      "12828\n",
      "12913\n",
      "12966\n",
      "13051\n",
      "13135\n",
      "13220\n",
      "13306\n",
      "13379\n",
      "13476\n",
      "13571\n",
      "13663\n",
      "13744\n",
      "13849\n",
      "13928\n",
      "14029\n",
      "14108\n",
      "14182\n",
      "14257\n",
      "14356\n",
      "14441\n",
      "14536\n",
      "14616\n",
      "14700\n",
      "14794\n",
      "14880\n",
      "14953\n",
      "15038\n",
      "15129\n",
      "15241\n",
      "15315\n",
      "15407\n",
      "15508\n",
      "15596\n",
      "15676\n",
      "15760\n",
      "15845\n",
      "15938\n",
      "16033\n",
      "16115\n",
      "16179\n",
      "16261\n",
      "16349\n",
      "16450\n",
      "16533\n",
      "16616\n",
      "16696\n",
      "16799\n",
      "16897\n",
      "16993\n",
      "17069\n",
      "17140\n",
      "17214\n",
      "17308\n",
      "17396\n",
      "17480\n",
      "17574\n",
      "17655\n",
      "17736\n",
      "17822\n",
      "17915\n",
      "17992\n",
      "18070\n",
      "18153\n",
      "18228\n",
      "18327\n",
      "18427\n",
      "18513\n",
      "18601\n",
      "18674\n",
      "18745\n",
      "18836\n",
      "18934\n",
      "19013\n",
      "19095\n",
      "19173\n",
      "19253\n",
      "19345\n",
      "19431\n",
      "19533\n",
      "19610\n",
      "19707\n",
      "19820\n",
      "19911\n",
      "19989\n",
      "20069\n",
      "20155\n",
      "20240\n",
      "20331\n",
      "20408\n",
      "20495\n",
      "20584\n",
      "20667\n",
      "20754\n",
      "20845\n",
      "20920\n",
      "21005\n",
      "21094\n",
      "21189\n",
      "21271\n",
      "21355\n",
      "21437\n",
      "21512\n",
      "21592\n",
      "21682\n",
      "21768\n",
      "21859\n",
      "21933\n",
      "22020\n",
      "22101\n",
      "22202\n",
      "22283\n",
      "22389\n",
      "22460\n",
      "22556\n",
      "22642\n",
      "22715\n",
      "22794\n",
      "22892\n",
      "22991\n",
      "23079\n",
      "23158\n",
      "23246\n",
      "23345\n",
      "23430\n",
      "23523\n",
      "23617\n",
      "23727\n",
      "23825\n",
      "23929\n",
      "24011\n",
      "24090\n",
      "24174\n",
      "24252\n",
      "24357\n",
      "24443\n",
      "24541\n",
      "24635\n",
      "24716\n",
      "24804\n",
      "24890\n",
      "24985\n",
      "25070\n",
      "25167\n",
      "25252\n",
      "25332\n",
      "25411\n",
      "25498\n",
      "25583\n",
      "25657\n",
      "25745\n",
      "25835\n",
      "25912\n",
      "26008\n",
      "26125\n",
      "26209\n",
      "26288\n",
      "26373\n",
      "26464\n",
      "26550\n",
      "26644\n",
      "26728\n",
      "26819\n",
      "26908\n",
      "27010\n",
      "27105\n",
      "27200\n",
      "27288\n",
      "27388\n",
      "27486\n",
      "27570\n",
      "27654\n",
      "27758\n",
      "27837\n",
      "27930\n",
      "28013\n",
      "28109\n",
      "28195\n",
      "28291\n",
      "28377\n",
      "28446\n",
      "28532\n",
      "28618\n",
      "28694\n",
      "28790\n",
      "28894\n",
      "28984\n",
      "29063\n",
      "29147\n",
      "29242\n",
      "29323\n",
      "29413\n",
      "29508\n",
      "29597\n",
      "29676\n",
      "29772\n",
      "29863\n",
      "29940\n",
      "30024\n",
      "30098\n",
      "30188\n",
      "30267\n",
      "30360\n",
      "30447\n",
      "30539\n",
      "30648\n",
      "30745\n",
      "30821\n",
      "30904\n",
      "30992\n",
      "31080\n",
      "31168\n",
      "31257\n",
      "31339\n",
      "31430\n",
      "31522\n",
      "31596\n",
      "31693\n",
      "31787\n",
      "31874\n",
      "31942\n",
      "32037\n",
      "32115\n",
      "32206\n",
      "32283\n",
      "32369\n",
      "32442\n",
      "32526\n",
      "32609\n",
      "32714\n",
      "32805\n",
      "32894\n",
      "32996\n",
      "33087\n",
      "33183\n",
      "33268\n",
      "33343\n",
      "33426\n",
      "33518\n",
      "33614\n",
      "33697\n",
      "33782\n",
      "33869\n",
      "33972\n",
      "34048\n",
      "34122\n",
      "34220\n",
      "34296\n",
      "34381\n",
      "34468\n",
      "34555\n",
      "34648\n",
      "34725\n",
      "34810\n",
      "34899\n",
      "34979\n",
      "35063\n",
      "35138\n",
      "35230\n",
      "35300\n",
      "35386\n",
      "35448\n",
      "35526\n",
      "35611\n",
      "35693\n",
      "35784\n",
      "35861\n",
      "35951\n",
      "36035\n",
      "36097\n",
      "36182\n",
      "36264\n",
      "36346\n",
      "36435\n",
      "36520\n",
      "36610\n",
      "36696\n",
      "36792\n",
      "36885\n",
      "36996\n",
      "37086\n",
      "37168\n",
      "37247\n",
      "37339\n",
      "37422\n",
      "37506\n",
      "37588\n",
      "37683\n",
      "37773\n",
      "37860\n",
      "37943\n",
      "38029\n",
      "38117\n",
      "38209\n",
      "38308\n",
      "38393\n",
      "38462\n",
      "38553\n",
      "38641\n",
      "38736\n",
      "38837\n",
      "38931\n",
      "39012\n",
      "39102\n",
      "39181\n",
      "39275\n",
      "39356\n",
      "39449\n",
      "39536\n",
      "39624\n",
      "39712\n",
      "39800\n",
      "39892\n",
      "39976\n",
      "40071\n",
      "40163\n",
      "40256\n",
      "40339\n",
      "40433\n",
      "40521\n",
      "40604\n",
      "40685\n",
      "40774\n",
      "40862\n",
      "40959\n",
      "41033\n",
      "41130\n",
      "41218\n",
      "41330\n",
      "41416\n",
      "41497\n",
      "41592\n",
      "41677\n",
      "41759\n",
      "41850\n",
      "41925\n",
      "42025\n",
      "42127\n",
      "42215\n",
      "42302\n",
      "42377\n",
      "42458\n",
      "42560\n",
      "42652\n",
      "42727\n",
      "42808\n",
      "42891\n",
      "42971\n",
      "43054\n",
      "43134\n",
      "43213\n",
      "43307\n",
      "43399\n",
      "43487\n",
      "43559\n",
      "43638\n",
      "43731\n",
      "43807\n",
      "43884\n",
      "43967\n",
      "44037\n",
      "44128\n",
      "44236\n",
      "44315\n",
      "44402\n",
      "44485\n",
      "44563\n",
      "44656\n",
      "44738\n",
      "44826\n",
      "44903\n",
      "44994\n",
      "45075\n",
      "45176\n",
      "45256\n",
      "45345\n",
      "45421\n",
      "45492\n",
      "45579\n",
      "45681\n",
      "45760\n",
      "45817\n",
      "45905\n",
      "45989\n",
      "46080\n",
      "46164\n",
      "46248\n",
      "46338\n",
      "46423\n",
      "46514\n",
      "46599\n",
      "46686\n",
      "46772\n",
      "46855\n",
      "46936\n",
      "47011\n",
      "47093\n",
      "47176\n",
      "47262\n",
      "47341\n",
      "47418\n",
      "47510\n",
      "47597\n",
      "47698\n",
      "47781\n",
      "47873\n",
      "47967\n",
      "48043\n",
      "48130\n",
      "48221\n",
      "48314\n",
      "48408\n",
      "48492\n",
      "48591\n",
      "48688\n",
      "48767\n",
      "48861\n",
      "48940\n",
      "49031\n",
      "49127\n",
      "49237\n",
      "49325\n",
      "49421\n",
      "49495\n",
      "49566\n",
      "49651\n",
      "49732\n",
      "49811\n",
      "49893\n",
      "49974\n",
      "50067\n",
      "50157\n",
      "50241\n",
      "50335\n",
      "50414\n",
      "50496\n",
      "50588\n",
      "50656\n",
      "50743\n",
      "50827\n",
      "50902\n",
      "50999\n",
      "51074\n",
      "51162\n",
      "51249\n",
      "51347\n",
      "51434\n",
      "51512\n",
      "51608\n",
      "51700\n",
      "51778\n",
      "51876\n",
      "51964\n",
      "52051\n",
      "52133\n",
      "52216\n",
      "52302\n",
      "52385\n",
      "52485\n",
      "52568\n",
      "52662\n",
      "52735\n",
      "52813\n",
      "52907\n",
      "52984\n",
      "53073\n",
      "53182\n",
      "53276\n",
      "53356\n",
      "53432\n",
      "53512\n",
      "53595\n",
      "53661\n",
      "53735\n",
      "53812\n",
      "53895\n",
      "53969\n",
      "54066\n",
      "54176\n",
      "54261\n",
      "54348\n",
      "54432\n",
      "54527\n",
      "54624\n",
      "54710\n",
      "54790\n",
      "54871\n",
      "54966\n",
      "55037\n",
      "55123\n",
      "55194\n",
      "55264\n",
      "55356\n",
      "55453\n",
      "55532\n",
      "55619\n",
      "55706\n",
      "55794\n",
      "55867\n",
      "55950\n",
      "56023\n",
      "56100\n",
      "56195\n",
      "56286\n",
      "56370\n",
      "56471\n",
      "56563\n",
      "56641\n",
      "56750\n",
      "56834\n",
      "56932\n",
      "57023\n",
      "57123\n",
      "57216\n",
      "57297\n",
      "57388\n",
      "57470\n",
      "57561\n",
      "57679\n",
      "57763\n",
      "57858\n",
      "57948\n",
      "58028\n",
      "58115\n",
      "58203\n",
      "58281\n",
      "58362\n",
      "58485\n",
      "58577\n",
      "58666\n",
      "58755\n",
      "58844\n",
      "58929\n",
      "59011\n",
      "59091\n",
      "59184\n",
      "59249\n",
      "59335\n",
      "59411\n",
      "59504\n",
      "59593\n",
      "59684\n",
      "59763\n",
      "59849\n",
      "59937\n",
      "60032\n",
      "60135\n",
      "60224\n",
      "60326\n",
      "60408\n",
      "60496\n",
      "60582\n",
      "60683\n",
      "60784\n",
      "60868\n",
      "60938\n",
      "61025\n",
      "61121\n",
      "61207\n",
      "61301\n",
      "61374\n",
      "61465\n",
      "61549\n",
      "61643\n",
      "61726\n",
      "61814\n",
      "61897\n",
      "61992\n",
      "62065\n",
      "62158\n",
      "62245\n",
      "62331\n",
      "62414\n",
      "62498\n",
      "62585\n",
      "62675\n",
      "62750\n",
      "62836\n",
      "62920\n",
      "63021\n",
      "63117\n",
      "63202\n",
      "63277\n",
      "63364\n",
      "63412\n",
      "63491\n",
      "63574\n",
      "63651\n",
      "63747\n",
      "63832\n",
      "63930\n",
      "64013\n",
      "64098\n",
      "64184\n",
      "64268\n",
      "64360\n",
      "64445\n",
      "64535\n",
      "64627\n",
      "64720\n",
      "64827\n",
      "64930\n",
      "65019\n",
      "65091\n",
      "65180\n",
      "65268\n",
      "65356\n",
      "65451\n",
      "65563\n",
      "65644\n",
      "65742\n",
      "65841\n",
      "65917\n",
      "66023\n",
      "66099\n",
      "66188\n",
      "66286\n",
      "66380\n",
      "66473\n",
      "66575\n",
      "66660\n",
      "66747\n",
      "66835\n",
      "66919\n",
      "67006\n",
      "67086\n",
      "67168\n",
      "67266\n",
      "67343\n",
      "67440\n",
      "67520\n",
      "67608\n",
      "67691\n",
      "67779\n",
      "67872\n",
      "67950\n",
      "68046\n",
      "68151\n",
      "68255\n",
      "68338\n",
      "68429\n",
      "68522\n",
      "68604\n",
      "68693\n",
      "68782\n",
      "68871\n",
      "68966\n",
      "69058\n",
      "69141\n",
      "69217\n",
      "69306\n",
      "69388\n",
      "69466\n",
      "69571\n",
      "69664\n",
      "69767\n",
      "69859\n",
      "69946\n",
      "70032\n",
      "70128\n",
      "70207\n",
      "70301\n",
      "70402\n",
      "70500\n",
      "70588\n",
      "70677\n",
      "70761\n",
      "70851\n",
      "70942\n",
      "71022\n",
      "71104\n",
      "71177\n",
      "71275\n",
      "71366\n",
      "71452\n",
      "71541\n",
      "71622\n",
      "71725\n",
      "71822\n",
      "71925\n",
      "71996\n",
      "72082\n",
      "72166\n",
      "72253\n",
      "72352\n",
      "72423\n",
      "72510\n",
      "72600\n",
      "72685\n",
      "72762\n",
      "72862\n",
      "72946\n",
      "73026\n",
      "73113\n",
      "73191\n",
      "73279\n",
      "73356\n",
      "73448\n",
      "73534\n",
      "73614\n",
      "73701\n",
      "73812\n",
      "73901\n",
      "73985\n",
      "74059\n",
      "74140\n",
      "74239\n",
      "74328\n",
      "74420\n",
      "74502\n",
      "74591\n",
      "74685\n",
      "74779\n",
      "74859\n",
      "74933\n",
      "75020\n",
      "75110\n",
      "75193\n",
      "75275\n",
      "75360\n",
      "75448\n",
      "75529\n",
      "75627\n",
      "75715\n",
      "75795\n",
      "75900\n",
      "75979\n",
      "76072\n",
      "76162\n",
      "76259\n",
      "76351\n",
      "76434\n",
      "76516\n",
      "76613\n",
      "76689\n",
      "76770\n",
      "76867\n",
      "76952\n",
      "77041\n",
      "77110\n",
      "77189\n",
      "77262\n",
      "77358\n",
      "77455\n",
      "77543\n",
      "77630\n",
      "77707\n",
      "77789\n",
      "77875\n",
      "77954\n",
      "78046\n",
      "78145\n",
      "78235\n",
      "78313\n",
      "78395\n",
      "78497\n",
      "78575\n",
      "78634\n",
      "78759\n",
      "78842\n",
      "78941\n",
      "79019\n",
      "79101\n",
      "79190\n",
      "79264\n",
      "79346\n",
      "79431\n",
      "79518\n",
      "79615\n",
      "79724\n",
      "79808\n",
      "79892\n",
      "79992\n",
      "80083\n",
      "80166\n",
      "80263\n",
      "80349\n",
      "80438\n",
      "80532\n",
      "80607\n",
      "80690\n",
      "80782\n",
      "80877\n",
      "80965\n",
      "81064\n",
      "81140\n",
      "81214\n",
      "81305\n",
      "81392\n",
      "81481\n",
      "81569\n",
      "81650\n",
      "81745\n",
      "81841\n",
      "81924\n",
      "82018\n",
      "82103\n",
      "82188\n",
      "82274\n",
      "82358\n",
      "82452\n",
      "82540\n",
      "82623\n",
      "82717\n",
      "82793\n",
      "82876\n",
      "82972\n",
      "83080\n",
      "83158\n",
      "83250\n",
      "83328\n",
      "83421\n",
      "83511\n",
      "83604\n",
      "83690\n",
      "83765\n",
      "83863\n",
      "83946\n",
      "84013\n",
      "84086\n",
      "84182\n",
      "84267\n",
      "84367\n",
      "84449\n",
      "84542\n",
      "84635\n",
      "84721\n",
      "84809\n",
      "84897\n",
      "84988\n",
      "85079\n",
      "85170\n",
      "85267\n",
      "85354\n",
      "85434\n",
      "85521\n",
      "85615\n",
      "85727\n",
      "85805\n",
      "85890\n",
      "85982\n",
      "86067\n",
      "86147\n",
      "86261\n",
      "86328\n",
      "86401\n",
      "86496\n",
      "86579\n",
      "86670\n",
      "86759\n",
      "86856\n",
      "86942\n",
      "87029\n",
      "87118\n",
      "87214\n",
      "87297\n",
      "87372\n",
      "87464\n",
      "87554\n",
      "87642\n",
      "87719\n",
      "87808\n",
      "87900\n",
      "87993\n",
      "88065\n",
      "88152\n",
      "88233\n",
      "88324\n",
      "88438\n",
      "88518\n",
      "88597\n",
      "88673\n",
      "88762\n",
      "88842\n",
      "88914\n",
      "88997\n",
      "89077\n",
      "89175\n",
      "89258\n",
      "89366\n",
      "89443\n",
      "89544\n",
      "89613\n",
      "89699\n",
      "89790\n",
      "89872\n",
      "89969\n",
      "90056\n",
      "90144\n",
      "90225\n",
      "90321\n",
      "90410\n",
      "90478\n",
      "90565\n",
      "90645\n",
      "90718\n",
      "90822\n",
      "90923\n",
      "91006\n",
      "91071\n",
      "91153\n",
      "91251\n",
      "91335\n",
      "91419\n",
      "91504\n",
      "91606\n",
      "91690\n",
      "91779\n",
      "91868\n",
      "91956\n",
      "92039\n",
      "92114\n",
      "92203\n",
      "92286\n",
      "92383\n",
      "92478\n",
      "92569\n",
      "92663\n",
      "92760\n",
      "92831\n",
      "92910\n",
      "93004\n",
      "93074\n",
      "93132\n",
      "93213\n",
      "93292\n",
      "93370\n",
      "93450\n",
      "93530\n",
      "93624\n",
      "93723\n",
      "93811\n",
      "93877\n",
      "93964\n",
      "94052\n",
      "94137\n",
      "94224\n",
      "94316\n",
      "94405\n",
      "94490\n",
      "94569\n",
      "94656\n",
      "94724\n",
      "94807\n",
      "94911\n",
      "94999\n",
      "95094\n",
      "95178\n",
      "95262\n",
      "95355\n",
      "95452\n",
      "95539\n",
      "95628\n",
      "95708\n",
      "95797\n",
      "95884\n",
      "95972\n",
      "96042\n",
      "96130\n",
      "96212\n",
      "96300\n",
      "96395\n",
      "96472\n",
      "96554\n",
      "96653\n",
      "96744\n",
      "96831\n",
      "96914\n",
      "97034\n",
      "97124\n",
      "97211\n",
      "97293\n",
      "97369\n",
      "97471\n",
      "97561\n",
      "97647\n",
      "97728\n",
      "97802\n",
      "97882\n",
      "97971\n",
      "98063\n",
      "98146\n",
      "98237\n",
      "98344\n",
      "98418\n",
      "98504\n",
      "98593\n",
      "98683\n",
      "98765\n",
      "98859\n",
      "98935\n",
      "99026\n",
      "99112\n",
      "99217\n",
      "99304\n",
      "99389\n",
      "99508\n",
      "99605\n",
      "99685\n",
      "99767\n",
      "99849\n",
      "99936\n",
      "100014\n",
      "100100\n",
      "100191\n",
      "100257\n",
      "100347\n",
      "100449\n",
      "100542\n",
      "100618\n",
      "100711\n",
      "100800\n",
      "100873\n",
      "100961\n",
      "101053\n",
      "101147\n",
      "101223\n",
      "101307\n",
      "101394\n",
      "101469\n",
      "101545\n",
      "101644\n",
      "101709\n",
      "101791\n",
      "101882\n",
      "101968\n",
      "102063\n",
      "102138\n",
      "102228\n",
      "102307\n",
      "102396\n",
      "102488\n",
      "102576\n",
      "102666\n",
      "102754\n",
      "102857\n",
      "102945\n",
      "103033\n",
      "103123\n",
      "103218\n",
      "103309\n",
      "103388\n",
      "103476\n",
      "103571\n",
      "103648\n",
      "103736\n",
      "103825\n",
      "103905\n",
      "104003\n",
      "104092\n",
      "104166\n",
      "104267\n",
      "104347\n",
      "104430\n",
      "104517\n",
      "104603\n",
      "104704\n",
      "104780\n",
      "104862\n",
      "104939\n",
      "105029\n",
      "105112\n",
      "105204\n",
      "105302\n",
      "105390\n",
      "105473\n",
      "105574\n",
      "105645\n",
      "105734\n",
      "105821\n",
      "105902\n",
      "105978\n",
      "106073\n",
      "106165\n",
      "106257\n",
      "106352\n",
      "106442\n",
      "106533\n",
      "106621\n",
      "106738\n",
      "106826\n",
      "106919\n",
      "107006\n",
      "107091\n",
      "107178\n",
      "107258\n",
      "107349\n",
      "107438\n",
      "107520\n",
      "107600\n",
      "107694\n",
      "107777\n",
      "107871\n",
      "107955\n",
      "108040\n",
      "108124\n",
      "108200\n",
      "108292\n",
      "108374\n",
      "108466\n",
      "108545\n",
      "108630\n",
      "108730\n",
      "108838\n",
      "108919\n",
      "108999\n",
      "109078\n",
      "109165\n",
      "109251\n",
      "109337\n",
      "109420\n",
      "109506\n",
      "109581\n",
      "109665\n",
      "109753\n",
      "109838\n",
      "109953\n",
      "110056\n",
      "110131\n",
      "110213\n",
      "110297\n",
      "110401\n",
      "110495\n",
      "110580\n",
      "110667\n",
      "110750\n",
      "110835\n",
      "110930\n",
      "111005\n",
      "111091\n",
      "111171\n",
      "111259\n",
      "111346\n",
      "111440\n",
      "111539\n",
      "111626\n",
      "111713\n",
      "111785\n",
      "111867\n",
      "111960\n",
      "112051\n",
      "112138\n",
      "112209\n",
      "112303\n",
      "112375\n",
      "112453\n",
      "112546\n",
      "112635\n",
      "112713\n",
      "112779\n",
      "112866\n",
      "112948\n",
      "113040\n",
      "113140\n",
      "113230\n",
      "113335\n",
      "113415\n",
      "113489\n",
      "113579\n",
      "113675\n",
      "113763\n",
      "113847\n",
      "113940\n",
      "114035\n",
      "114129\n",
      "114202\n",
      "114295\n",
      "114373\n",
      "114466\n",
      "114542\n",
      "114627\n",
      "114727\n",
      "114812\n",
      "114902\n",
      "114978\n",
      "115065\n",
      "115158\n",
      "115247\n",
      "115328\n",
      "115417\n",
      "115503\n",
      "115583\n",
      "115683\n",
      "115776\n",
      "115865\n",
      "115934\n",
      "116023\n",
      "116125\n",
      "116225\n",
      "116316\n",
      "116392\n",
      "116477\n",
      "116549\n",
      "116623\n",
      "116710\n",
      "116805\n",
      "116873\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "116977\n",
      "117062\n",
      "117147\n",
      "117239\n",
      "117315\n",
      "117421\n",
      "117509\n",
      "117593\n",
      "117678\n",
      "117761\n",
      "117845\n",
      "117924\n",
      "118003\n",
      "118096\n",
      "118177\n",
      "118263\n",
      "118361\n",
      "118427\n",
      "118490\n",
      "118581\n",
      "118662\n",
      "118738\n",
      "118826\n",
      "118915\n",
      "119006\n",
      "119115\n",
      "119192\n",
      "119286\n",
      "119363\n",
      "119466\n",
      "119539\n",
      "119630\n",
      "119714\n",
      "119788\n",
      "119867\n",
      "119948\n",
      "120033\n",
      "120105\n",
      "120192\n",
      "120273\n",
      "120358\n",
      "120438\n",
      "120530\n",
      "120608\n",
      "120689\n",
      "120791\n",
      "120872\n",
      "120969\n",
      "121061\n",
      "121160\n",
      "121237\n",
      "121318\n",
      "121403\n",
      "121497\n",
      "121578\n",
      "121657\n",
      "121743\n",
      "121836\n",
      "121929\n",
      "122023\n",
      "122111\n",
      "122201\n",
      "122275\n",
      "122358\n",
      "122466\n",
      "122564\n",
      "122644\n",
      "122747\n",
      "122827\n",
      "122916\n",
      "123008\n",
      "123104\n",
      "123185\n",
      "123271\n",
      "123346\n",
      "123433\n",
      "123512\n",
      "123596\n",
      "123683\n",
      "123768\n",
      "123835\n",
      "123944\n",
      "124028\n",
      "124109\n",
      "124189\n",
      "124289\n",
      "124380\n",
      "124476\n",
      "124561\n",
      "124637\n",
      "124740\n",
      "124837\n",
      "124909\n",
      "124987\n",
      "125067\n",
      "125154\n",
      "125247\n",
      "125319\n",
      "125404\n",
      "125476\n",
      "125575\n",
      "125643\n",
      "125721\n",
      "125810\n",
      "125885\n",
      "125963\n",
      "126055\n",
      "126143\n",
      "126247\n",
      "126336\n",
      "126413\n",
      "126505\n",
      "126582\n",
      "126664\n",
      "126743\n",
      "126824\n",
      "126907\n",
      "126997\n",
      "127086\n",
      "127161\n",
      "127265\n",
      "127349\n",
      "127444\n",
      "127564\n",
      "127644\n",
      "127734\n",
      "127821\n",
      "127908\n",
      "128001\n"
     ]
    }
   ],
   "source": [
    "if 1:\n",
    "    item=128000\n",
    "    \n",
    "    \n",
    "    \n",
    "    x2_train=[]\n",
    "    y2_train=[]\n",
    "    \n",
    "    \n",
    "    \n",
    "    while 1:\n",
    "        game = Game(4, score_to_win=2048, random=False)\n",
    "        agent = ExpectiMaxAgent(game, display=display)\n",
    "        \n",
    "        \n",
    "        \n",
    "        while game.end==False:\n",
    "            tmp=[]\n",
    "            tmp=(game.board)\n",
    "            tmp[tmp==0]=1\n",
    "            \n",
    "            \n",
    "            \n",
    "            a=tmp.reshape(16)\n",
    "            b=np.sort(a)\n",
    "            a1=b[-1]\n",
    "            a2=b[-2]\n",
    "            a3=b[-3]\n",
    "            tmp=np.log2(tmp)\n",
    "            tmp=tmp.tolist()\n",
    "            if a1<256:\n",
    "                game.move(agent.step())\n",
    "                continue\n",
    "            elif a1<512:\n",
    "                if len(x2_train)>item:\n",
    "                    game.move(agent.step())\n",
    "                    continue\n",
    "                x2_train.append(tmp)\n",
    "                y2_train.append(agent.step())\n",
    "            else:\n",
    "                break\n",
    "            game.move(agent.step())\n",
    "        print(len(x2_train))\n",
    "        if len(x2_train)>=item:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('x0_512.npy',x2_train)\n",
    "np.save('y0_512.npy',y2_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "x2_train=np.load('x0_512.npy')\n",
    "y2_train=np.load('y0_512.npy')\n",
    "x2_train=np.array(x2_train,dtype=float)\n",
    "y2_train=np.array(y2_train,dtype=float)\n",
    "p = np.random.permutation(range(len(x2_train)))\n",
    "x2_train,y2_train = x2_train[p],y2_train[p]\n",
    "x2_train=x2_train.astype('float32')\n",
    "x2_train=to_categorical(x2_train,9)\n",
    "y2_train=to_categorical(y2_train)\n",
    "np.save('x_512.npy',x2_train)\n",
    "np.save('y_512.npy',y2_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "x2_train=np.load('x0_512.npy')\n",
    "y2_train=np.load('y0_512.npy')\n",
    "x2_train=x2_train.tolist()\n",
    "y2_train=y2_train.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "128001"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x2_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "x20_train=np.array(x2_train,dtype=float)\n",
    "y20_train=np.array(y2_train,dtype=float)\n",
    "l=len(x2_train)\n",
    "for i in range(l):\n",
    "    tmp=x20_train[i,:,:]\n",
    "    step=y20_train[i]\n",
    "    tmp1=[]\n",
    "    tmp2=[]\n",
    "    tmp3=[]\n",
    "    tmp4=[]\n",
    "    tmp5=[]\n",
    "    tmp1[:] = list(map(list,zip(*tmp[::-1])))\n",
    "    tmp1=np.array(tmp1,dtype=float)\n",
    "    x2_train.append(tmp1)\n",
    "    y2_train.append((step+3)%4)\n",
    "    tmp2[:] = list(map(list,zip(*tmp1[::-1])))\n",
    "    tmp2=np.array(tmp2,dtype=float)\n",
    "    x2_train.append(tmp2)\n",
    "    y2_train.append((step+2)%4)\n",
    "    tmp3[:] = list(map(list,zip(*tmp2[::-1])))\n",
    "    tmp3=np.array(tmp3,dtype=float)\n",
    "    x2_train.append(tmp3)\n",
    "    y2_train.append((step+1)%4)\n",
    "\n",
    "x2_train=np.array(x2_train,dtype=float)\n",
    "y2_train=np.array(y2_train,dtype=float)\n",
    "p = np.random.permutation(range(len(x2_train)))\n",
    "x2_train,y2_train = x2_train[p],y2_train[p]\n",
    "x2_train=x2_train.astype('float32')\n",
    "x2_train=to_categorical(x2_train,9)\n",
    "y2_train=to_categorical(y2_train)\n",
    "np.save('x_512.npy',x2_train)\n",
    "np.save('y_512.npy',y2_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2.,  1., -1., -2.],\n",
       "       [ 3.,  0.,  0., -3.],\n",
       "       [ 5.,  2., -2., -5.],\n",
       "       [ 8.,  5., -5., -8.]])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_4 (InputLayer)            (None, 4, 4, 9)      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_19 (Conv2D)              (None, 4, 4, 256)    9472        input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_20 (Conv2D)              (None, 4, 4, 256)    9472        input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_21 (Conv2D)              (None, 4, 4, 256)    2560        input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_22 (Conv2D)              (None, 4, 4, 256)    9472        input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_23 (Conv2D)              (None, 4, 4, 256)    20992       input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_24 (Conv2D)              (None, 4, 4, 256)    37120       input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "flatten_19 (Flatten)            (None, 4096)         0           conv2d_19[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "flatten_20 (Flatten)            (None, 4096)         0           conv2d_20[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "flatten_21 (Flatten)            (None, 4096)         0           conv2d_21[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "flatten_22 (Flatten)            (None, 4096)         0           conv2d_22[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "flatten_23 (Flatten)            (None, 4096)         0           conv2d_23[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "flatten_24 (Flatten)            (None, 4096)         0           conv2d_24[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_4 (Concatenate)     (None, 24576)        0           flatten_19[0][0]                 \n",
      "                                                                 flatten_20[0][0]                 \n",
      "                                                                 flatten_21[0][0]                 \n",
      "                                                                 flatten_22[0][0]                 \n",
      "                                                                 flatten_23[0][0]                 \n",
      "                                                                 flatten_24[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_13 (Activation)      (None, 24576)        0           concatenate_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_13 (Dense)                (None, 2048)         50333696    activation_13[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_14 (BatchNo (None, 2048)         8192        dense_13[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_14 (Activation)      (None, 2048)         0           batch_normalization_14[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_10 (Dropout)            (None, 2048)         0           activation_14[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_14 (Dense)                (None, 512)          1049088     dropout_10[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_15 (BatchNo (None, 512)          2048        dense_14[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_15 (Activation)      (None, 512)          0           batch_normalization_15[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_11 (Dropout)            (None, 512)          0           activation_15[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_15 (Dense)                (None, 128)          65664       dropout_11[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_16 (BatchNo (None, 128)          512         dense_15[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_16 (Activation)      (None, 128)          0           batch_normalization_16[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_12 (Dropout)            (None, 128)          0           activation_16[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_16 (Dense)                (None, 4)            516         dropout_12[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 51,548,804\n",
      "Trainable params: 51,543,428\n",
      "Non-trainable params: 5,376\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/20\n",
      "512004/512004 [==============================] - 138s 270us/step - loss: 1.0208 - acc: 0.4956\n",
      "Epoch 2/20\n",
      "512004/512004 [==============================] - 135s 264us/step - loss: 0.8285 - acc: 0.5996\n",
      "Epoch 3/20\n",
      "512004/512004 [==============================] - 135s 263us/step - loss: 0.7725 - acc: 0.6287\n",
      "Epoch 4/20\n",
      "512004/512004 [==============================] - 136s 266us/step - loss: 0.7366 - acc: 0.6488\n",
      "Epoch 5/20\n",
      "512004/512004 [==============================] - 136s 265us/step - loss: 0.7043 - acc: 0.6669\n",
      "Epoch 6/20\n",
      "512004/512004 [==============================] - 135s 264us/step - loss: 0.6716 - acc: 0.6858\n",
      "Epoch 7/20\n",
      "512004/512004 [==============================] - 135s 263us/step - loss: 0.6394 - acc: 0.7032\n",
      "Epoch 8/20\n",
      "512004/512004 [==============================] - 135s 264us/step - loss: 0.6069 - acc: 0.7209\n",
      "Epoch 9/20\n",
      "512004/512004 [==============================] - 135s 264us/step - loss: 0.5699 - acc: 0.7410\n",
      "Epoch 10/20\n",
      "512004/512004 [==============================] - 135s 263us/step - loss: 0.5305 - acc: 0.7620\n",
      "Epoch 11/20\n",
      "512004/512004 [==============================] - 136s 265us/step - loss: 0.4912 - acc: 0.7824\n",
      "Epoch 12/20\n",
      "512004/512004 [==============================] - 135s 264us/step - loss: 0.4507 - acc: 0.8036\n",
      "Epoch 13/20\n",
      "512004/512004 [==============================] - 135s 264us/step - loss: 0.4113 - acc: 0.8228\n",
      "Epoch 14/20\n",
      "512004/512004 [==============================] - 134s 263us/step - loss: 0.3711 - acc: 0.8429\n",
      "Epoch 15/20\n",
      "512004/512004 [==============================] - 135s 264us/step - loss: 0.3353 - acc: 0.8601\n",
      "Epoch 16/20\n",
      "512004/512004 [==============================] - 135s 263us/step - loss: 0.3029 - acc: 0.8750\n",
      "Epoch 17/20\n",
      "512004/512004 [==============================] - 136s 265us/step - loss: 0.2725 - acc: 0.8891\n",
      "Epoch 18/20\n",
      "512004/512004 [==============================] - 135s 264us/step - loss: 0.2457 - acc: 0.9017\n",
      "Epoch 19/20\n",
      "512004/512004 [==============================] - 135s 264us/step - loss: 0.2234 - acc: 0.9121\n",
      "Epoch 20/20\n",
      "512004/512004 [==============================] - 135s 264us/step - loss: 0.2018 - acc: 0.9208\n"
     ]
    }
   ],
   "source": [
    "first_train('512',9)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while 1:\n",
    "    second_train('512',9,256,512,0,512,0,512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'b' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-d89a9adf0ffd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mpp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-11-d89a9adf0ffd>\u001b[0m in \u001b[0;36mpp\u001b[0;34m(a)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mpp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0ma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mb\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnboundLocalError\u001b[0m: local variable 'b' referenced before assignment"
     ]
    }
   ],
   "source": [
    "b=2\n",
    "def pp(a):\n",
    "    a=a+1\n",
    "    b=b+1\n",
    "a=1\n",
    "\n",
    "pp(a)\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_tensorflow_p36)",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
