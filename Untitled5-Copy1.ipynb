{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from game2048.game import Game\n",
    "from game2048.displays import Display, IPythonDisplay\n",
    "from game2048.agents import Agent, RandomAgent, ExpectiMaxAgent\n",
    "import json\n",
    "import numpy as np\n",
    "import random,math\n",
    "import keras\n",
    "from keras.models import Sequential,load_model,model_from_json,Input,Model\n",
    "from keras.optimizers import SGD,RMSprop\n",
    "from keras.layers import SimpleRNN,BatchNormalization,Dense, Dropout, Flatten, MaxPooling3D, MaxPooling2D ,Activation ,Concatenate ,Conv3D,Conv2D,concatenate\n",
    "from keras.utils import to_categorical\n",
    "from keras.utils import np_utils\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64.0"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43 87 74 101 89 78 84 129\n",
      "85 172 157 186 171 172 159 228\n",
      "128 260 257 273 260 250 249 311\n",
      "161 353 344 365 324 345 343 430\n",
      "205 430 445 477 371 474 379 528\n",
      "257 495 544 592 412 585 440 621\n",
      "303 585 615 678 489 694 509 701\n",
      "347 670 709 753 563 780 616 762\n",
      "394 756 803 851 635 882 705 824\n",
      "431 835 881 942 717 974 790 908\n",
      "487 907 966 1031 791 1106 884 959\n",
      "530 990 1063 1114 868 1199 968 1037\n",
      "565 1077 1156 1228 930 1276 1068 1132\n",
      "606 1164 1238 1316 1023 1369 1150 1220\n",
      "653 1255 1324 1400 1105 1461 1257 1287\n",
      "688 1337 1406 1490 1198 1552 1341 1356\n",
      "739 1417 1491 1575 1286 1630 1428 1451\n",
      "782 1498 1591 1652 1381 1706 1511 1541\n",
      "829 1600 1654 1747 1460 1782 1625 1622\n",
      "875 1692 1748 1826 1548 1871 1704 1702\n",
      "931 1765 1855 1898 1630 1971 1797 1780\n",
      "968 1863 1915 1994 1726 2036 1890 1860\n",
      "1002 1958 2014 2068 1817 2134 1970 1985\n",
      "1051 2036 2112 2146 1916 2216 2059 2076\n",
      "1098 2135 2183 2244 1998 2313 2132 2166\n",
      "1146 2217 2284 2327 2079 2417 2206 2243\n",
      "1177 2304 2364 2420 2176 2494 2283 2332\n",
      "1219 2393 2473 2482 2256 2583 2368 2412\n",
      "1252 2498 2545 2566 2339 2676 2438 2508\n",
      "1308 2564 2636 2652 2424 2762 2529 2593\n",
      "1349 2658 2702 2752 2516 2827 2642 2651\n",
      "1390 2758 2773 2856 2594 2920 2727 2740\n",
      "1428 2845 2860 2959 2681 3003 2810 2817\n",
      "1467 2937 2935 3076 2749 3095 2894 2909\n",
      "1513 3016 3021 3164 2824 3193 2973 3000\n",
      "1564 3097 3111 3247 2910 3278 3071 3081\n",
      "1609 3203 3166 3341 2992 3389 3126 3172\n",
      "1653 3290 3244 3445 3066 3476 3213 3251\n",
      "1683 3389 3337 3535 3123 3593 3276 3340\n",
      "1727 3479 3426 3625 3197 3674 3365 3427\n",
      "1777 3567 3515 3746 3232 3768 3450 3519\n",
      "1803 3674 3599 3882 3262 3852 3544 3596\n",
      "1847 3757 3686 3971 3350 3925 3641 3683\n",
      "1888 3835 3812 4024 3435 4001 3736 3778\n",
      "1941 3924 3883 4122 3521 4102 3808 3865\n",
      "1972 4011 3969 4198 3609 4186 3898 3951\n",
      "2001 4113 4048 4295 3689 4262 3983 4042\n",
      "2042 4191 4153 4369 3785 4338 4070 4128\n",
      "2079 4269 4242 4369 3785 4338 4070 4128\n",
      "2135 4349 4329 4447 3863 4429 4146 4218\n",
      "2177 4450 4407 4535 3946 4517 4225 4307\n",
      "2223 4538 4493 4621 4037 4599 4312 4400\n",
      "2264 4640 4565 4714 4118 4694 4413 4473\n",
      "2296 4725 4653 4809 4203 4771 4544 4506\n",
      "2336 4817 4725 4911 4284 4854 4624 4601\n",
      "2402 4897 4796 4987 4370 4958 4724 4706\n",
      "2460 4983 4873 5090 4442 5050 4805 4796\n",
      "2516 5052 4955 5189 4529 5122 4890 4887\n",
      "2553 5140 5040 5281 4609 5221 4961 4983\n",
      "2601 5220 5128 5374 4694 5299 5047 5077\n",
      "2648 5301 5225 5461 4810 5347 5137 5157\n",
      "2682 5396 5301 5552 4886 5465 5196 5231\n",
      "2716 5496 5381 5640 4968 5556 5280 5317\n",
      "2752 5602 5441 5721 5064 5637 5365 5404\n",
      "2783 5692 5537 5812 5125 5717 5456 5491\n",
      "2836 5782 5618 5904 5198 5812 5544 5583\n",
      "2883 5868 5704 5984 5291 5890 5629 5672\n",
      "2925 5962 5786 6081 5364 5994 5705 5800\n",
      "2981 6048 5858 6184 5425 6084 5786 5884\n",
      "3020 6140 5933 6281 5496 6167 5873 5970\n",
      "3071 6218 6024 6370 5573 6269 5947 6062\n",
      "3102 6301 6155 6435 5639 6392 5986 6148\n",
      "3159 6381 6236 6522 5731 6497 6043 6232\n",
      "3211 6469 6313 6615 5820 6571 6131 6322\n",
      "3254 6570 6389 6711 5903 6675 6211 6403\n",
      "3300 6646 6468 6797 5986 6760 6296 6483\n",
      "3345 6738 6549 6884 6080 6829 6390 6566\n",
      "3387 6814 6654 6945 6151 6935 6469 6655\n",
      "3427 6916 6740 7028 6238 7029 6581 6750\n",
      "3496 6982 6826 7116 6319 7116 6658 6847\n",
      "3537 7069 6920 7191 6404 7199 6748 6929\n",
      "3580 7178 7007 7269 6482 7292 6829 7020\n",
      "3611 7271 7097 7399 6526 7377 6922 7115\n",
      "3646 7348 7189 7489 6601 7468 7001 7211\n",
      "3691 7438 7291 7565 6677 7560 7082 7299\n",
      "3729 7528 7361 7657 6764 7637 7167 7384\n",
      "3776 7616 7446 7731 6858 7760 7222 7506\n",
      "3826 7689 7540 7835 6924 7853 7321 7599\n",
      "3871 7773 7635 7918 7003 7954 7399 7688\n",
      "3914 7865 7711 8002 7098 8030 7481 7775\n",
      "3967 7941 7794 8093 7185 8107 7573 7864\n",
      "4010 8033 7876 8178 7270 8206 7648 7954\n",
      "4065 8114 7951 8273 7360 8284 7735 8042\n",
      "4104 8205 8025 8369 7448 8368 7841 8104\n",
      "4160 8278 8118 8467 7528 8456 7923 8193\n",
      "4210 8372 8199 8542 7611 8540 8002 8290\n",
      "4254 8467 8278 8642 7690 8631 8091 8375\n",
      "4293 8551 8382 8714 7770 8723 8181 8462\n",
      "4339 8642 8472 8792 7860 8848 8220 8551\n",
      "4391 8711 8555 8884 7931 8932 8305 8651\n",
      "4440 8803 8627 8964 8017 9017 8394 8743\n",
      "4478 8895 8700 9064 8090 9092 8487 8827\n",
      "4525 8975 8802 9138 8185 9171 8565 8920\n",
      "4563 9071 8880 9228 8271 9259 8653 8999\n",
      "4615 9158 8997 9290 8373 9328 8726 9085\n",
      "4661 9247 9083 9368 8448 9424 8806 9169\n",
      "4708 9341 9165 9456 8539 9511 8881 9257\n",
      "4744 9440 9230 9553 8621 9607 8995 9298\n",
      "4793 9526 9326 9629 8704 9691 9076 9390\n",
      "4905 9538 9409 9717 8776 9789 9156 9481\n",
      "4947 9621 9500 9795 8858 9883 9233 9574\n",
      "4981 9730 9586 9888 8925 9974 9363 9623\n",
      "5026 9835 9658 9967 9008 10071 9451 9690\n",
      "5075 9922 9743 10038 9098 10161 9536 9776\n",
      "5123 10010 9840 10122 9202 10231 9632 9865\n",
      "5158 10104 9925 10203 9281 10314 9725 9966\n",
      "5203 10191 10020 10290 9356 10426 9792 10054\n",
      "5256 10272 10115 10370 9439 10520 9873 10145\n",
      "5303 10355 10195 10472 9515 10608 9961 10229\n",
      "5343 10451 10275 10564 9594 10690 10052 10321\n",
      "5384 10548 10338 10655 9681 10768 10143 10404\n",
      "5431 10617 10431 10740 9750 10861 10221 10498\n",
      "5463 10714 10517 10824 9833 10939 10315 10579\n",
      "5526 10789 10594 10915 9915 11033 10393 10668\n",
      "5570 10877 10684 11005 9992 11113 10478 10750\n",
      "5616 10971 10757 11095 10072 11209 10581 10836\n",
      "5646 11069 10831 11192 10150 11302 10662 11012\n",
      "5687 11160 10911 11281 10227 11396 10747 11092\n",
      "5726 11256 11006 11352 10305 11489 10822 11185\n",
      "5759 11358 11091 11440 10378 11579 10912 11275\n",
      "5812 11449 11179 11531 10454 11665 10998 11371\n",
      "5852 11535 11263 11628 10528 11774 11084 11480\n",
      "5895 11653 11334 11701 10611 11860 11166 11571\n",
      "5940 11748 11417 11784 10689 11944 11271 11654\n",
      "5989 11827 11499 11879 10769 12028 11351 11742\n",
      "6029 11916 11584 11973 10850 12114 11429 11825\n",
      "6068 11999 11670 12060 10935 12193 11515 11920\n",
      "6111 12089 11754 12139 11018 12323 11559 12007\n",
      "6163 12178 11833 12224 11096 12409 11631 12098\n",
      "6201 12265 11964 12273 11171 12500 11714 12184\n",
      "6246 12346 12052 12359 11248 12585 11798 12275\n",
      "6288 12442 12171 12421 11319 12674 11877 12358\n",
      "6326 12533 12256 12523 11391 12756 11973 12446\n",
      "6375 12623 12355 12588 11477 12836 12055 12534\n",
      "6404 12717 12439 12694 11552 12918 12147 12619\n",
      "6446 12832 12495 12792 11622 13007 12238 12707\n",
      "6497 12920 12568 12883 11702 13109 12300 12816\n",
      "6539 13005 12656 12960 11789 13216 12374 12909\n",
      "6574 13096 12772 13013 11874 13306 12464 12982\n",
      "6624 13174 12867 13107 11946 13385 12549 13065\n",
      "6677 13254 12951 13221 12009 13468 12632 13164\n",
      "6723 13345 13025 13315 12089 13559 12711 13247\n",
      "6762 13433 13135 13386 12200 13600 12808 13332\n",
      "6787 13531 13217 13467 12280 13678 12902 13418\n",
      "6831 13622 13343 13510 12361 13774 12976 13510\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-38-a49946478736>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     99\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m             \u001b[0mgame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmove\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m         \u001b[0mu\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx1_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx2_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx3_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx4_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx5_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx6_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx7_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx8_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx1_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx2_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx3_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx4_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx5_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx6_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx7_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx8_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/2048-api/game2048/agents.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m         \u001b[0mdirection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msearch_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mboard\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdirection\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/2048-api/game2048/expectimax/_ext.py\u001b[0m in \u001b[0;36mboard_to_move\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    108\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m         \u001b[0mret\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 110\u001b[0;31m     \u001b[0mmove\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mm_to_move\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    111\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmove\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/2048-api/game2048/expectimax/_ext.py\u001b[0m in \u001b[0;36mm_to_move\u001b[0;34m(m)\u001b[0m\n\u001b[1;32m     98\u001b[0m     \u001b[0mmine\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mldru\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m     '''\n\u001b[0;32m--> 100\u001b[0;31m     \u001b[0mmove\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfind_best_move\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    101\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmove\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/2048-api/game2048/expectimax/_ext.py\u001b[0m in \u001b[0;36mfind_best_move\u001b[0;34m(m)\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m         scores = pool.map(score_toplevel_move, [\n\u001b[0;32m---> 83\u001b[0;31m                           (board, move) for move in range(4)])\n\u001b[0m\u001b[1;32m     84\u001b[0m         \u001b[0mbestmove\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbestscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbestscore\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/multiprocessing/pool.py\u001b[0m in \u001b[0;36mmap\u001b[0;34m(self, func, iterable, chunksize)\u001b[0m\n\u001b[1;32m    264\u001b[0m         \u001b[0;32min\u001b[0m \u001b[0ma\u001b[0m \u001b[0mlist\u001b[0m \u001b[0mthat\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mreturned\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m         '''\n\u001b[0;32m--> 266\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_map_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmapstar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunksize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    267\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstarmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunksize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/multiprocessing/pool.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    636\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    637\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 638\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    639\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mready\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    640\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/multiprocessing/pool.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    633\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    634\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 635\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_event\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    636\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    637\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    549\u001b[0m             \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flag\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    550\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 551\u001b[0;31m                 \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cond\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    552\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    553\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    293\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 295\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    296\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if 1:\n",
    "    item=64000\n",
    "    \n",
    "    x1_train=[]\n",
    "    y1_train=[]\n",
    "    \n",
    "    x2_train=[]\n",
    "    y2_train=[]\n",
    "    \n",
    "    x3_train=[]\n",
    "    y3_train=[]\n",
    "    \n",
    "    x4_train=[]\n",
    "    y4_train=[]\n",
    "    \n",
    "    x5_train=[]\n",
    "    y5_train=[]\n",
    "    \n",
    "    x6_train=[]\n",
    "    y6_train=[]\n",
    "    \n",
    "    x7_train=[]\n",
    "    y7_train=[]\n",
    "    \n",
    "    x8_train=[]\n",
    "    y8_train=[]\n",
    "    \n",
    "    while 1:\n",
    "        game = Game(4, score_to_win=2048, random=False)\n",
    "        agent = ExpectiMaxAgent(game, display=display)\n",
    "        \n",
    "        \n",
    "        \n",
    "        while game.end==False:\n",
    "            tmp=[]\n",
    "            tmp=(game.board)\n",
    "            tmp[tmp==0]=1\n",
    "            \n",
    "            \n",
    "            \n",
    "            a=tmp.reshape(16)\n",
    "            b=np.sort(a)\n",
    "            a1=b[-1]\n",
    "            a2=b[-2]\n",
    "            a3=b[-3]\n",
    "            tmp=np.log2(tmp)\n",
    "            tmp=tmp.tolist()\n",
    "            if a1<128:\n",
    "                game.move(agent.step())\n",
    "                continue\n",
    "            elif a1<256:\n",
    "                if len(x1_train)>item:\n",
    "                    game.move(agent.step())\n",
    "                    continue\n",
    "                x1_train.append(tmp)\n",
    "                y1_train.append(agent.step())\n",
    "            elif a1<512:\n",
    "                if len(x2_train)>item:\n",
    "                    game.move(agent.step())\n",
    "                    continue\n",
    "                x2_train.append(tmp)\n",
    "                y2_train.append(agent.step())\n",
    "            elif a1<1024 and a2<256:\n",
    "                if len(x3_train)>item:\n",
    "                    game.move(agent.step())\n",
    "                    continue\n",
    "                x3_train.append(tmp)\n",
    "                y3_train.append(agent.step())\n",
    "            elif a1<1024:\n",
    "                if len(x4_train)>item:\n",
    "                    game.move(agent.step())\n",
    "                    continue\n",
    "                x4_train.append(tmp)\n",
    "                y4_train.append(agent.step())\n",
    "            elif a1<2048 and a2<256:\n",
    "                if len(x5_train)>item:\n",
    "                    game.move(agent.step())\n",
    "                    continue\n",
    "                x5_train.append(tmp)\n",
    "                y5_train.append(agent.step())\n",
    "            elif a1<2048 and a2<512:\n",
    "                if len(x6_train)>item:\n",
    "                    game.move(agent.step())\n",
    "                    continue\n",
    "                x6_train.append(tmp)\n",
    "                y6_train.append(agent.step())\n",
    "            elif a1<2048 and a3<256:\n",
    "                if len(x7_train)>item:\n",
    "                    game.move(agent.step())\n",
    "                    continue\n",
    "                x7_train.append(tmp)\n",
    "                y7_train.append(agent.step())\n",
    "            elif a1<2048:\n",
    "                if len(x8_train)>item:\n",
    "                    game.move(agent.step())\n",
    "                    continue\n",
    "                x8_train.append(tmp)\n",
    "                y8_train.append(agent.step())\n",
    "            else:\n",
    "                break\n",
    "            game.move(agent.step())\n",
    "        u=min(len(x1_train),len(x2_train),len(x3_train),len(x4_train),len(x5_train),len(x6_train),len(x7_train),len(x8_train))\n",
    "        print(len(x1_train),len(x2_train),len(x3_train),len(x4_train),len(x5_train),len(x6_train),len(x7_train),len(x8_train))\n",
    "        if u>=item:\n",
    "            break\n",
    "\n",
    "                \n",
    "                \n",
    "            \n",
    "            \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 1:\n",
    "    x1_train=np.array(x1_train,dtype=float)\n",
    "    y1_train=np.array(y1_train,dtype=float)\n",
    "    x2_train=np.array(x2_train,dtype=float)\n",
    "    y2_train=np.array(y2_train,dtype=float)\n",
    "    x3_train=np.array(x3_train,dtype=float)\n",
    "    y3_train=np.array(y3_train,dtype=float)\n",
    "    x4_train=np.array(x4_train,dtype=float)\n",
    "    y4_train=np.array(y4_train,dtype=float)\n",
    "    x5_train=np.array(x5_train,dtype=float)\n",
    "    y5_train=np.array(y5_train,dtype=float)\n",
    "    x6_train=np.array(x6_train,dtype=float)\n",
    "    y6_train=np.array(y6_train,dtype=float)\n",
    "    x7_train=np.array(x7_train,dtype=float)\n",
    "    y7_train=np.array(y7_train,dtype=float)\n",
    "    x8_train=np.array(x8_train,dtype=float)\n",
    "    y8_train=np.array(y8_train,dtype=float)\n",
    "    \n",
    "    p = np.random.permutation(range(len(x1_train)))\n",
    "    x1_train,y1_train = x1_train[p],y1_train[p]\n",
    "    x1_train=x1_train.astype('float32')\n",
    "    x1_train=to_categorical(x1_train,8)\n",
    "    y1_train=to_categorical(y1_train)\n",
    "    np.save('x_256.npy',x1_train)\n",
    "    np.save('y_256.npy',y1_train)\n",
    "    \n",
    "    p = np.random.permutation(range(len(x2_train)))\n",
    "    x2_train,y2_train = x2_train[p],y2_train[p]\n",
    "    x2_train=x2_train.astype('float32')\n",
    "    x2_train=to_categorical(x2_train,9)\n",
    "    y2_train=to_categorical(y2_train)\n",
    "    np.save('x_512.npy',x2_train)\n",
    "    np.save('y_512.npy',y2_train)\n",
    "    \n",
    "    p = np.random.permutation(range(len(x3_train)))\n",
    "    x3_train,y3_train = x3_train[p],y3_train[p]\n",
    "    x3_train=x3_train.astype('float32')\n",
    "    x3_train=to_categorical(x3_train,10)\n",
    "    y3_train=to_categorical(y3_train)\n",
    "    np.save('x_1024_1.npy',x3_train)\n",
    "    np.save('y_1024_1.npy',y3_train)\n",
    "\n",
    "    p = np.random.permutation(range(len(x4_train)))\n",
    "    x4_train,y4_train = x4_train[p],y4_train[p]\n",
    "    x4_train=x4_train.astype('float32')\n",
    "    x4_train=to_categorical(x4_train,10)\n",
    "    y4_train=to_categorical(y4_train)\n",
    "    np.save('x_1024_2.npy',x4_train)\n",
    "    np.save('y_1024_2.npy',y4_train)\n",
    "    \n",
    "    p = np.random.permutation(range(len(x5_train)))\n",
    "    x5_train,y5_train = x5_train[p],y5_train[p]\n",
    "    x5_train=x5_train.astype('float32')\n",
    "    x5_train=to_categorical(x5_train,11)\n",
    "    y5_train=to_categorical(y5_train)\n",
    "    np.save('x_2048_1.npy',x5_train)\n",
    "    np.save('y_2048_1.npy',y5_train)\n",
    "    \n",
    "    p = np.random.permutation(range(len(x6_train)))\n",
    "    x6_train,y6_train = x6_train[p],y6_train[p]\n",
    "    x6_train=x6_train.astype('float32')\n",
    "    x6_train=to_categorical(x6_train,11)\n",
    "    y6_train=to_categorical(y6_train)\n",
    "    np.save('x_2048_2.npy',x6_train)\n",
    "    np.save('y_2048_2.npy',y6_train)\n",
    "    \n",
    "    p = np.random.permutation(range(len(x7_train)))\n",
    "    x7_train,y7_train = x7_train[p],y7_train[p]\n",
    "    x7_train=x7_train.astype('float32')\n",
    "    x7_train=to_categorical(x7_train,11)\n",
    "    y7_train=to_categorical(y7_train)\n",
    "    np.save('x_2048_3.npy',x7_train)\n",
    "    np.save('y_2048_3.npy',y7_train)\n",
    "\n",
    "    p = np.random.permutation(range(len(x8_train)))\n",
    "    x8_train,y8_train = x8_train[p],y8_train[p]\n",
    "    x8_train=x8_train.astype('float32')\n",
    "    x8_train=to_categorical(x8_train,11)\n",
    "    y8_train=to_categorical(y8_train)\n",
    "    np.save('x_2048_4.npy',x8_train)\n",
    "    np.save('y_2048_4.npy',y8_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def first_train(s,cand):\n",
    "    x_train=np.load('x_'+s+'.npy')\n",
    "    y_train=np.load('y_'+s+'.npy')\n",
    "    display = IPythonDisplay()\n",
    "    model = Sequential()\n",
    "    Filters=128\n",
    "    inputs=Input((4,4,cand))\n",
    "    conv=inputs\n",
    "    conv1=Conv2D(Filters,kernel_size=(4,1),kernel_initializer='he_uniform',padding='Same')(conv)\n",
    "\n",
    "    conv2=Conv2D(Filters,kernel_size=(1,4),kernel_initializer='he_uniform',padding='Same')(conv)\n",
    "\n",
    "    conv3=Conv2D(Filters,kernel_size=(1,1),kernel_initializer='he_uniform',padding='Same')(conv)\n",
    "\n",
    "    conv4=Conv2D(Filters,kernel_size=(2,2),kernel_initializer='he_uniform',padding='Same')(conv)\n",
    "\n",
    "    conv5=Conv2D(Filters,kernel_size=(3,3),kernel_initializer='he_uniform',padding='Same')(conv)\n",
    "\n",
    "    conv6=Conv2D(Filters,kernel_size=(4,4),kernel_initializer='he_uniform',padding='Same')(conv)\n",
    "\n",
    "    hidden=concatenate([Flatten()(conv1),Flatten()(conv2),Flatten()(conv3),Flatten()(conv4),Flatten()(conv5),Flatten()(conv6)])\n",
    "    x=BatchNormalization()(hidden)\n",
    "    x=Activation('relu')(hidden)\n",
    "    x=Dense(512,kernel_initializer='he_uniform')(x)\n",
    "    x=BatchNormalization()(x)\n",
    "    x=Activation('relu')(x)\n",
    "    x=Dropout(0.2)(x)\n",
    "    x=Dense(128,kernel_initializer='he_uniform')(x)\n",
    "    x=BatchNormalization()(x)\n",
    "    x=Activation('relu')(x)\n",
    "    x=Dropout(0.2)(x)\n",
    "    outputs=Dense(4,activation='softmax')(x)\n",
    "    model=Model(inputs,outputs)\n",
    "    model.summary()\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=RMSprop(lr = 0.0001, decay=0.0),metrics=['accuracy'])\n",
    "    model.fit(x=x_train,y=y_train,epochs=20,batch_size=2048,verbose=1,shuffle=True)\n",
    "    model.save('2048_'+s+'.h5')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def move(model,tm,cand):\n",
    "    \n",
    "    tm=np.array(tm,dtype=float)\n",
    "    tmp0=tm.astype('float32')\n",
    "       \n",
    "    tmp=tmp0\n",
    "        \n",
    "        \n",
    "    tmp=to_categorical(tmp0,cand)\n",
    "        \n",
    "    tmp = tmp.reshape(1,4,4,cand)\n",
    "    tmp_list=[]\n",
    "    tmp_list.append(tmp)\n",
    "    tmp_list=max(model.predict(tmp_list,batch_size=128))\n",
    "    tmp_pre=tmp_list.tolist()\n",
    "    direction=(tmp_pre.index(max(tmp_pre)))\n",
    "    return direction\n",
    "\n",
    "def second_train(s,cand,s1,e1,s2,e2,s3,e3):\n",
    "    model=load_model('2048_'+s+'.h5')\n",
    "    item=6400\n",
    "    m=0\n",
    "    n=0\n",
    "    x_train=[]\n",
    "    y_train=[]\n",
    "    while len(x_train)<item:\n",
    "        \n",
    "        game = Game(4, score_to_win=2048, random=False)\n",
    "        agent = ExpectiMaxAgent(game, display=display)\n",
    "        m=m+1\n",
    "        \n",
    "        \n",
    "        while game.end==False:\n",
    "            tmp=[]\n",
    "            tmp=(game.board)\n",
    "            tmp[tmp==0]=1\n",
    "            \n",
    "            \n",
    "            \n",
    "            a=tmp.reshape(16)\n",
    "            b=np.sort(a)\n",
    "            a1=b[-1]\n",
    "            a2=b[-2]\n",
    "            a3=b[-3]\n",
    "            tmp=np.log2(tmp)\n",
    "            \n",
    "            \n",
    "            if a1>=s1 and a1<e1 and a2>=s2 and a2<e2 and a3>=s3 and a3<e3:\n",
    "                choice=move(model,tmp,cand)\n",
    "                tmp=tmp.tolist()\n",
    "                x_train.append(tmp)\n",
    "                y_train.append(agent.step())\n",
    "                game.move(choice)\n",
    "            elif a1<s1 or (a1>=s1 and a1<e1 and a2<s2) or (a1>=s1 and a1<e1 and a2>=s2 and a2<e2 and a3<s3):\n",
    "                game.move(agent.step())\n",
    "                continue\n",
    "            else:\n",
    "                n=n+1\n",
    "                break\n",
    "    print(n,m)\n",
    "            \n",
    "    x_train=np.array(x_train,dtype=float)\n",
    "    y_train=np.array(y_train,dtype=float)\n",
    "    p = np.random.permutation(range(len(x_train)))\n",
    "    x_train,y_train = x_train[p],y_train[p]\n",
    "    x_train=x_train.astype('float32')\n",
    "    x_train=to_categorical(x_train,cand)\n",
    "    y_train=to_categorical(y_train)\n",
    "    model.fit(x=x_train,y=y_train,epochs=1,batch_size=512,verbose=1,shuffle=True)\n",
    "    model.save('2048_'+s+'.h5')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 4, 4, 8)      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 4, 4, 128)    4224        input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 4, 4, 128)    4224        input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 4, 4, 128)    1152        input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 4, 4, 128)    4224        input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 4, 4, 128)    9344        input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, 4, 4, 128)    16512       input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 2048)         0           conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "flatten_2 (Flatten)             (None, 2048)         0           conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "flatten_3 (Flatten)             (None, 2048)         0           conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "flatten_4 (Flatten)             (None, 2048)         0           conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "flatten_5 (Flatten)             (None, 2048)         0           conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "flatten_6 (Flatten)             (None, 2048)         0           conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 12288)        0           flatten_1[0][0]                  \n",
      "                                                                 flatten_2[0][0]                  \n",
      "                                                                 flatten_3[0][0]                  \n",
      "                                                                 flatten_4[0][0]                  \n",
      "                                                                 flatten_5[0][0]                  \n",
      "                                                                 flatten_6[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 12288)        0           concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 512)          6291968     activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 512)          2048        dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 512)          0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 512)          0           activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 128)          65664       dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 128)          512         dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 128)          0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 128)          0           activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 4)            516         dropout_2[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 6,400,388\n",
      "Trainable params: 6,399,108\n",
      "Non-trainable params: 1,280\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/40\n",
      "6867/6867 [==============================] - 4s 588us/step - loss: 1.2929 - acc: 0.4032\n",
      "Epoch 2/40\n",
      "6867/6867 [==============================] - 2s 234us/step - loss: 1.1384 - acc: 0.4490\n",
      "Epoch 3/40\n",
      "6867/6867 [==============================] - 2s 235us/step - loss: 1.0684 - acc: 0.4827\n",
      "Epoch 4/40\n",
      "6867/6867 [==============================] - 2s 234us/step - loss: 1.0010 - acc: 0.5286\n",
      "Epoch 5/40\n",
      "6867/6867 [==============================] - 2s 233us/step - loss: 0.9400 - acc: 0.5743\n",
      "Epoch 6/40\n",
      "6867/6867 [==============================] - 2s 233us/step - loss: 0.8656 - acc: 0.6078\n",
      "Epoch 7/40\n",
      "6867/6867 [==============================] - 2s 234us/step - loss: 0.7975 - acc: 0.6530\n",
      "Epoch 8/40\n",
      "6867/6867 [==============================] - 2s 234us/step - loss: 0.7197 - acc: 0.6866\n",
      "Epoch 9/40\n",
      "6867/6867 [==============================] - 2s 235us/step - loss: 0.6434 - acc: 0.7287\n",
      "Epoch 10/40\n",
      "6867/6867 [==============================] - 2s 234us/step - loss: 0.5713 - acc: 0.7613\n",
      "Epoch 11/40\n",
      "6867/6867 [==============================] - 2s 233us/step - loss: 0.5010 - acc: 0.7979\n",
      "Epoch 12/40\n",
      "6867/6867 [==============================] - 2s 235us/step - loss: 0.4328 - acc: 0.8287\n",
      "Epoch 13/40\n",
      "6867/6867 [==============================] - 2s 240us/step - loss: 0.3808 - acc: 0.8522\n",
      "Epoch 14/40\n",
      "6867/6867 [==============================] - 2s 236us/step - loss: 0.3276 - acc: 0.8729\n",
      "Epoch 15/40\n",
      "6867/6867 [==============================] - 2s 237us/step - loss: 0.2821 - acc: 0.8949\n",
      "Epoch 16/40\n",
      "6867/6867 [==============================] - 2s 235us/step - loss: 0.2595 - acc: 0.9013\n",
      "Epoch 17/40\n",
      "6867/6867 [==============================] - 2s 235us/step - loss: 0.2122 - acc: 0.9219\n",
      "Epoch 18/40\n",
      "6867/6867 [==============================] - 2s 235us/step - loss: 0.1973 - acc: 0.9305\n",
      "Epoch 19/40\n",
      "6867/6867 [==============================] - 2s 235us/step - loss: 0.1787 - acc: 0.9361\n",
      "Epoch 20/40\n",
      "6867/6867 [==============================] - 2s 234us/step - loss: 0.1580 - acc: 0.9455\n",
      "Epoch 21/40\n",
      "6867/6867 [==============================] - 2s 234us/step - loss: 0.1496 - acc: 0.9457\n",
      "Epoch 22/40\n",
      "6867/6867 [==============================] - 2s 234us/step - loss: 0.1353 - acc: 0.9518\n",
      "Epoch 23/40\n",
      "6867/6867 [==============================] - 2s 235us/step - loss: 0.1360 - acc: 0.9522\n",
      "Epoch 24/40\n",
      "6867/6867 [==============================] - 2s 234us/step - loss: 0.1198 - acc: 0.9572\n",
      "Epoch 25/40\n",
      "6867/6867 [==============================] - 2s 235us/step - loss: 0.1192 - acc: 0.9588\n",
      "Epoch 26/40\n",
      "6867/6867 [==============================] - 2s 234us/step - loss: 0.1068 - acc: 0.9632\n",
      "Epoch 27/40\n",
      "6867/6867 [==============================] - 2s 234us/step - loss: 0.1041 - acc: 0.9645\n",
      "Epoch 28/40\n",
      "6867/6867 [==============================] - 2s 234us/step - loss: 0.1021 - acc: 0.9645\n",
      "Epoch 29/40\n",
      "6867/6867 [==============================] - 2s 233us/step - loss: 0.0861 - acc: 0.9703\n",
      "Epoch 30/40\n",
      "6867/6867 [==============================] - 2s 234us/step - loss: 0.0885 - acc: 0.9699\n",
      "Epoch 31/40\n",
      "6867/6867 [==============================] - 2s 234us/step - loss: 0.0834 - acc: 0.9696\n",
      "Epoch 32/40\n",
      "6867/6867 [==============================] - 2s 233us/step - loss: 0.0790 - acc: 0.9751\n",
      "Epoch 33/40\n",
      "6867/6867 [==============================] - 2s 233us/step - loss: 0.0876 - acc: 0.9699\n",
      "Epoch 34/40\n",
      "6867/6867 [==============================] - 2s 234us/step - loss: 0.0800 - acc: 0.9735\n",
      "Epoch 35/40\n",
      "6867/6867 [==============================] - 2s 233us/step - loss: 0.0775 - acc: 0.9738\n",
      "Epoch 36/40\n",
      "6867/6867 [==============================] - 2s 234us/step - loss: 0.0723 - acc: 0.9754\n",
      "Epoch 37/40\n",
      "6867/6867 [==============================] - 2s 234us/step - loss: 0.0744 - acc: 0.9761\n",
      "Epoch 38/40\n",
      "6867/6867 [==============================] - 2s 234us/step - loss: 0.0762 - acc: 0.9715\n",
      "Epoch 39/40\n",
      "6867/6867 [==============================] - 2s 233us/step - loss: 0.0758 - acc: 0.9760\n",
      "Epoch 40/40\n",
      "6867/6867 [==============================] - 2s 234us/step - loss: 0.0616 - acc: 0.9796\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            (None, 4, 4, 9)      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, 4, 4, 128)    4736        input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)               (None, 4, 4, 128)    4736        input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_9 (Conv2D)               (None, 4, 4, 128)    1280        input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_10 (Conv2D)              (None, 4, 4, 128)    4736        input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_11 (Conv2D)              (None, 4, 4, 128)    10496       input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_12 (Conv2D)              (None, 4, 4, 128)    18560       input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "flatten_7 (Flatten)             (None, 2048)         0           conv2d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "flatten_8 (Flatten)             (None, 2048)         0           conv2d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "flatten_9 (Flatten)             (None, 2048)         0           conv2d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "flatten_10 (Flatten)            (None, 2048)         0           conv2d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "flatten_11 (Flatten)            (None, 2048)         0           conv2d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "flatten_12 (Flatten)            (None, 2048)         0           conv2d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 12288)        0           flatten_7[0][0]                  \n",
      "                                                                 flatten_8[0][0]                  \n",
      "                                                                 flatten_9[0][0]                  \n",
      "                                                                 flatten_10[0][0]                 \n",
      "                                                                 flatten_11[0][0]                 \n",
      "                                                                 flatten_12[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 12288)        0           concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 512)          6291968     activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 512)          2048        dense_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 512)          0           batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 512)          0           activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 128)          65664       dropout_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 128)          512         dense_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 128)          0           batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 128)          0           activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 4)            516         dropout_4[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 6,405,252\n",
      "Trainable params: 6,403,972\n",
      "Non-trainable params: 1,280\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/40\n",
      "13706/13706 [==============================] - 4s 324us/step - loss: 1.2356 - acc: 0.3981\n",
      "Epoch 2/40\n",
      "13706/13706 [==============================] - 3s 234us/step - loss: 1.1216 - acc: 0.4374\n",
      "Epoch 3/40\n",
      "13706/13706 [==============================] - 3s 235us/step - loss: 1.0754 - acc: 0.4726\n",
      "Epoch 4/40\n",
      "13706/13706 [==============================] - 3s 236us/step - loss: 1.0220 - acc: 0.5104\n",
      "Epoch 5/40\n",
      "13706/13706 [==============================] - 3s 238us/step - loss: 0.9631 - acc: 0.5425\n",
      "Epoch 6/40\n",
      "13706/13706 [==============================] - 3s 235us/step - loss: 0.8979 - acc: 0.5866\n",
      "Epoch 7/40\n",
      "13706/13706 [==============================] - 3s 235us/step - loss: 0.8288 - acc: 0.6311\n",
      "Epoch 8/40\n",
      "13706/13706 [==============================] - 3s 235us/step - loss: 0.7662 - acc: 0.6615\n",
      "Epoch 9/40\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13706/13706 [==============================] - 3s 235us/step - loss: 0.6916 - acc: 0.7009\n",
      "Epoch 10/40\n",
      "13706/13706 [==============================] - 3s 234us/step - loss: 0.6292 - acc: 0.7337\n",
      "Epoch 11/40\n",
      "13706/13706 [==============================] - 3s 234us/step - loss: 0.5539 - acc: 0.7701\n",
      "Epoch 12/40\n",
      "13706/13706 [==============================] - 3s 234us/step - loss: 0.4930 - acc: 0.8022\n",
      "Epoch 13/40\n",
      "13706/13706 [==============================] - 3s 234us/step - loss: 0.4444 - acc: 0.8244\n",
      "Epoch 14/40\n",
      "13706/13706 [==============================] - 3s 234us/step - loss: 0.3954 - acc: 0.8420\n",
      "Epoch 15/40\n",
      "13706/13706 [==============================] - 3s 234us/step - loss: 0.3418 - acc: 0.8672\n",
      "Epoch 16/40\n",
      "13706/13706 [==============================] - 3s 234us/step - loss: 0.3054 - acc: 0.8827\n",
      "Epoch 17/40\n",
      "13706/13706 [==============================] - 3s 250us/step - loss: 0.2758 - acc: 0.8978\n",
      "Epoch 18/40\n",
      "13706/13706 [==============================] - 3s 235us/step - loss: 0.2544 - acc: 0.9054\n",
      "Epoch 19/40\n",
      "13706/13706 [==============================] - 3s 235us/step - loss: 0.2315 - acc: 0.9151\n",
      "Epoch 20/40\n",
      "13706/13706 [==============================] - 3s 234us/step - loss: 0.2127 - acc: 0.9227\n",
      "Epoch 21/40\n",
      "13706/13706 [==============================] - 3s 234us/step - loss: 0.2081 - acc: 0.9243\n",
      "Epoch 22/40\n",
      "13706/13706 [==============================] - 3s 234us/step - loss: 0.1908 - acc: 0.9297\n",
      "Epoch 23/40\n",
      "13706/13706 [==============================] - 3s 238us/step - loss: 0.1823 - acc: 0.9342\n",
      "Epoch 24/40\n",
      "13706/13706 [==============================] - 3s 236us/step - loss: 0.1661 - acc: 0.9403\n",
      "Epoch 25/40\n",
      "13706/13706 [==============================] - 3s 235us/step - loss: 0.1675 - acc: 0.9421\n",
      "Epoch 26/40\n",
      "13706/13706 [==============================] - 3s 234us/step - loss: 0.1490 - acc: 0.9487\n",
      "Epoch 27/40\n",
      "13706/13706 [==============================] - 3s 234us/step - loss: 0.1440 - acc: 0.9483\n",
      "Epoch 28/40\n",
      "13706/13706 [==============================] - 3s 235us/step - loss: 0.1497 - acc: 0.9484\n",
      "Epoch 29/40\n",
      "13706/13706 [==============================] - 3s 234us/step - loss: 0.1408 - acc: 0.9515\n",
      "Epoch 30/40\n",
      "13706/13706 [==============================] - 3s 233us/step - loss: 0.1438 - acc: 0.9505\n",
      "Epoch 31/40\n",
      "13706/13706 [==============================] - 3s 234us/step - loss: 0.1374 - acc: 0.9524\n",
      "Epoch 32/40\n",
      "13706/13706 [==============================] - 3s 234us/step - loss: 0.1361 - acc: 0.9521\n",
      "Epoch 33/40\n",
      "13706/13706 [==============================] - 3s 234us/step - loss: 0.1366 - acc: 0.9553\n",
      "Epoch 34/40\n",
      "13706/13706 [==============================] - 3s 233us/step - loss: 0.1322 - acc: 0.9555\n",
      "Epoch 35/40\n",
      "13706/13706 [==============================] - 3s 233us/step - loss: 0.1268 - acc: 0.9569\n",
      "Epoch 36/40\n",
      "13706/13706 [==============================] - 3s 233us/step - loss: 0.1238 - acc: 0.9605\n",
      "Epoch 37/40\n",
      "13706/13706 [==============================] - 3s 233us/step - loss: 0.1257 - acc: 0.9583\n",
      "Epoch 38/40\n",
      "13706/13706 [==============================] - 3s 233us/step - loss: 0.1256 - acc: 0.9570\n",
      "Epoch 39/40\n",
      "13706/13706 [==============================] - 3s 233us/step - loss: 0.1295 - acc: 0.9575\n",
      "Epoch 40/40\n",
      "13706/13706 [==============================] - 3s 233us/step - loss: 0.1200 - acc: 0.9600\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_3 (InputLayer)            (None, 4, 4, 10)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_13 (Conv2D)              (None, 4, 4, 128)    5248        input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_14 (Conv2D)              (None, 4, 4, 128)    5248        input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_15 (Conv2D)              (None, 4, 4, 128)    1408        input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_16 (Conv2D)              (None, 4, 4, 128)    5248        input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_17 (Conv2D)              (None, 4, 4, 128)    11648       input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_18 (Conv2D)              (None, 4, 4, 128)    20608       input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "flatten_13 (Flatten)            (None, 2048)         0           conv2d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "flatten_14 (Flatten)            (None, 2048)         0           conv2d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "flatten_15 (Flatten)            (None, 2048)         0           conv2d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "flatten_16 (Flatten)            (None, 2048)         0           conv2d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "flatten_17 (Flatten)            (None, 2048)         0           conv2d_17[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "flatten_18 (Flatten)            (None, 2048)         0           conv2d_18[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 12288)        0           flatten_13[0][0]                 \n",
      "                                                                 flatten_14[0][0]                 \n",
      "                                                                 flatten_15[0][0]                 \n",
      "                                                                 flatten_16[0][0]                 \n",
      "                                                                 flatten_17[0][0]                 \n",
      "                                                                 flatten_18[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, 12288)        0           concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (None, 512)          6291968     activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, 512)          2048        dense_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_8 (Activation)       (None, 512)          0           batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_5 (Dropout)             (None, 512)          0           activation_8[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dense_8 (Dense)                 (None, 128)          65664       dropout_5[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (None, 128)          512         dense_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_9 (Activation)       (None, 128)          0           batch_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_6 (Dropout)             (None, 128)          0           activation_9[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dense_9 (Dense)                 (None, 4)            516         dropout_6[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 6,410,116\n",
      "Trainable params: 6,408,836\n",
      "Non-trainable params: 1,280\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "13425/13425 [==============================] - 4s 332us/step - loss: 1.2364 - acc: 0.3923\n",
      "Epoch 2/40\n",
      "13425/13425 [==============================] - 3s 233us/step - loss: 1.1151 - acc: 0.4324\n",
      "Epoch 3/40\n",
      "13425/13425 [==============================] - 3s 233us/step - loss: 1.0646 - acc: 0.4697\n",
      "Epoch 4/40\n",
      "13425/13425 [==============================] - 3s 234us/step - loss: 1.0143 - acc: 0.5065\n",
      "Epoch 5/40\n",
      "13425/13425 [==============================] - 3s 234us/step - loss: 0.9601 - acc: 0.5476\n",
      "Epoch 6/40\n",
      "13425/13425 [==============================] - 3s 234us/step - loss: 0.8960 - acc: 0.5853\n",
      "Epoch 7/40\n",
      "13425/13425 [==============================] - 3s 233us/step - loss: 0.8254 - acc: 0.6267\n",
      "Epoch 8/40\n",
      "13425/13425 [==============================] - 3s 233us/step - loss: 0.7473 - acc: 0.6713\n",
      "Epoch 9/40\n",
      "13425/13425 [==============================] - 3s 233us/step - loss: 0.6802 - acc: 0.7076\n",
      "Epoch 10/40\n",
      "13425/13425 [==============================] - 3s 235us/step - loss: 0.6023 - acc: 0.7459\n",
      "Epoch 11/40\n",
      "13425/13425 [==============================] - 3s 237us/step - loss: 0.5494 - acc: 0.7718\n",
      "Epoch 12/40\n",
      "13425/13425 [==============================] - 3s 236us/step - loss: 0.4821 - acc: 0.8039\n",
      "Epoch 13/40\n",
      "13425/13425 [==============================] - 3s 235us/step - loss: 0.4234 - acc: 0.8302\n",
      "Epoch 14/40\n",
      "13425/13425 [==============================] - 3s 233us/step - loss: 0.3702 - acc: 0.8541\n",
      "Epoch 15/40\n",
      "13425/13425 [==============================] - 3s 233us/step - loss: 0.3321 - acc: 0.8734\n",
      "Epoch 16/40\n",
      "13425/13425 [==============================] - 3s 234us/step - loss: 0.2948 - acc: 0.8916\n",
      "Epoch 17/40\n",
      "13425/13425 [==============================] - 3s 233us/step - loss: 0.2706 - acc: 0.8992\n",
      "Epoch 18/40\n",
      "13425/13425 [==============================] - 3s 233us/step - loss: 0.2491 - acc: 0.9076\n",
      "Epoch 19/40\n",
      "13425/13425 [==============================] - 3s 233us/step - loss: 0.2227 - acc: 0.9179\n",
      "Epoch 20/40\n",
      "13425/13425 [==============================] - 3s 235us/step - loss: 0.1987 - acc: 0.9272\n",
      "Epoch 21/40\n",
      "13425/13425 [==============================] - 3s 233us/step - loss: 0.2003 - acc: 0.9266\n",
      "Epoch 22/40\n",
      "13425/13425 [==============================] - 3s 234us/step - loss: 0.1912 - acc: 0.9321\n",
      "Epoch 23/40\n",
      "13425/13425 [==============================] - 3s 233us/step - loss: 0.1794 - acc: 0.9376\n",
      "Epoch 24/40\n",
      "13425/13425 [==============================] - 3s 234us/step - loss: 0.1638 - acc: 0.9401\n",
      "Epoch 25/40\n",
      "13425/13425 [==============================] - 3s 233us/step - loss: 0.1572 - acc: 0.9412\n",
      "Epoch 26/40\n",
      "13425/13425 [==============================] - 3s 233us/step - loss: 0.1645 - acc: 0.9436\n",
      "Epoch 27/40\n",
      "13425/13425 [==============================] - 3s 233us/step - loss: 0.1551 - acc: 0.9458\n",
      "Epoch 28/40\n",
      "13425/13425 [==============================] - 3s 233us/step - loss: 0.1413 - acc: 0.9496\n",
      "Epoch 29/40\n",
      "13425/13425 [==============================] - 3s 233us/step - loss: 0.1331 - acc: 0.9555\n",
      "Epoch 30/40\n",
      "13425/13425 [==============================] - 3s 233us/step - loss: 0.1437 - acc: 0.9505\n",
      "Epoch 31/40\n",
      "13425/13425 [==============================] - 3s 234us/step - loss: 0.1408 - acc: 0.9530\n",
      "Epoch 32/40\n",
      "13425/13425 [==============================] - 3s 233us/step - loss: 0.1344 - acc: 0.9539\n",
      "Epoch 33/40\n",
      "13425/13425 [==============================] - 3s 249us/step - loss: 0.1322 - acc: 0.9550\n",
      "Epoch 34/40\n",
      "13425/13425 [==============================] - 3s 232us/step - loss: 0.1353 - acc: 0.9555\n",
      "Epoch 35/40\n",
      "13425/13425 [==============================] - 3s 233us/step - loss: 0.1322 - acc: 0.9543\n",
      "Epoch 36/40\n",
      "13425/13425 [==============================] - 3s 232us/step - loss: 0.1271 - acc: 0.9574\n",
      "Epoch 37/40\n",
      "13425/13425 [==============================] - 3s 233us/step - loss: 0.1333 - acc: 0.9567\n",
      "Epoch 38/40\n",
      "13425/13425 [==============================] - 3s 233us/step - loss: 0.1395 - acc: 0.9530\n",
      "Epoch 39/40\n",
      "13425/13425 [==============================] - 3s 237us/step - loss: 0.1307 - acc: 0.9564\n",
      "Epoch 40/40\n",
      "13425/13425 [==============================] - 3s 235us/step - loss: 0.1246 - acc: 0.9580\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_4 (InputLayer)            (None, 4, 4, 10)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_19 (Conv2D)              (None, 4, 4, 128)    5248        input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_20 (Conv2D)              (None, 4, 4, 128)    5248        input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_21 (Conv2D)              (None, 4, 4, 128)    1408        input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_22 (Conv2D)              (None, 4, 4, 128)    5248        input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_23 (Conv2D)              (None, 4, 4, 128)    11648       input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_24 (Conv2D)              (None, 4, 4, 128)    20608       input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "flatten_19 (Flatten)            (None, 2048)         0           conv2d_19[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "flatten_20 (Flatten)            (None, 2048)         0           conv2d_20[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "flatten_21 (Flatten)            (None, 2048)         0           conv2d_21[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "flatten_22 (Flatten)            (None, 2048)         0           conv2d_22[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "flatten_23 (Flatten)            (None, 2048)         0           conv2d_23[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "flatten_24 (Flatten)            (None, 2048)         0           conv2d_24[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_4 (Concatenate)     (None, 12288)        0           flatten_19[0][0]                 \n",
      "                                                                 flatten_20[0][0]                 \n",
      "                                                                 flatten_21[0][0]                 \n",
      "                                                                 flatten_22[0][0]                 \n",
      "                                                                 flatten_23[0][0]                 \n",
      "                                                                 flatten_24[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_10 (Activation)      (None, 12288)        0           concatenate_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_10 (Dense)                (None, 512)          6291968     activation_10[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_11 (BatchNo (None, 512)          2048        dense_10[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_11 (Activation)      (None, 512)          0           batch_normalization_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_7 (Dropout)             (None, 512)          0           activation_11[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_11 (Dense)                (None, 128)          65664       dropout_7[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_12 (BatchNo (None, 128)          512         dense_11[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_12 (Activation)      (None, 128)          0           batch_normalization_12[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_8 (Dropout)             (None, 128)          0           activation_12[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_12 (Dense)                (None, 4)            516         dropout_8[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 6,410,116\n",
      "Trainable params: 6,408,836\n",
      "Non-trainable params: 1,280\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "13598/13598 [==============================] - 5s 336us/step - loss: 1.2441 - acc: 0.3992\n",
      "Epoch 2/40\n",
      "13598/13598 [==============================] - 3s 235us/step - loss: 1.1136 - acc: 0.4458\n",
      "Epoch 3/40\n",
      "13598/13598 [==============================] - 3s 236us/step - loss: 1.0619 - acc: 0.4838\n",
      "Epoch 4/40\n",
      "13598/13598 [==============================] - 3s 241us/step - loss: 1.0139 - acc: 0.5162\n",
      "Epoch 5/40\n",
      "13598/13598 [==============================] - 3s 243us/step - loss: 0.9510 - acc: 0.5554\n",
      "Epoch 6/40\n",
      "13598/13598 [==============================] - 3s 236us/step - loss: 0.8778 - acc: 0.5971\n",
      "Epoch 7/40\n",
      "13598/13598 [==============================] - 3s 236us/step - loss: 0.8142 - acc: 0.6348\n",
      "Epoch 8/40\n",
      "13598/13598 [==============================] - 3s 237us/step - loss: 0.7416 - acc: 0.6750\n",
      "Epoch 9/40\n",
      "13598/13598 [==============================] - 3s 239us/step - loss: 0.6645 - acc: 0.7160\n",
      "Epoch 10/40\n",
      "13598/13598 [==============================] - 3s 236us/step - loss: 0.5859 - acc: 0.7522\n",
      "Epoch 11/40\n",
      "13598/13598 [==============================] - 3s 237us/step - loss: 0.5287 - acc: 0.7814\n",
      "Epoch 12/40\n",
      "13598/13598 [==============================] - 3s 238us/step - loss: 0.4583 - acc: 0.8175\n",
      "Epoch 13/40\n",
      "13598/13598 [==============================] - 3s 237us/step - loss: 0.4033 - acc: 0.8427\n",
      "Epoch 14/40\n",
      "13598/13598 [==============================] - 3s 236us/step - loss: 0.3650 - acc: 0.8536\n",
      "Epoch 15/40\n",
      "13598/13598 [==============================] - 3s 237us/step - loss: 0.3283 - acc: 0.8758\n",
      "Epoch 16/40\n",
      "13598/13598 [==============================] - 3s 238us/step - loss: 0.2928 - acc: 0.8912\n",
      "Epoch 17/40\n",
      "13598/13598 [==============================] - 3s 237us/step - loss: 0.2621 - acc: 0.8993\n",
      "Epoch 18/40\n",
      "13598/13598 [==============================] - 3s 236us/step - loss: 0.2423 - acc: 0.9085\n",
      "Epoch 19/40\n",
      "13598/13598 [==============================] - 3s 236us/step - loss: 0.2157 - acc: 0.9194\n",
      "Epoch 20/40\n",
      "13598/13598 [==============================] - 3s 235us/step - loss: 0.2042 - acc: 0.9250\n",
      "Epoch 21/40\n",
      "13598/13598 [==============================] - 3s 235us/step - loss: 0.2007 - acc: 0.9265\n",
      "Epoch 22/40\n",
      "13598/13598 [==============================] - 3s 237us/step - loss: 0.1836 - acc: 0.9345\n",
      "Epoch 23/40\n",
      "13598/13598 [==============================] - 3s 235us/step - loss: 0.1750 - acc: 0.9369\n",
      "Epoch 24/40\n",
      "13598/13598 [==============================] - 3s 235us/step - loss: 0.1630 - acc: 0.9412\n",
      "Epoch 25/40\n",
      "13598/13598 [==============================] - 3s 241us/step - loss: 0.1593 - acc: 0.9423\n",
      "Epoch 26/40\n",
      "13598/13598 [==============================] - 3s 249us/step - loss: 0.1525 - acc: 0.9474\n",
      "Epoch 27/40\n",
      "13598/13598 [==============================] - 3s 235us/step - loss: 0.1537 - acc: 0.9468\n",
      "Epoch 28/40\n",
      "13598/13598 [==============================] - 3s 236us/step - loss: 0.1468 - acc: 0.9495\n",
      "Epoch 29/40\n",
      "13598/13598 [==============================] - 3s 235us/step - loss: 0.1362 - acc: 0.9515\n",
      "Epoch 30/40\n",
      "13598/13598 [==============================] - 3s 236us/step - loss: 0.1462 - acc: 0.9502\n",
      "Epoch 31/40\n",
      "13598/13598 [==============================] - 3s 234us/step - loss: 0.1321 - acc: 0.9551\n",
      "Epoch 32/40\n",
      "13598/13598 [==============================] - 3s 233us/step - loss: 0.1393 - acc: 0.9526\n",
      "Epoch 33/40\n",
      "13598/13598 [==============================] - 3s 233us/step - loss: 0.1422 - acc: 0.9505\n",
      "Epoch 34/40\n",
      "13598/13598 [==============================] - 3s 233us/step - loss: 0.1402 - acc: 0.9521\n",
      "Epoch 35/40\n",
      "13598/13598 [==============================] - 3s 234us/step - loss: 0.1264 - acc: 0.9574\n",
      "Epoch 36/40\n",
      "13598/13598 [==============================] - 3s 232us/step - loss: 0.1350 - acc: 0.9546\n",
      "Epoch 37/40\n",
      "13598/13598 [==============================] - 3s 233us/step - loss: 0.1415 - acc: 0.9551\n",
      "Epoch 38/40\n",
      "13598/13598 [==============================] - 3s 232us/step - loss: 0.1375 - acc: 0.9558\n",
      "Epoch 39/40\n",
      "13598/13598 [==============================] - 3s 232us/step - loss: 0.1266 - acc: 0.9584\n",
      "Epoch 40/40\n",
      "13598/13598 [==============================] - 3s 233us/step - loss: 0.1233 - acc: 0.9570\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_5 (InputLayer)            (None, 4, 4, 11)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_25 (Conv2D)              (None, 4, 4, 128)    5760        input_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_26 (Conv2D)              (None, 4, 4, 128)    5760        input_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_27 (Conv2D)              (None, 4, 4, 128)    1536        input_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_28 (Conv2D)              (None, 4, 4, 128)    5760        input_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_29 (Conv2D)              (None, 4, 4, 128)    12800       input_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_30 (Conv2D)              (None, 4, 4, 128)    22656       input_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "flatten_25 (Flatten)            (None, 2048)         0           conv2d_25[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "flatten_26 (Flatten)            (None, 2048)         0           conv2d_26[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "flatten_27 (Flatten)            (None, 2048)         0           conv2d_27[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "flatten_28 (Flatten)            (None, 2048)         0           conv2d_28[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "flatten_29 (Flatten)            (None, 2048)         0           conv2d_29[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "flatten_30 (Flatten)            (None, 2048)         0           conv2d_30[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_5 (Concatenate)     (None, 12288)        0           flatten_25[0][0]                 \n",
      "                                                                 flatten_26[0][0]                 \n",
      "                                                                 flatten_27[0][0]                 \n",
      "                                                                 flatten_28[0][0]                 \n",
      "                                                                 flatten_29[0][0]                 \n",
      "                                                                 flatten_30[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_13 (Activation)      (None, 12288)        0           concatenate_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_13 (Dense)                (None, 512)          6291968     activation_13[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_14 (BatchNo (None, 512)          2048        dense_13[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_14 (Activation)      (None, 512)          0           batch_normalization_14[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_9 (Dropout)             (None, 512)          0           activation_14[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_14 (Dense)                (None, 128)          65664       dropout_9[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_15 (BatchNo (None, 128)          512         dense_14[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_15 (Activation)      (None, 128)          0           batch_normalization_15[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_10 (Dropout)            (None, 128)          0           activation_15[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_15 (Dense)                (None, 4)            516         dropout_10[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 6,414,980\n",
      "Trainable params: 6,413,700\n",
      "Non-trainable params: 1,280\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "12457/12457 [==============================] - 4s 348us/step - loss: 1.2476 - acc: 0.3935\n",
      "Epoch 2/40\n",
      "12457/12457 [==============================] - 3s 234us/step - loss: 1.1244 - acc: 0.4361\n",
      "Epoch 3/40\n",
      "12457/12457 [==============================] - 3s 234us/step - loss: 1.0758 - acc: 0.4747\n",
      "Epoch 4/40\n",
      "12457/12457 [==============================] - 3s 234us/step - loss: 1.0163 - acc: 0.5076\n",
      "Epoch 5/40\n",
      "12457/12457 [==============================] - 3s 234us/step - loss: 0.9591 - acc: 0.5494\n",
      "Epoch 6/40\n",
      "12457/12457 [==============================] - 3s 239us/step - loss: 0.8944 - acc: 0.5874\n",
      "Epoch 7/40\n",
      "12457/12457 [==============================] - 3s 246us/step - loss: 0.8135 - acc: 0.6332\n",
      "Epoch 8/40\n",
      "12457/12457 [==============================] - 3s 244us/step - loss: 0.7492 - acc: 0.6678\n",
      "Epoch 9/40\n",
      "12457/12457 [==============================] - 3s 245us/step - loss: 0.6733 - acc: 0.7084\n",
      "Epoch 10/40\n",
      "12457/12457 [==============================] - 3s 234us/step - loss: 0.5988 - acc: 0.7527\n",
      "Epoch 11/40\n",
      "12457/12457 [==============================] - 3s 234us/step - loss: 0.5265 - acc: 0.7851\n",
      "Epoch 12/40\n",
      "12457/12457 [==============================] - 3s 233us/step - loss: 0.4668 - acc: 0.8122\n",
      "Epoch 13/40\n",
      "12457/12457 [==============================] - 3s 236us/step - loss: 0.4092 - acc: 0.8388\n",
      "Epoch 14/40\n",
      "12457/12457 [==============================] - 3s 237us/step - loss: 0.3551 - acc: 0.8626\n",
      "Epoch 15/40\n",
      "12457/12457 [==============================] - 3s 235us/step - loss: 0.3214 - acc: 0.8773\n",
      "Epoch 16/40\n",
      "12457/12457 [==============================] - 3s 235us/step - loss: 0.2859 - acc: 0.8939\n",
      "Epoch 17/40\n",
      "12457/12457 [==============================] - 3s 237us/step - loss: 0.2626 - acc: 0.9016\n",
      "Epoch 18/40\n",
      "12457/12457 [==============================] - 3s 234us/step - loss: 0.2332 - acc: 0.9149\n",
      "Epoch 19/40\n",
      "12457/12457 [==============================] - 3s 234us/step - loss: 0.2140 - acc: 0.9212\n",
      "Epoch 20/40\n",
      "12457/12457 [==============================] - 3s 235us/step - loss: 0.1997 - acc: 0.9273\n",
      "Epoch 21/40\n",
      "12457/12457 [==============================] - 3s 234us/step - loss: 0.1759 - acc: 0.9330\n",
      "Epoch 22/40\n",
      "12457/12457 [==============================] - 3s 234us/step - loss: 0.1748 - acc: 0.9374\n",
      "Epoch 23/40\n",
      "12457/12457 [==============================] - 3s 234us/step - loss: 0.1656 - acc: 0.9424\n",
      "Epoch 24/40\n",
      "12457/12457 [==============================] - 3s 234us/step - loss: 0.1552 - acc: 0.9459\n",
      "Epoch 25/40\n",
      "12457/12457 [==============================] - 3s 234us/step - loss: 0.1508 - acc: 0.9489\n",
      "Epoch 26/40\n",
      "12457/12457 [==============================] - 3s 234us/step - loss: 0.1430 - acc: 0.9477\n",
      "Epoch 27/40\n",
      "12457/12457 [==============================] - 3s 234us/step - loss: 0.1352 - acc: 0.9544\n",
      "Epoch 28/40\n",
      "12457/12457 [==============================] - 3s 234us/step - loss: 0.1358 - acc: 0.9508\n",
      "Epoch 29/40\n",
      "12457/12457 [==============================] - 3s 233us/step - loss: 0.1364 - acc: 0.9511\n",
      "Epoch 30/40\n",
      "12457/12457 [==============================] - 3s 234us/step - loss: 0.1216 - acc: 0.9601\n",
      "Epoch 31/40\n",
      "12457/12457 [==============================] - 3s 234us/step - loss: 0.1304 - acc: 0.9549\n",
      "Epoch 32/40\n",
      "12457/12457 [==============================] - 3s 233us/step - loss: 0.1296 - acc: 0.9555\n",
      "Epoch 33/40\n",
      "12457/12457 [==============================] - 3s 234us/step - loss: 0.1278 - acc: 0.9583\n",
      "Epoch 34/40\n",
      "12457/12457 [==============================] - 3s 236us/step - loss: 0.1266 - acc: 0.9564\n",
      "Epoch 35/40\n",
      "12457/12457 [==============================] - 3s 236us/step - loss: 0.1213 - acc: 0.9571\n",
      "Epoch 36/40\n",
      "12457/12457 [==============================] - 3s 234us/step - loss: 0.1238 - acc: 0.9587\n",
      "Epoch 37/40\n",
      "12457/12457 [==============================] - 3s 235us/step - loss: 0.1193 - acc: 0.9608\n",
      "Epoch 38/40\n",
      "12457/12457 [==============================] - 3s 234us/step - loss: 0.1168 - acc: 0.9599\n",
      "Epoch 39/40\n",
      "12457/12457 [==============================] - 3s 234us/step - loss: 0.1190 - acc: 0.9614\n",
      "Epoch 40/40\n",
      "12457/12457 [==============================] - 3s 237us/step - loss: 0.1145 - acc: 0.9620\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_6 (InputLayer)            (None, 4, 4, 11)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_31 (Conv2D)              (None, 4, 4, 128)    5760        input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_32 (Conv2D)              (None, 4, 4, 128)    5760        input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_33 (Conv2D)              (None, 4, 4, 128)    1536        input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_34 (Conv2D)              (None, 4, 4, 128)    5760        input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_35 (Conv2D)              (None, 4, 4, 128)    12800       input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_36 (Conv2D)              (None, 4, 4, 128)    22656       input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "flatten_31 (Flatten)            (None, 2048)         0           conv2d_31[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "flatten_32 (Flatten)            (None, 2048)         0           conv2d_32[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "flatten_33 (Flatten)            (None, 2048)         0           conv2d_33[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "flatten_34 (Flatten)            (None, 2048)         0           conv2d_34[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "flatten_35 (Flatten)            (None, 2048)         0           conv2d_35[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "flatten_36 (Flatten)            (None, 2048)         0           conv2d_36[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_6 (Concatenate)     (None, 12288)        0           flatten_31[0][0]                 \n",
      "                                                                 flatten_32[0][0]                 \n",
      "                                                                 flatten_33[0][0]                 \n",
      "                                                                 flatten_34[0][0]                 \n",
      "                                                                 flatten_35[0][0]                 \n",
      "                                                                 flatten_36[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_16 (Activation)      (None, 12288)        0           concatenate_6[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_16 (Dense)                (None, 512)          6291968     activation_16[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_17 (BatchNo (None, 512)          2048        dense_16[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_17 (Activation)      (None, 512)          0           batch_normalization_17[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_11 (Dropout)            (None, 512)          0           activation_17[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_17 (Dense)                (None, 128)          65664       dropout_11[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_18 (BatchNo (None, 128)          512         dense_17[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_18 (Activation)      (None, 128)          0           batch_normalization_18[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_12 (Dropout)            (None, 128)          0           activation_18[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_18 (Dense)                (None, 4)            516         dropout_12[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 6,414,980\n",
      "Trainable params: 6,413,700\n",
      "Non-trainable params: 1,280\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "13853/13853 [==============================] - 5s 347us/step - loss: 1.2633 - acc: 0.3905\n",
      "Epoch 2/40\n",
      "13853/13853 [==============================] - 3s 235us/step - loss: 1.1340 - acc: 0.4356\n",
      "Epoch 3/40\n",
      "13853/13853 [==============================] - 3s 235us/step - loss: 1.0748 - acc: 0.4700\n",
      "Epoch 4/40\n",
      "13853/13853 [==============================] - 3s 234us/step - loss: 1.0195 - acc: 0.5127\n",
      "Epoch 5/40\n",
      "13853/13853 [==============================] - 3s 234us/step - loss: 0.9595 - acc: 0.5488\n",
      "Epoch 6/40\n",
      "13853/13853 [==============================] - 3s 235us/step - loss: 0.8904 - acc: 0.5976\n",
      "Epoch 7/40\n",
      "13853/13853 [==============================] - 3s 235us/step - loss: 0.8205 - acc: 0.6342\n",
      "Epoch 8/40\n",
      "13853/13853 [==============================] - 3s 235us/step - loss: 0.7404 - acc: 0.6770\n",
      "Epoch 9/40\n",
      "13853/13853 [==============================] - 3s 235us/step - loss: 0.6624 - acc: 0.7191\n",
      "Epoch 10/40\n",
      "13853/13853 [==============================] - 3s 236us/step - loss: 0.5823 - acc: 0.7606\n",
      "Epoch 11/40\n",
      "13853/13853 [==============================] - 3s 236us/step - loss: 0.5121 - acc: 0.7935\n",
      "Epoch 12/40\n",
      "13853/13853 [==============================] - 3s 237us/step - loss: 0.4521 - acc: 0.8182\n",
      "Epoch 13/40\n",
      "13853/13853 [==============================] - 3s 238us/step - loss: 0.4088 - acc: 0.8388\n",
      "Epoch 14/40\n",
      "13853/13853 [==============================] - 3s 236us/step - loss: 0.3461 - acc: 0.8649\n",
      "Epoch 15/40\n",
      "13853/13853 [==============================] - 3s 235us/step - loss: 0.3057 - acc: 0.8836\n",
      "Epoch 16/40\n",
      "13853/13853 [==============================] - 3s 235us/step - loss: 0.2759 - acc: 0.8962\n",
      "Epoch 17/40\n",
      "13853/13853 [==============================] - 3s 236us/step - loss: 0.2562 - acc: 0.9045\n",
      "Epoch 18/40\n",
      "13853/13853 [==============================] - 3s 238us/step - loss: 0.2307 - acc: 0.9158\n",
      "Epoch 19/40\n",
      "13853/13853 [==============================] - 3s 235us/step - loss: 0.2119 - acc: 0.9251\n",
      "Epoch 20/40\n",
      "13853/13853 [==============================] - 3s 236us/step - loss: 0.2012 - acc: 0.9272\n",
      "Epoch 21/40\n",
      "13853/13853 [==============================] - 3s 236us/step - loss: 0.1889 - acc: 0.9320\n",
      "Epoch 22/40\n",
      "13853/13853 [==============================] - 3s 236us/step - loss: 0.1787 - acc: 0.9363\n",
      "Epoch 23/40\n",
      "13853/13853 [==============================] - 3s 238us/step - loss: 0.1787 - acc: 0.9349\n",
      "Epoch 24/40\n",
      "13853/13853 [==============================] - 3s 240us/step - loss: 0.1676 - acc: 0.9419\n",
      "Epoch 25/40\n",
      "13853/13853 [==============================] - 4s 253us/step - loss: 0.1584 - acc: 0.9465\n",
      "Epoch 26/40\n",
      "13853/13853 [==============================] - 3s 236us/step - loss: 0.1481 - acc: 0.9459\n",
      "Epoch 27/40\n",
      "13853/13853 [==============================] - 3s 238us/step - loss: 0.1442 - acc: 0.9498\n",
      "Epoch 28/40\n",
      "13853/13853 [==============================] - 3s 237us/step - loss: 0.1375 - acc: 0.9514\n",
      "Epoch 29/40\n",
      "13853/13853 [==============================] - 3s 235us/step - loss: 0.1428 - acc: 0.9495\n",
      "Epoch 30/40\n",
      "13853/13853 [==============================] - 3s 238us/step - loss: 0.1432 - acc: 0.9512\n",
      "Epoch 31/40\n",
      "13853/13853 [==============================] - 3s 237us/step - loss: 0.1358 - acc: 0.9529\n",
      "Epoch 32/40\n",
      "13853/13853 [==============================] - 3s 238us/step - loss: 0.1373 - acc: 0.9531\n",
      "Epoch 33/40\n",
      "13853/13853 [==============================] - 3s 237us/step - loss: 0.1382 - acc: 0.9542\n",
      "Epoch 34/40\n",
      "13853/13853 [==============================] - 3s 238us/step - loss: 0.1331 - acc: 0.9577\n",
      "Epoch 35/40\n",
      "13853/13853 [==============================] - 3s 237us/step - loss: 0.1284 - acc: 0.9557\n",
      "Epoch 36/40\n",
      "13853/13853 [==============================] - 3s 239us/step - loss: 0.1276 - acc: 0.9560\n",
      "Epoch 37/40\n",
      "13853/13853 [==============================] - 3s 238us/step - loss: 0.1230 - acc: 0.9573\n",
      "Epoch 38/40\n",
      "13853/13853 [==============================] - 3s 238us/step - loss: 0.1406 - acc: 0.9570\n",
      "Epoch 39/40\n",
      "13853/13853 [==============================] - 3s 240us/step - loss: 0.1258 - acc: 0.9573\n",
      "Epoch 40/40\n",
      "13853/13853 [==============================] - 3s 239us/step - loss: 0.1278 - acc: 0.9554\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_7 (InputLayer)            (None, 4, 4, 11)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_37 (Conv2D)              (None, 4, 4, 128)    5760        input_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_38 (Conv2D)              (None, 4, 4, 128)    5760        input_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_39 (Conv2D)              (None, 4, 4, 128)    1536        input_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_40 (Conv2D)              (None, 4, 4, 128)    5760        input_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_41 (Conv2D)              (None, 4, 4, 128)    12800       input_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_42 (Conv2D)              (None, 4, 4, 128)    22656       input_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "flatten_37 (Flatten)            (None, 2048)         0           conv2d_37[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "flatten_38 (Flatten)            (None, 2048)         0           conv2d_38[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "flatten_39 (Flatten)            (None, 2048)         0           conv2d_39[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "flatten_40 (Flatten)            (None, 2048)         0           conv2d_40[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "flatten_41 (Flatten)            (None, 2048)         0           conv2d_41[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "flatten_42 (Flatten)            (None, 2048)         0           conv2d_42[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_7 (Concatenate)     (None, 12288)        0           flatten_37[0][0]                 \n",
      "                                                                 flatten_38[0][0]                 \n",
      "                                                                 flatten_39[0][0]                 \n",
      "                                                                 flatten_40[0][0]                 \n",
      "                                                                 flatten_41[0][0]                 \n",
      "                                                                 flatten_42[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_19 (Activation)      (None, 12288)        0           concatenate_7[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_19 (Dense)                (None, 512)          6291968     activation_19[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_20 (BatchNo (None, 512)          2048        dense_19[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_20 (Activation)      (None, 512)          0           batch_normalization_20[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_13 (Dropout)            (None, 512)          0           activation_20[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_20 (Dense)                (None, 128)          65664       dropout_13[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_21 (BatchNo (None, 128)          512         dense_20[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_21 (Activation)      (None, 128)          0           batch_normalization_21[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_14 (Dropout)            (None, 128)          0           activation_21[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_21 (Dense)                (None, 4)            516         dropout_14[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 6,414,980\n",
      "Trainable params: 6,413,700\n",
      "Non-trainable params: 1,280\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "13055/13055 [==============================] - 5s 374us/step - loss: 1.2370 - acc: 0.4022\n",
      "Epoch 2/40\n",
      "13055/13055 [==============================] - 3s 238us/step - loss: 1.1151 - acc: 0.4512\n",
      "Epoch 3/40\n",
      "13055/13055 [==============================] - 3s 239us/step - loss: 1.0580 - acc: 0.4868\n",
      "Epoch 4/40\n",
      "13055/13055 [==============================] - 3s 240us/step - loss: 1.0007 - acc: 0.5237\n",
      "Epoch 5/40\n",
      "13055/13055 [==============================] - 3s 240us/step - loss: 0.9405 - acc: 0.5593\n",
      "Epoch 6/40\n",
      "13055/13055 [==============================] - 3s 242us/step - loss: 0.8741 - acc: 0.6050\n",
      "Epoch 7/40\n",
      "13055/13055 [==============================] - 3s 243us/step - loss: 0.7997 - acc: 0.6496\n",
      "Epoch 8/40\n",
      "13055/13055 [==============================] - 3s 242us/step - loss: 0.7283 - acc: 0.6856\n",
      "Epoch 9/40\n",
      "13055/13055 [==============================] - 3s 242us/step - loss: 0.6396 - acc: 0.7305\n",
      "Epoch 10/40\n",
      "13055/13055 [==============================] - 3s 242us/step - loss: 0.5713 - acc: 0.7636\n",
      "Epoch 11/40\n",
      "13055/13055 [==============================] - 3s 238us/step - loss: 0.4972 - acc: 0.7969\n",
      "Epoch 12/40\n",
      "13055/13055 [==============================] - 3s 237us/step - loss: 0.4344 - acc: 0.8273\n",
      "Epoch 13/40\n",
      "13055/13055 [==============================] - 3s 235us/step - loss: 0.3835 - acc: 0.8531\n",
      "Epoch 14/40\n",
      "13055/13055 [==============================] - 3s 234us/step - loss: 0.3347 - acc: 0.8714\n",
      "Epoch 15/40\n",
      "13055/13055 [==============================] - 3s 235us/step - loss: 0.3010 - acc: 0.8847\n",
      "Epoch 16/40\n",
      "13055/13055 [==============================] - 3s 235us/step - loss: 0.2733 - acc: 0.8997\n",
      "Epoch 17/40\n",
      "13055/13055 [==============================] - 3s 236us/step - loss: 0.2386 - acc: 0.9111\n",
      "Epoch 18/40\n",
      "13055/13055 [==============================] - 3s 236us/step - loss: 0.2162 - acc: 0.9187\n",
      "Epoch 19/40\n",
      "13055/13055 [==============================] - 3s 236us/step - loss: 0.2080 - acc: 0.9236\n",
      "Epoch 20/40\n",
      "13055/13055 [==============================] - 3s 234us/step - loss: 0.1862 - acc: 0.9327\n",
      "Epoch 21/40\n",
      "13055/13055 [==============================] - 3s 235us/step - loss: 0.1714 - acc: 0.9386\n",
      "Epoch 22/40\n",
      "13055/13055 [==============================] - 3s 234us/step - loss: 0.1766 - acc: 0.9383\n",
      "Epoch 23/40\n",
      "13055/13055 [==============================] - 3s 235us/step - loss: 0.1654 - acc: 0.9415\n",
      "Epoch 24/40\n",
      "13055/13055 [==============================] - 3s 236us/step - loss: 0.1549 - acc: 0.9442\n",
      "Epoch 25/40\n",
      "13055/13055 [==============================] - 3s 235us/step - loss: 0.1467 - acc: 0.9508\n",
      "Epoch 26/40\n",
      "13055/13055 [==============================] - 3s 235us/step - loss: 0.1432 - acc: 0.9509\n",
      "Epoch 27/40\n",
      "13055/13055 [==============================] - 3s 237us/step - loss: 0.1371 - acc: 0.9517\n",
      "Epoch 28/40\n",
      "13055/13055 [==============================] - 3s 235us/step - loss: 0.1415 - acc: 0.9529\n",
      "Epoch 29/40\n",
      "13055/13055 [==============================] - 3s 237us/step - loss: 0.1379 - acc: 0.9516\n",
      "Epoch 30/40\n",
      "13055/13055 [==============================] - 3s 239us/step - loss: 0.1272 - acc: 0.9547\n",
      "Epoch 31/40\n",
      "13055/13055 [==============================] - 3s 237us/step - loss: 0.1311 - acc: 0.9549\n",
      "Epoch 32/40\n",
      "13055/13055 [==============================] - 3s 238us/step - loss: 0.1316 - acc: 0.9549\n",
      "Epoch 33/40\n",
      "13055/13055 [==============================] - 3s 240us/step - loss: 0.1175 - acc: 0.9618\n",
      "Epoch 34/40\n",
      "13055/13055 [==============================] - 3s 239us/step - loss: 0.1267 - acc: 0.9573\n",
      "Epoch 35/40\n",
      "13055/13055 [==============================] - 3s 238us/step - loss: 0.1179 - acc: 0.9589\n",
      "Epoch 36/40\n",
      "13055/13055 [==============================] - 3s 239us/step - loss: 0.1284 - acc: 0.9580\n",
      "Epoch 37/40\n",
      "13055/13055 [==============================] - 3s 240us/step - loss: 0.1197 - acc: 0.9592\n",
      "Epoch 38/40\n",
      "13055/13055 [==============================] - 3s 241us/step - loss: 0.1188 - acc: 0.9575\n",
      "Epoch 39/40\n",
      "13055/13055 [==============================] - 3s 240us/step - loss: 0.1221 - acc: 0.9617\n",
      "Epoch 40/40\n",
      "13055/13055 [==============================] - 3s 247us/step - loss: 0.1207 - acc: 0.9596\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_8 (InputLayer)            (None, 4, 4, 11)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_43 (Conv2D)              (None, 4, 4, 128)    5760        input_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_44 (Conv2D)              (None, 4, 4, 128)    5760        input_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_45 (Conv2D)              (None, 4, 4, 128)    1536        input_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_46 (Conv2D)              (None, 4, 4, 128)    5760        input_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_47 (Conv2D)              (None, 4, 4, 128)    12800       input_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_48 (Conv2D)              (None, 4, 4, 128)    22656       input_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "flatten_43 (Flatten)            (None, 2048)         0           conv2d_43[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "flatten_44 (Flatten)            (None, 2048)         0           conv2d_44[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "flatten_45 (Flatten)            (None, 2048)         0           conv2d_45[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "flatten_46 (Flatten)            (None, 2048)         0           conv2d_46[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "flatten_47 (Flatten)            (None, 2048)         0           conv2d_47[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "flatten_48 (Flatten)            (None, 2048)         0           conv2d_48[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_8 (Concatenate)     (None, 12288)        0           flatten_43[0][0]                 \n",
      "                                                                 flatten_44[0][0]                 \n",
      "                                                                 flatten_45[0][0]                 \n",
      "                                                                 flatten_46[0][0]                 \n",
      "                                                                 flatten_47[0][0]                 \n",
      "                                                                 flatten_48[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_22 (Activation)      (None, 12288)        0           concatenate_8[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_22 (Dense)                (None, 512)          6291968     activation_22[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_23 (BatchNo (None, 512)          2048        dense_22[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_23 (Activation)      (None, 512)          0           batch_normalization_23[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_15 (Dropout)            (None, 512)          0           activation_23[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_23 (Dense)                (None, 128)          65664       dropout_15[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_24 (BatchNo (None, 128)          512         dense_23[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_24 (Activation)      (None, 128)          0           batch_normalization_24[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_16 (Dropout)            (None, 128)          0           activation_24[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_24 (Dense)                (None, 4)            516         dropout_16[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 6,414,980\n",
      "Trainable params: 6,413,700\n",
      "Non-trainable params: 1,280\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "13579/13579 [==============================] - 5s 368us/step - loss: 1.2594 - acc: 0.3833\n",
      "Epoch 2/40\n",
      "13579/13579 [==============================] - 3s 238us/step - loss: 1.1396 - acc: 0.4366\n",
      "Epoch 3/40\n",
      "13579/13579 [==============================] - 3s 239us/step - loss: 1.0847 - acc: 0.4812\n",
      "Epoch 4/40\n",
      "13579/13579 [==============================] - 3s 240us/step - loss: 1.0326 - acc: 0.5119\n",
      "Epoch 5/40\n",
      "13579/13579 [==============================] - 3s 239us/step - loss: 0.9690 - acc: 0.5534\n",
      "Epoch 6/40\n",
      "13579/13579 [==============================] - 3s 238us/step - loss: 0.9065 - acc: 0.5889\n",
      "Epoch 7/40\n",
      "13579/13579 [==============================] - 3s 239us/step - loss: 0.8329 - acc: 0.6324\n",
      "Epoch 8/40\n",
      "13579/13579 [==============================] - 3s 239us/step - loss: 0.7427 - acc: 0.6802\n",
      "Epoch 9/40\n",
      "13579/13579 [==============================] - 3s 237us/step - loss: 0.6630 - acc: 0.7238\n",
      "Epoch 10/40\n",
      "13579/13579 [==============================] - 3s 237us/step - loss: 0.5878 - acc: 0.7616\n",
      "Epoch 11/40\n",
      "13579/13579 [==============================] - 3s 236us/step - loss: 0.5081 - acc: 0.7939\n",
      "Epoch 12/40\n",
      "13579/13579 [==============================] - 3s 236us/step - loss: 0.4520 - acc: 0.8176\n",
      "Epoch 13/40\n",
      "13579/13579 [==============================] - 3s 238us/step - loss: 0.3899 - acc: 0.8479\n",
      "Epoch 14/40\n",
      "13579/13579 [==============================] - 3s 239us/step - loss: 0.3497 - acc: 0.8672\n",
      "Epoch 15/40\n",
      "13579/13579 [==============================] - 3s 240us/step - loss: 0.3107 - acc: 0.8815\n",
      "Epoch 16/40\n",
      "13579/13579 [==============================] - 3s 238us/step - loss: 0.2765 - acc: 0.8960\n",
      "Epoch 17/40\n",
      "13579/13579 [==============================] - 3s 236us/step - loss: 0.2497 - acc: 0.9077\n",
      "Epoch 18/40\n",
      "13579/13579 [==============================] - 3s 236us/step - loss: 0.2310 - acc: 0.9152\n",
      "Epoch 19/40\n",
      "13579/13579 [==============================] - 3s 238us/step - loss: 0.2120 - acc: 0.9201\n",
      "Epoch 20/40\n",
      "13579/13579 [==============================] - 3s 237us/step - loss: 0.1936 - acc: 0.9303\n",
      "Epoch 21/40\n",
      "13579/13579 [==============================] - 3s 241us/step - loss: 0.1889 - acc: 0.9320\n",
      "Epoch 22/40\n",
      "13579/13579 [==============================] - 3s 237us/step - loss: 0.1752 - acc: 0.9371\n",
      "Epoch 23/40\n",
      "13579/13579 [==============================] - 3s 243us/step - loss: 0.1679 - acc: 0.9425\n",
      "Epoch 24/40\n",
      "13579/13579 [==============================] - 3s 237us/step - loss: 0.1626 - acc: 0.9413\n",
      "Epoch 25/40\n",
      "13579/13579 [==============================] - 3s 235us/step - loss: 0.1569 - acc: 0.9437\n",
      "Epoch 26/40\n",
      "13579/13579 [==============================] - 3s 234us/step - loss: 0.1504 - acc: 0.9468\n",
      "Epoch 27/40\n",
      "13579/13579 [==============================] - 3s 234us/step - loss: 0.1503 - acc: 0.9487\n",
      "Epoch 28/40\n",
      "13579/13579 [==============================] - 3s 233us/step - loss: 0.1512 - acc: 0.9482\n",
      "Epoch 29/40\n",
      "13579/13579 [==============================] - 3s 233us/step - loss: 0.1399 - acc: 0.9496\n",
      "Epoch 30/40\n",
      "13579/13579 [==============================] - 3s 234us/step - loss: 0.1378 - acc: 0.9546\n",
      "Epoch 31/40\n",
      "13579/13579 [==============================] - 3s 234us/step - loss: 0.1434 - acc: 0.9516\n",
      "Epoch 32/40\n",
      "13579/13579 [==============================] - 3s 240us/step - loss: 0.1444 - acc: 0.9503\n",
      "Epoch 33/40\n",
      "13579/13579 [==============================] - 3s 236us/step - loss: 0.1336 - acc: 0.9522\n",
      "Epoch 34/40\n",
      "13579/13579 [==============================] - 3s 237us/step - loss: 0.1274 - acc: 0.9565\n",
      "Epoch 35/40\n",
      "13579/13579 [==============================] - 3s 238us/step - loss: 0.1349 - acc: 0.9558\n",
      "Epoch 36/40\n",
      "13579/13579 [==============================] - 3s 239us/step - loss: 0.1312 - acc: 0.9570\n",
      "Epoch 37/40\n",
      "13579/13579 [==============================] - 3s 234us/step - loss: 0.1362 - acc: 0.9537\n",
      "Epoch 38/40\n",
      "13579/13579 [==============================] - 3s 235us/step - loss: 0.1428 - acc: 0.9538\n",
      "Epoch 39/40\n",
      "13579/13579 [==============================] - 3s 238us/step - loss: 0.1280 - acc: 0.9567\n",
      "Epoch 40/40\n",
      "13579/13579 [==============================] - 3s 237us/step - loss: 0.1256 - acc: 0.9588\n"
     ]
    }
   ],
   "source": [
    "first_train('256',8)\n",
    "first_train('512',9)\n",
    "first_train('1024_1',10)\n",
    "first_train('1024_2',10)\n",
    "first_train('2048_1',11)\n",
    "first_train('2048_2',11)\n",
    "first_train('2048_3',11)\n",
    "first_train('2048_4',11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "second_train('256',8,128,256,0,256,0,256)\n",
    "second_train('512',9,256,512,0,512,0,512)\n",
    "second_train('1024_1',10,512,1024,0,256,0,256)\n",
    "second_train('1024_2',10,512,1024,256,513,0,512)\n",
    "second_train('2048_1',11,1024,2048,0,256,0,256)\n",
    "second_train('2048_2',11,1024,2048,256,512,0,257)\n",
    "second_train('2048_3',11,1024,2048,512,1024,0,256)\n",
    "second_train('2048_4',11,1024,2048,512,1024,256,513)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded expectmax lib for 2048: /home/ubuntu/2048-api/game2048/expectimax/bin/2048.so\n",
      "85 106\n",
      "Epoch 1/1\n",
      "6419/6419 [==============================] - 4s 583us/step - loss: 0.9657 - acc: 0.5375\n",
      "93 110\n",
      "Epoch 1/1\n",
      "6424/6424 [==============================] - 4s 572us/step - loss: 0.9812 - acc: 0.5243\n",
      "88 104\n",
      "Epoch 1/1\n",
      "6425/6425 [==============================] - 4s 556us/step - loss: 0.9572 - acc: 0.5387\n",
      "78 100\n",
      "Epoch 1/1\n",
      "6405/6405 [==============================] - 4s 577us/step - loss: 0.9797 - acc: 0.5286\n",
      "83 105\n",
      "Epoch 1/1\n",
      "6427/6427 [==============================] - 4s 623us/step - loss: 0.9804 - acc: 0.5211\n"
     ]
    }
   ],
   "source": [
    "while 1:\n",
    "    second_train('256',8,128,256,0,256,0,256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 4, 4, 8)      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 4, 4, 256)    8448        input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 4, 4, 256)    8448        input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 4, 4, 256)    2304        input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 4, 4, 256)    8448        input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 4, 4, 256)    18688       input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, 4, 4, 256)    33024       input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 4096)         0           conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "flatten_2 (Flatten)             (None, 4096)         0           conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "flatten_3 (Flatten)             (None, 4096)         0           conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "flatten_4 (Flatten)             (None, 4096)         0           conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "flatten_5 (Flatten)             (None, 4096)         0           conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "flatten_6 (Flatten)             (None, 4096)         0           conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 24576)        0           flatten_1[0][0]                  \n",
      "                                                                 flatten_2[0][0]                  \n",
      "                                                                 flatten_3[0][0]                  \n",
      "                                                                 flatten_4[0][0]                  \n",
      "                                                                 flatten_5[0][0]                  \n",
      "                                                                 flatten_6[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 24576)        0           concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 2048)         50333696    activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 2048)         8192        dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 2048)         0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 2048)         0           activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 512)          1049088     dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 512)          2048        dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 512)          0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 512)          0           activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 128)          65664       dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 128)          512         dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 128)          0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 128)          0           activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 4)            516         dropout_3[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 51,539,076\n",
      "Trainable params: 51,533,700\n",
      "Non-trainable params: 5,376\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/20\n",
      "64001/64001 [==============================] - 41s 643us/step - loss: 1.1514 - acc: 0.4273\n",
      "Epoch 2/20\n",
      "64001/64001 [==============================] - 38s 591us/step - loss: 1.0393 - acc: 0.4794\n",
      "Epoch 3/20\n",
      "64001/64001 [==============================] - 38s 591us/step - loss: 0.9760 - acc: 0.5230\n",
      "Epoch 4/20\n",
      "64001/64001 [==============================] - 38s 592us/step - loss: 0.9193 - acc: 0.5585\n",
      "Epoch 5/20\n",
      "64001/64001 [==============================] - 38s 590us/step - loss: 0.8529 - acc: 0.5991\n",
      "Epoch 6/20\n",
      "64001/64001 [==============================] - 38s 591us/step - loss: 0.7853 - acc: 0.6377\n",
      "Epoch 7/20\n",
      "64001/64001 [==============================] - 38s 590us/step - loss: 0.7229 - acc: 0.6711\n",
      "Epoch 8/20\n",
      "64001/64001 [==============================] - 38s 590us/step - loss: 0.6634 - acc: 0.7009\n",
      "Epoch 9/20\n",
      "64001/64001 [==============================] - 38s 591us/step - loss: 0.6040 - acc: 0.7358\n",
      "Epoch 10/20\n",
      "64001/64001 [==============================] - 38s 591us/step - loss: 0.5453 - acc: 0.7623\n",
      "Epoch 11/20\n",
      "64001/64001 [==============================] - 38s 591us/step - loss: 0.4921 - acc: 0.7893\n",
      "Epoch 12/20\n",
      "64001/64001 [==============================] - 38s 590us/step - loss: 0.4385 - acc: 0.8156\n",
      "Epoch 13/20\n",
      "64001/64001 [==============================] - 38s 591us/step - loss: 0.3909 - acc: 0.8408\n",
      "Epoch 14/20\n",
      "64001/64001 [==============================] - 38s 590us/step - loss: 0.3434 - acc: 0.8611\n",
      "Epoch 15/20\n",
      "64001/64001 [==============================] - 38s 591us/step - loss: 0.3031 - acc: 0.8807\n",
      "Epoch 16/20\n",
      "64001/64001 [==============================] - 38s 589us/step - loss: 0.2699 - acc: 0.8940\n",
      "Epoch 17/20\n",
      "64001/64001 [==============================] - 38s 593us/step - loss: 0.2438 - acc: 0.9054\n",
      "Epoch 18/20\n",
      "64001/64001 [==============================] - 38s 591us/step - loss: 0.2183 - acc: 0.9168\n",
      "Epoch 19/20\n",
      "64001/64001 [==============================] - 38s 590us/step - loss: 0.1977 - acc: 0.9257\n",
      "Epoch 20/20\n",
      "64001/64001 [==============================] - 38s 591us/step - loss: 0.1823 - acc: 0.9333\n"
     ]
    }
   ],
   "source": [
    "first_train('256',8)           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded expectmax lib for 2048: /home/ubuntu/2048-api/game2048/expectimax/bin/2048.so\n",
      "76\n",
      "164\n",
      "250\n",
      "330\n",
      "417\n",
      "508\n",
      "593\n",
      "675\n",
      "761\n",
      "830\n",
      "928\n",
      "1018\n",
      "1101\n",
      "1190\n",
      "1271\n",
      "1347\n",
      "1427\n",
      "1520\n",
      "1624\n",
      "1704\n",
      "1772\n",
      "1847\n",
      "1938\n",
      "2031\n",
      "2146\n",
      "2253\n",
      "2348\n",
      "2423\n",
      "2535\n",
      "2623\n",
      "2715\n",
      "2817\n",
      "2906\n",
      "2990\n",
      "3090\n",
      "3183\n",
      "3280\n",
      "3363\n",
      "3448\n",
      "3530\n",
      "3627\n",
      "3703\n",
      "3798\n",
      "3894\n",
      "3985\n",
      "4071\n",
      "4145\n",
      "4236\n",
      "4326\n",
      "4416\n",
      "4495\n",
      "4582\n",
      "4703\n",
      "4790\n",
      "4897\n",
      "4977\n",
      "5067\n",
      "5157\n",
      "5242\n",
      "5331\n",
      "5405\n",
      "5502\n",
      "5601\n",
      "5693\n",
      "5783\n",
      "5855\n",
      "5944\n",
      "6023\n",
      "6110\n",
      "6214\n",
      "6314\n",
      "6424\n",
      "6518\n",
      "6598\n",
      "6671\n",
      "6761\n",
      "6849\n",
      "6948\n",
      "7038\n",
      "7129\n",
      "7219\n",
      "7300\n",
      "7394\n",
      "7482\n",
      "7562\n",
      "7650\n",
      "7740\n",
      "7829\n",
      "7900\n",
      "7986\n",
      "8074\n",
      "8175\n",
      "8258\n",
      "8341\n",
      "8424\n",
      "8511\n",
      "8583\n",
      "8663\n",
      "8772\n",
      "8855\n",
      "8927\n",
      "9020\n",
      "9108\n",
      "9198\n",
      "9285\n",
      "9378\n",
      "9470\n",
      "9555\n",
      "9636\n",
      "9722\n",
      "9808\n",
      "9891\n",
      "9982\n",
      "10092\n",
      "10178\n",
      "10268\n",
      "10358\n",
      "10451\n",
      "10543\n",
      "10636\n",
      "10720\n",
      "10804\n",
      "10894\n",
      "10976\n",
      "11056\n",
      "11169\n",
      "11251\n",
      "11336\n",
      "11420\n",
      "11513\n",
      "11604\n",
      "11684\n",
      "11766\n",
      "11852\n",
      "11923\n",
      "12027\n",
      "12112\n",
      "12199\n",
      "12293\n",
      "12375\n",
      "12462\n",
      "12550\n",
      "12642\n",
      "12732\n",
      "12828\n",
      "12913\n",
      "12966\n",
      "13051\n",
      "13135\n",
      "13220\n",
      "13306\n",
      "13379\n",
      "13476\n",
      "13571\n",
      "13663\n",
      "13744\n",
      "13849\n",
      "13928\n",
      "14029\n",
      "14108\n",
      "14182\n",
      "14257\n",
      "14356\n",
      "14441\n",
      "14536\n",
      "14616\n",
      "14700\n",
      "14794\n",
      "14880\n",
      "14953\n",
      "15038\n",
      "15129\n",
      "15241\n",
      "15315\n",
      "15407\n",
      "15508\n",
      "15596\n",
      "15676\n",
      "15760\n",
      "15845\n",
      "15938\n",
      "16033\n",
      "16115\n",
      "16179\n",
      "16261\n",
      "16349\n",
      "16450\n",
      "16533\n",
      "16616\n",
      "16696\n",
      "16799\n",
      "16897\n",
      "16993\n",
      "17069\n",
      "17140\n",
      "17214\n",
      "17308\n",
      "17396\n",
      "17480\n",
      "17574\n",
      "17655\n",
      "17736\n",
      "17822\n",
      "17915\n",
      "17992\n",
      "18070\n",
      "18153\n",
      "18228\n",
      "18327\n",
      "18427\n",
      "18513\n",
      "18601\n",
      "18674\n",
      "18745\n",
      "18836\n",
      "18934\n",
      "19013\n",
      "19095\n",
      "19173\n"
     ]
    }
   ],
   "source": [
    "if 1:\n",
    "    item=128000\n",
    "    \n",
    "    \n",
    "    \n",
    "    x2_train=[]\n",
    "    y2_train=[]\n",
    "    \n",
    "    \n",
    "    \n",
    "    while 1:\n",
    "        game = Game(4, score_to_win=2048, random=False)\n",
    "        agent = ExpectiMaxAgent(game, display=display)\n",
    "        \n",
    "        \n",
    "        \n",
    "        while game.end==False:\n",
    "            tmp=[]\n",
    "            tmp=(game.board)\n",
    "            tmp[tmp==0]=1\n",
    "            \n",
    "            \n",
    "            \n",
    "            a=tmp.reshape(16)\n",
    "            b=np.sort(a)\n",
    "            a1=b[-1]\n",
    "            a2=b[-2]\n",
    "            a3=b[-3]\n",
    "            tmp=np.log2(tmp)\n",
    "            tmp=tmp.tolist()\n",
    "            if a1<256:\n",
    "                game.move(agent.step())\n",
    "                continue\n",
    "            elif a1<512:\n",
    "                if len(x2_train)>item:\n",
    "                    game.move(agent.step())\n",
    "                    continue\n",
    "                x2_train.append(tmp)\n",
    "                y2_train.append(agent.step())\n",
    "            else:\n",
    "                break\n",
    "            game.move(agent.step())\n",
    "        print(len(x2_train))\n",
    "        if len(x2_train)>=item:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x2_train=np.array(x2_train,dtype=float)\n",
    "y2_train=np.array(y2_train,dtype=float)\n",
    "p = np.random.permutation(range(len(x2_train)))\n",
    "x2_train,y2_train = x2_train[p],y2_train[p]\n",
    "x2_train=x2_train.astype('float32')\n",
    "x2_train=to_categorical(x2_train,9)\n",
    "y2_train=to_categorical(y2_train)\n",
    "np.save('x_512.npy',x2_train)\n",
    "np.save('y_512.npy',y2_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_train('512',9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "91 98\n",
      "177 200\n",
      "269 300\n",
      "344 407\n",
      "432 480\n",
      "522 571\n",
      "599 662\n",
      "681 749\n",
      "756 879\n",
      "828 990\n",
      "914 1088\n",
      "987 1187\n",
      "1098 1260\n",
      "1177 1349\n",
      "1267 1424\n",
      "1350 1499\n",
      "1427 1607\n",
      "1491 1713\n",
      "1573 1799\n",
      "1661 1880\n",
      "1746 1968\n",
      "1837 2057\n",
      "1912 2137\n",
      "2005 2230\n",
      "2098 2324\n",
      "2178 2404\n",
      "2273 2482\n",
      "2315 2580\n",
      "2414 2664\n",
      "2500 2750\n",
      "2573 2840\n",
      "2653 2931\n",
      "2746 3028\n",
      "2838 3115\n",
      "2912 3195\n",
      "3012 3286\n",
      "3090 3385\n",
      "3151 3476\n",
      "3250 3560\n",
      "3342 3638\n",
      "3433 3724\n",
      "3518 3809\n",
      "3607 3907\n",
      "3672 3993\n",
      "3757 4084\n",
      "3837 4179\n",
      "3912 4270\n",
      "4024 4346\n",
      "4115 4427\n",
      "4173 4531\n",
      "4267 4613\n",
      "4362 4701\n",
      "4454 4791\n",
      "4553 4874\n",
      "4634 4963\n",
      "4752 5023\n",
      "4824 5120\n",
      "4909 5209\n",
      "4996 5302\n",
      "5089 5378\n",
      "5169 5477\n",
      "5257 5571\n",
      "5331 5676\n",
      "5418 5749\n",
      "5509 5840\n",
      "5599 5919\n",
      "5670 6010\n",
      "5759 6092\n",
      "5848 6170\n",
      "5924 6267\n",
      "6018 6355\n",
      "6094 6446\n",
      "6187 6521\n",
      "6264 6625\n",
      "6348 6718\n",
      "6436 6808\n",
      "6529 6905\n",
      "6610 6999\n",
      "6678 7099\n",
      "6752 7197\n",
      "6820 7292\n",
      "6904 7377\n",
      "6970 7461\n",
      "7052 7549\n",
      "7136 7639\n",
      "7217 7731\n",
      "7303 7822\n",
      "7386 7921\n",
      "7463 8006\n",
      "7547 8086\n",
      "7635 8169\n",
      "7720 8243\n",
      "7797 8340\n",
      "7902 8403\n",
      "7977 8494\n",
      "8079 8569\n",
      "8151 8660\n",
      "8216 8752\n",
      "8301 8842\n",
      "8397 8920\n",
      "8481 9001\n",
      "8558 9085\n",
      "8653 9178\n",
      "8740 9292\n",
      "8822 9379\n",
      "8903 9473\n",
      "8978 9548\n",
      "9069 9646\n",
      "9145 9741\n",
      "9227 9828\n",
      "9323 9901\n",
      "9398 9990\n",
      "9495 10095\n",
      "9589 10185\n",
      "9660 10276\n",
      "9762 10363\n",
      "9839 10467\n",
      "9928 10544\n",
      "9994 10637\n",
      "10079 10718\n",
      "10177 10803\n",
      "10252 10905\n",
      "10359 10978\n",
      "10448 11057\n",
      "10535 11148\n",
      "10634 11224\n",
      "10718 11315\n",
      "10814 11398\n",
      "10903 11488\n",
      "10993 11652\n",
      "11072 11746\n",
      "11162 11845\n",
      "11272 11903\n",
      "11358 12003\n",
      "11441 12095\n",
      "11525 12176\n",
      "11633 12245\n",
      "11723 12331\n",
      "11807 12427\n",
      "11899 12501\n",
      "11979 12592\n",
      "12047 12682\n",
      "12154 12760\n",
      "12225 12857\n",
      "12327 12949\n",
      "12419 13028\n",
      "12512 13100\n",
      "12593 13198\n",
      "12681 13286\n",
      "12744 13381\n",
      "12837 13477\n",
      "12906 13573\n",
      "12985 13682\n",
      "13061 13771\n",
      "13165 13834\n",
      "13224 13921\n",
      "13306 14006\n",
      "13381 14099\n",
      "13475 14187\n",
      "13559 14269\n",
      "13610 14350\n",
      "13685 14433\n",
      "13777 14523\n",
      "13859 14621\n",
      "13926 14728\n",
      "14006 14814\n",
      "14090 14912\n",
      "14168 15001\n",
      "14257 15090\n",
      "14347 15162\n",
      "14433 15251\n",
      "14521 15333\n",
      "14609 15419\n",
      "14693 15514\n",
      "14784 15584\n",
      "14864 15666\n",
      "14950 15767\n",
      "15072 15818\n",
      "15159 15902\n",
      "15243 15989\n",
      "15317 16069\n",
      "15404 16156\n",
      "15498 16230\n",
      "15583 16322\n",
      "15657 16417\n",
      "15721 16506\n",
      "15827 16579\n",
      "15921 16663\n",
      "16011 16731\n",
      "16122 16798\n",
      "16204 16900\n",
      "16284 17013\n",
      "16358 17109\n",
      "16446 17218\n",
      "16545 17298\n",
      "16619 17404\n",
      "16718 17488\n",
      "16806 17569\n",
      "16893 17654\n",
      "16977 17738\n",
      "17053 17829\n",
      "17130 17918\n",
      "17213 17968\n",
      "17321 18049\n",
      "17416 18126\n",
      "17484 18222\n",
      "17573 18304\n",
      "17644 18396\n",
      "17724 18482\n",
      "17817 18564\n",
      "17900 18660\n",
      "17975 18769\n",
      "18077 18849\n",
      "18160 18927\n",
      "18237 19061\n",
      "18322 19143\n",
      "18402 19226\n",
      "18478 19320\n",
      "18566 19406\n",
      "18640 19506\n",
      "18717 19601\n",
      "18808 19702\n",
      "18921 19759\n",
      "19014 19823\n",
      "19090 19923\n",
      "19179 19998\n",
      "19270 20087\n",
      "19349 20184\n",
      "19434 20279\n",
      "19514 20363\n",
      "19600 20453\n",
      "19686 20531\n",
      "19771 20613\n",
      "19848 20738\n",
      "19919 20837\n",
      "20000 20919\n",
      "20097 21048\n",
      "20185 21118\n",
      "20268 21205\n",
      "20343 21279\n",
      "20439 21359\n",
      "20529 21463\n",
      "20621 21547\n",
      "20696 21638\n",
      "20793 21723\n",
      "20878 21811\n",
      "20969 21892\n",
      "21057 21968\n",
      "21146 22052\n",
      "21232 22137\n",
      "21298 22225\n",
      "21379 22321\n",
      "21467 22408\n",
      "21548 22489\n",
      "21629 22584\n",
      "21731 22666\n",
      "21830 22748\n",
      "21915 22845\n",
      "22000 22937\n",
      "22081 23025\n",
      "22166 23114\n",
      "22257 23189\n",
      "22333 23281\n",
      "22411 23375\n",
      "22504 23463\n",
      "22610 23556\n",
      "22702 23659\n",
      "22789 23740\n",
      "22886 23827\n",
      "22974 23908\n",
      "23059 23987\n",
      "23152 24066\n",
      "23241 24157\n",
      "23316 24247\n",
      "23414 24326\n",
      "23511 24404\n",
      "23600 24479\n",
      "23681 24575\n",
      "23770 24657\n",
      "23858 24745\n",
      "23939 24837\n",
      "24024 24911\n",
      "24102 25010\n",
      "24186 25099\n",
      "24265 25208\n",
      "24339 25313\n",
      "24431 25403\n",
      "24523 25485\n",
      "24616 25567\n",
      "24723 25623\n",
      "24800 25711\n",
      "24894 25798\n",
      "24981 25876\n",
      "25063 25966\n",
      "25151 26056\n",
      "25226 26145\n",
      "25309 26235\n",
      "25395 26332\n",
      "25481 26398\n",
      "25573 26481\n",
      "25661 26564\n",
      "25750 26643\n",
      "25839 26744\n",
      "25915 26841\n",
      "25994 26950\n",
      "26088 27039\n",
      "26177 27125\n",
      "26259 27201\n",
      "26344 27285\n",
      "26438 27353\n",
      "26531 27431\n",
      "26623 27510\n",
      "26693 27604\n",
      "26781 27696\n",
      "26850 27779\n",
      "26925 27870\n",
      "27006 27961\n",
      "27099 28053\n",
      "27176 28155\n",
      "27268 28261\n",
      "27367 28338\n",
      "27452 28430\n",
      "27534 28504\n",
      "27619 28593\n",
      "27700 28692\n",
      "27780 28805\n",
      "27869 28890\n",
      "27948 28975\n",
      "28027 29065\n",
      "28112 29140\n",
      "28177 29225\n",
      "28255 29325\n",
      "28340 29414\n",
      "28422 29510\n",
      "28490 29585\n",
      "28581 29652\n",
      "28661 29745\n",
      "28743 29838\n",
      "28836 29923\n",
      "28921 30024\n",
      "28999 30115\n",
      "29068 30195\n",
      "29153 30288\n",
      "29236 30379\n",
      "29301 30492\n",
      "29384 30603\n",
      "29483 30681\n",
      "29567 30774\n",
      "29651 30862\n",
      "29748 30952\n",
      "29853 31021\n",
      "29947 31099\n",
      "30031 31204\n",
      "30124 31280\n",
      "30210 31369\n",
      "30284 31457\n",
      "30370 31556\n",
      "30478 31617\n",
      "30566 31703\n",
      "30656 31796\n",
      "30742 31868\n",
      "30823 31959\n",
      "30922 32033\n",
      "30997 32110\n",
      "31072 32198\n",
      "31145 32302\n",
      "31254 32366\n",
      "31339 32490\n",
      "31403 32584\n",
      "31485 32671\n",
      "31576 32750\n",
      "31669 32826\n",
      "31753 32925\n",
      "31851 33048\n",
      "31936 33134\n",
      "32013 33232\n",
      "32082 33310\n",
      "32159 33409\n",
      "32236 33482\n",
      "32330 33553\n",
      "32414 33648\n",
      "32490 33747\n",
      "32580 33820\n",
      "32653 33914\n",
      "32741 34002\n",
      "32819 34089\n",
      "32921 34173\n",
      "33004 34243\n",
      "33092 34316\n",
      "33181 34406\n",
      "33253 34496\n",
      "33327 34602\n",
      "33405 34699\n",
      "33478 34792\n",
      "33552 34884\n",
      "33621 34976\n",
      "33698 35071\n",
      "33783 35170\n",
      "33871 35271\n",
      "33955 35363\n",
      "34036 35442\n",
      "34120 35525\n",
      "34208 35599\n",
      "34305 35689\n",
      "34383 35778\n",
      "34451 35861\n",
      "34528 35955\n",
      "34614 36046\n",
      "34698 36142\n",
      "34790 36226\n",
      "34862 36319\n",
      "34944 36419\n",
      "35017 36512\n",
      "35098 36596\n",
      "35183 36678\n",
      "35272 36811\n",
      "35353 36906\n",
      "35433 37007\n",
      "35507 37102\n",
      "35600 37194\n",
      "35689 37279\n",
      "35780 37348\n",
      "35853 37446\n",
      "35939 37528\n",
      "36037 37602\n",
      "36118 37680\n",
      "36193 37766\n",
      "36273 37856\n",
      "36374 37945\n",
      "36455 38042\n",
      "36534 38141\n",
      "36631 38228\n",
      "36728 38317\n",
      "36824 38401\n",
      "36917 38501\n",
      "37013 38581\n",
      "37101 38674\n",
      "37178 38780\n",
      "37279 38852\n",
      "37359 38915\n",
      "37449 39009\n",
      "37543 39097\n",
      "37633 39169\n",
      "37710 39265\n",
      "37805 39348\n",
      "37885 39438\n",
      "37967 39540\n",
      "38061 39640\n",
      "38152 39724\n",
      "38239 39814\n",
      "38338 39895\n",
      "38432 39983\n",
      "38513 40077\n",
      "38592 40177\n",
      "38723 40233\n",
      "38838 40331\n",
      "38912 40420\n",
      "38995 40508\n",
      "39092 40583\n",
      "39176 40674\n",
      "39244 40767\n",
      "39327 40853\n",
      "39416 40928\n",
      "39487 41014\n",
      "39553 41099\n",
      "39631 41194\n",
      "39721 41290\n",
      "39827 41357\n",
      "39905 41431\n",
      "39977 41523\n",
      "40064 41598\n",
      "40155 41675\n",
      "40237 41753\n",
      "40329 41828\n",
      "40427 41894\n",
      "40502 41982\n",
      "40591 42060\n",
      "40674 42145\n",
      "40760 42227\n",
      "40856 42306\n",
      "40940 42391\n",
      "41027 42484\n",
      "41108 42563\n",
      "41192 42645\n",
      "41273 42733\n",
      "41352 42827\n",
      "41436 42925\n",
      "41529 43008\n",
      "41611 43112\n",
      "41702 43193\n",
      "41796 43277\n",
      "41866 43362\n",
      "41948 43457\n",
      "42019 43557\n",
      "42097 43667\n",
      "42185 43751\n",
      "42276 43844\n",
      "42357 43925\n",
      "42414 44017\n",
      "42497 44104\n",
      "42561 44194\n",
      "42641 44294\n",
      "42725 44397\n",
      "42811 44463\n",
      "42896 44555\n",
      "42986 44646\n",
      "43076 44747\n",
      "43166 44822\n",
      "43264 44896\n",
      "43352 44986\n",
      "43435 45072\n",
      "43530 45175\n",
      "43619 45278\n",
      "43707 45374\n",
      "43803 45445\n",
      "43890 45549\n",
      "43971 45639\n",
      "44070 45698\n",
      "44172 45776\n",
      "44255 45855\n",
      "44338 45925\n",
      "44431 46002\n",
      "44516 46095\n",
      "44604 46179\n",
      "44693 46269\n",
      "44777 46356\n",
      "44869 46439\n",
      "44956 46530\n",
      "45041 46631\n",
      "45131 46719\n",
      "45206 46806\n",
      "45289 46886\n",
      "45372 46972\n",
      "45441 47073\n",
      "45523 47167\n",
      "45607 47262\n",
      "45692 47356\n",
      "45771 47462\n",
      "45846 47548\n",
      "45926 47649\n",
      "46014 47722\n",
      "46097 47816\n",
      "46183 47897\n",
      "46262 47993\n",
      "46340 48087\n",
      "46431 48156\n",
      "46517 48244\n",
      "46619 48335\n",
      "46706 48414\n",
      "46777 48493\n",
      "46866 48587\n",
      "46941 48667\n",
      "47022 48795\n",
      "47068 48876\n",
      "47167 48952\n",
      "47263 49019\n",
      "47332 49098\n",
      "47416 49194\n",
      "47488 49285\n",
      "47550 49393\n",
      "47630 49483\n",
      "47717 49574\n",
      "47799 49663\n",
      "47902 49738\n",
      "47992 49829\n",
      "48080 49915\n",
      "48161 50011\n",
      "48249 50091\n",
      "48340 50172\n",
      "48441 50242\n",
      "48520 50342\n",
      "48605 50439\n",
      "48674 50541\n",
      "48769 50642\n",
      "48863 50715\n",
      "48948 50809\n",
      "49036 50902\n",
      "49116 51001\n",
      "49202 51084\n",
      "49304 51161\n",
      "49381 51253\n",
      "49464 51359\n",
      "49549 51450\n",
      "49630 51535\n",
      "49711 51614\n",
      "49798 51696\n",
      "49878 51798\n",
      "49968 51884\n",
      "50050 51971\n",
      "50145 52050\n",
      "50235 52135\n",
      "50323 52216\n",
      "50412 52303\n",
      "50499 52381\n",
      "50590 52464\n",
      "50676 52551\n",
      "50743 52650\n",
      "50833 52733\n",
      "50916 52825\n",
      "50990 52913\n",
      "51084 52990\n",
      "51165 53086\n",
      "51260 53161\n",
      "51342 53304\n",
      "51424 53384\n",
      "51512 53482\n",
      "51602 53609\n",
      "51680 53693\n",
      "51767 53778\n",
      "51872 53859\n",
      "51974 53930\n",
      "52047 54027\n",
      "52134 54127\n",
      "52212 54215\n",
      "52312 54283\n",
      "52398 54417\n",
      "52478 54511\n",
      "52577 54593\n",
      "52663 54676\n",
      "52722 54760\n",
      "52805 54869\n",
      "52859 54963\n",
      "52955 55040\n",
      "53042 55127\n",
      "53139 55227\n",
      "53252 55272\n",
      "53332 55370\n",
      "53415 55480\n",
      "53492 55569\n",
      "53572 55649\n",
      "53659 55728\n",
      "53750 55835\n",
      "53830 55937\n",
      "53916 56019\n",
      "53996 56116\n",
      "54088 56184\n",
      "54191 56252\n",
      "54281 56343\n",
      "54374 56422\n",
      "54451 56521\n",
      "54527 56620\n",
      "54610 56713\n",
      "54693 56799\n",
      "54781 56884\n",
      "54860 56982\n",
      "54950 57070\n",
      "55034 57165\n",
      "55121 57259\n",
      "55194 57347\n",
      "55302 57419\n",
      "55389 57496\n",
      "55474 57592\n",
      "55564 57680\n",
      "55654 57772\n",
      "55745 57859\n",
      "55822 57927\n",
      "55903 58022\n",
      "56000 58121\n",
      "56090 58202\n",
      "56172 58313\n",
      "56245 58403\n",
      "56347 58463\n",
      "56426 58554\n",
      "56539 58610\n",
      "56610 58703\n",
      "56699 58776\n",
      "56784 58865\n",
      "56885 58935\n",
      "56960 59025\n",
      "57044 59112\n",
      "57138 59188\n",
      "57221 59277\n",
      "57325 59336\n",
      "57422 59419\n",
      "57512 59506\n",
      "57603 59589\n",
      "57691 59666\n",
      "57779 59766\n",
      "57856 59844\n",
      "57924 59935\n",
      "58005 60034\n",
      "58091 60126\n",
      "58180 60219\n",
      "58294 60290\n",
      "58367 60387\n",
      "58459 60474\n",
      "58551 60556\n",
      "58639 60651\n",
      "58713 60766\n",
      "58791 60861\n",
      "58879 60933\n",
      "58976 61010\n",
      "59070 61091\n",
      "59167 61164\n",
      "59253 61261\n",
      "59340 61346\n",
      "59444 61419\n",
      "59567 61461\n",
      "59638 61546\n",
      "59712 61638\n",
      "59801 61724\n",
      "59884 61810\n",
      "59978 61894\n",
      "60094 61952\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60167 62039\n",
      "60240 62124\n",
      "60336 62193\n",
      "60434 62271\n",
      "60517 62366\n",
      "60593 62461\n",
      "60723 62522\n",
      "60818 62623\n",
      "60912 62710\n",
      "61020 62774\n",
      "61107 62868\n",
      "61199 62947\n",
      "61292 63028\n",
      "61376 63105\n",
      "61458 63187\n",
      "61528 63238\n",
      "61609 63330\n",
      "61711 63397\n",
      "61802 63482\n",
      "61898 63550\n",
      "61977 63640\n",
      "62064 63730\n",
      "62150 63810\n",
      "62235 63892\n",
      "62332 63961\n",
      "62408 64027\n",
      "62476 64131\n",
      "62545 64220\n",
      "62628 64305\n",
      "62727 64379\n",
      "62818 64489\n",
      "62906 64566\n",
      "62970 64660\n",
      "63045 64758\n",
      "63122 64861\n",
      "63226 64935\n",
      "63302 65027\n",
      "63383 65120\n",
      "63460 65225\n",
      "63557 65306\n",
      "63638 65395\n",
      "63727 65483\n",
      "63815 65563\n",
      "63910 65636\n",
      "63988 65722\n",
      "64069 65824\n",
      "64162 65921\n",
      "64241 66015\n",
      "64344 66179\n",
      "64426 66291\n",
      "64524 66376\n",
      "64602 66461\n",
      "64702 66579\n",
      "64783 66664\n",
      "64877 66730\n",
      "64957 66819\n",
      "65032 66905\n",
      "65121 67002\n",
      "65207 67080\n",
      "65285 67174\n",
      "65361 67247\n",
      "65451 67346\n",
      "65534 67441\n",
      "65618 67540\n",
      "65711 67621\n",
      "65786 67714\n",
      "65874 67800\n",
      "65956 67885\n",
      "66052 67962\n",
      "66134 68066\n",
      "66247 68177\n",
      "66344 68254\n",
      "66434 68348\n",
      "66509 68447\n",
      "66599 68542\n",
      "66711 68600\n",
      "66791 68692\n",
      "66901 68753\n",
      "66990 68838\n",
      "67063 68935\n",
      "67158 69032\n",
      "67261 69103\n",
      "67331 69199\n",
      "67418 69290\n",
      "67483 69376\n",
      "67579 69463\n",
      "67666 69553\n",
      "67759 69642\n",
      "67824 69737\n",
      "67919 69832\n",
      "67996 69917\n",
      "68081 70002\n",
      "68165 70092\n",
      "68254 70186\n",
      "68344 70269\n",
      "68449 70336\n",
      "68528 70432\n",
      "68623 70517\n",
      "68710 70594\n",
      "68790 70692\n",
      "68873 70776\n",
      "68951 70871\n",
      "69044 70959\n",
      "69118 71047\n",
      "69208 71142\n",
      "69294 71222\n",
      "69370 71393\n",
      "69467 71468\n",
      "69534 71565\n",
      "69606 71656\n",
      "69685 71753\n",
      "69782 71843\n",
      "69880 71932\n",
      "69971 72009\n",
      "70055 72096\n",
      "70124 72180\n",
      "70200 72275\n",
      "70278 72368\n",
      "70358 72447\n",
      "70431 72543\n",
      "70521 72632\n",
      "70605 72745\n",
      "70693 72836\n",
      "70790 72920\n",
      "70873 73013\n",
      "70968 73094\n",
      "71037 73166\n",
      "71136 73235\n",
      "71232 73305\n",
      "71312 73388\n",
      "71417 73450\n",
      "71508 73541\n",
      "71590 73629\n",
      "71676 73725\n",
      "71791 73793\n",
      "71864 73885\n",
      "71950 73988\n",
      "72039 74072\n",
      "72128 74156\n",
      "72224 74230\n",
      "72304 74319\n",
      "72384 74408\n",
      "72499 74462\n",
      "72570 74552\n",
      "72654 74643\n",
      "72733 74743\n",
      "72845 74786\n",
      "72932 74875\n",
      "73020 74960\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-761a915eeaab>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     48\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m             \u001b[0mgame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmove\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx3_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx4_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx3_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m>=\u001b[0m\u001b[0mitem\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx4_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m>=\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/2048-api/game2048/agents.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m         \u001b[0mdirection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msearch_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mboard\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdirection\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/2048-api/game2048/expectimax/_ext.py\u001b[0m in \u001b[0;36mboard_to_move\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    108\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m         \u001b[0mret\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 110\u001b[0;31m     \u001b[0mmove\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mm_to_move\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    111\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmove\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/2048-api/game2048/expectimax/_ext.py\u001b[0m in \u001b[0;36mm_to_move\u001b[0;34m(m)\u001b[0m\n\u001b[1;32m     98\u001b[0m     \u001b[0mmine\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mldru\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m     '''\n\u001b[0;32m--> 100\u001b[0;31m     \u001b[0mmove\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfind_best_move\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    101\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmove\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/2048-api/game2048/expectimax/_ext.py\u001b[0m in \u001b[0;36mfind_best_move\u001b[0;34m(m)\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m         scores = pool.map(score_toplevel_move, [\n\u001b[0;32m---> 83\u001b[0;31m                           (board, move) for move in range(4)])\n\u001b[0m\u001b[1;32m     84\u001b[0m         \u001b[0mbestmove\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbestscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbestscore\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/multiprocessing/pool.py\u001b[0m in \u001b[0;36mmap\u001b[0;34m(self, func, iterable, chunksize)\u001b[0m\n\u001b[1;32m    264\u001b[0m         \u001b[0;32min\u001b[0m \u001b[0ma\u001b[0m \u001b[0mlist\u001b[0m \u001b[0mthat\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mreturned\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m         '''\n\u001b[0;32m--> 266\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_map_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmapstar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunksize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    267\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstarmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunksize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/multiprocessing/pool.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    636\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    637\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 638\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    639\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mready\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    640\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/multiprocessing/pool.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    633\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    634\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 635\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_event\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    636\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    637\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    549\u001b[0m             \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flag\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    550\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 551\u001b[0;31m                 \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cond\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    552\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    553\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    293\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 295\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    296\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if 1:\n",
    "    item=128000\n",
    "    \n",
    "    \n",
    "    \n",
    "    x3_train=[]\n",
    "    y3_train=[]\n",
    "    x4_train=[]\n",
    "    y4_train=[]\n",
    "    \n",
    "    \n",
    "    \n",
    "    while 1:\n",
    "        game = Game(4, score_to_win=2048, random=False)\n",
    "        agent = ExpectiMaxAgent(game, display=display)\n",
    "        \n",
    "        \n",
    "        \n",
    "        while game.end==False:\n",
    "            tmp=[]\n",
    "            tmp=(game.board)\n",
    "            tmp[tmp==0]=1\n",
    "            \n",
    "            \n",
    "            \n",
    "            a=tmp.reshape(16)\n",
    "            b=np.sort(a)\n",
    "            a1=b[-1]\n",
    "            a2=b[-2]\n",
    "            a3=b[-3]\n",
    "            tmp=np.log2(tmp)\n",
    "            tmp=tmp.tolist()\n",
    "            if a1<512:\n",
    "                game.move(agent.step())\n",
    "                continue\n",
    "            elif a1<1024 and a2<256:\n",
    "                if len(x3_train)>item:\n",
    "                    game.move(agent.step())\n",
    "                    continue\n",
    "                x3_train.append(tmp)\n",
    "                y3_train.append(agent.step())\n",
    "            elif a1<1024:\n",
    "                if len(x4_train)>item:\n",
    "                    game.move(agent.step())\n",
    "                    continue\n",
    "                x4_train.append(tmp)\n",
    "                y4_train.append(agent.step())\n",
    "            else:\n",
    "                break\n",
    "            game.move(agent.step())\n",
    "        print(len(x3_train),len(x4_train))\n",
    "        if len(x3_train)>=item or len(x4_train)>=item:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('x0_1024_1.npy',x3_train)\n",
    "np.save('y0_1024_1.npy',y3_train)\n",
    "np.save('x0_1024_2.npy',x4_train)\n",
    "np.save('y0_1024_2.npy',y4_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "x3_train=np.load('x0_1024_1.npy')\n",
    "y3_train=np.load('y0_1024_1.npy')\n",
    "x4_train=np.load('x0_1024_2.npy')\n",
    "y4_train=np.load('y0_1024_2.npy')\n",
    "x3_train=x3_train.tolist()\n",
    "y3_train=y3_train.tolist()\n",
    "x4_train=x4_train.tolist()\n",
    "y4_train=y4_train.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "x30_train=np.array(x3_train,dtype=float)\n",
    "y30_train=np.array(y3_train,dtype=float)\n",
    "l=len(x3_train)\n",
    "for i in range(l):\n",
    "    tmp=x30_train[i,:,:]\n",
    "    step=y30_train[i]\n",
    "    tmp1=[]\n",
    "    tmp2=[]\n",
    "    tmp3=[]\n",
    "    tmp4=[]\n",
    "    tmp5=[]\n",
    "    tmp1[:] = list(map(list,zip(*tmp[::-1])))\n",
    "    tmp1=np.array(tmp1,dtype=float)\n",
    "    x3_train.append(tmp1)\n",
    "    y3_train.append((step+3)%4)\n",
    "    tmp2[:] = list(map(list,zip(*tmp1[::-1])))\n",
    "    tmp2=np.array(tmp2,dtype=float)\n",
    "    x3_train.append(tmp2)\n",
    "    y3_train.append((step+2)%4)\n",
    "    tmp3[:] = list(map(list,zip(*tmp2[::-1])))\n",
    "    tmp3=np.array(tmp3,dtype=float)\n",
    "    x3_train.append(tmp3)\n",
    "    y3_train.append((step+1)%4)\n",
    "\n",
    "x3_train=np.array(x3_train,dtype=float)\n",
    "y3_train=np.array(y3_train,dtype=float)\n",
    "p = np.random.permutation(range(len(x3_train)))\n",
    "x3_train,y3_train = x3_train[p],y3_train[p]\n",
    "x3_train=x3_train.astype('float32')\n",
    "x3_train=to_categorical(x3_train,10)\n",
    "y3_train=to_categorical(y3_train)\n",
    "np.save('x_1024_1.npy',x3_train)\n",
    "np.save('y_1024_1.npy',y3_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "x40_train=np.array(x4_train,dtype=float)\n",
    "y40_train=np.array(y4_train,dtype=float)\n",
    "l=len(x4_train)\n",
    "for i in range(l):\n",
    "    tmp=x40_train[i,:,:]\n",
    "    step=y40_train[i]\n",
    "    tmp1=[]\n",
    "    tmp2=[]\n",
    "    tmp3=[]\n",
    "    tmp4=[]\n",
    "    tmp5=[]\n",
    "    tmp1[:] = list(map(list,zip(*tmp[::-1])))\n",
    "    tmp1=np.array(tmp1,dtype=float)\n",
    "    x4_train.append(tmp1)\n",
    "    y4_train.append((step+3)%4)\n",
    "    tmp2[:] = list(map(list,zip(*tmp1[::-1])))\n",
    "    tmp2=np.array(tmp2,dtype=float)\n",
    "    x4_train.append(tmp2)\n",
    "    y4_train.append((step+2)%4)\n",
    "    tmp3[:] = list(map(list,zip(*tmp2[::-1])))\n",
    "    tmp3=np.array(tmp3,dtype=float)\n",
    "    x4_train.append(tmp3)\n",
    "    y4_train.append((step+1)%4)\n",
    "\n",
    "x4_train=np.array(x4_train,dtype=float)\n",
    "y4_train=np.array(y4_train,dtype=float)\n",
    "p = np.random.permutation(range(len(x4_train)))\n",
    "x4_train,y4_train = x4_train[p],y4_train[p]\n",
    "x4_train=x4_train.astype('float32')\n",
    "x4_train=to_categorical(x4_train,10)\n",
    "y4_train=to_categorical(y4_train)\n",
    "np.save('x_1024_2.npy',x4_train)\n",
    "np.save('y_1024_2.npy',y4_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_5 (InputLayer)            (None, 4, 4, 10)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_25 (Conv2D)              (None, 4, 4, 128)    5248        input_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_26 (Conv2D)              (None, 4, 4, 128)    5248        input_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_27 (Conv2D)              (None, 4, 4, 128)    1408        input_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_28 (Conv2D)              (None, 4, 4, 128)    5248        input_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_29 (Conv2D)              (None, 4, 4, 128)    11648       input_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_30 (Conv2D)              (None, 4, 4, 128)    20608       input_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "flatten_25 (Flatten)            (None, 2048)         0           conv2d_25[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "flatten_26 (Flatten)            (None, 2048)         0           conv2d_26[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "flatten_27 (Flatten)            (None, 2048)         0           conv2d_27[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "flatten_28 (Flatten)            (None, 2048)         0           conv2d_28[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "flatten_29 (Flatten)            (None, 2048)         0           conv2d_29[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "flatten_30 (Flatten)            (None, 2048)         0           conv2d_30[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_5 (Concatenate)     (None, 12288)        0           flatten_25[0][0]                 \n",
      "                                                                 flatten_26[0][0]                 \n",
      "                                                                 flatten_27[0][0]                 \n",
      "                                                                 flatten_28[0][0]                 \n",
      "                                                                 flatten_29[0][0]                 \n",
      "                                                                 flatten_30[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_21 (Activation)      (None, 12288)        0           concatenate_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_21 (Dense)                (None, 512)          6291968     activation_21[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_22 (BatchNo (None, 512)          2048        dense_21[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_22 (Activation)      (None, 512)          0           batch_normalization_22[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_17 (Dropout)            (None, 512)          0           activation_22[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_22 (Dense)                (None, 128)          65664       dropout_17[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_23 (BatchNo (None, 128)          512         dense_22[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_23 (Activation)      (None, 128)          0           batch_normalization_23[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_18 (Dropout)            (None, 128)          0           activation_23[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_23 (Dense)                (None, 4)            516         dropout_18[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 6,410,116\n",
      "Trainable params: 6,408,836\n",
      "Non-trainable params: 1,280\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/20\n",
      "292548/292548 [==============================] - 12s 42us/step - loss: 1.1626 - acc: 0.4353\n",
      "Epoch 2/20\n",
      "292548/292548 [==============================] - 10s 35us/step - loss: 1.0743 - acc: 0.4737\n",
      "Epoch 3/20\n",
      "292548/292548 [==============================] - 10s 35us/step - loss: 1.0402 - acc: 0.4933\n",
      "Epoch 4/20\n",
      "292548/292548 [==============================] - 10s 35us/step - loss: 1.0138 - acc: 0.5102\n",
      "Epoch 5/20\n",
      "292548/292548 [==============================] - 10s 35us/step - loss: 0.9932 - acc: 0.5227\n",
      "Epoch 6/20\n",
      "292548/292548 [==============================] - 10s 35us/step - loss: 0.9748 - acc: 0.5360\n",
      "Epoch 7/20\n",
      "292548/292548 [==============================] - 10s 35us/step - loss: 0.9591 - acc: 0.5453\n",
      "Epoch 8/20\n",
      "292548/292548 [==============================] - 10s 35us/step - loss: 0.9428 - acc: 0.5563\n",
      "Epoch 9/20\n",
      "292548/292548 [==============================] - 10s 35us/step - loss: 0.9281 - acc: 0.5652\n",
      "Epoch 10/20\n",
      "292548/292548 [==============================] - 10s 36us/step - loss: 0.9118 - acc: 0.5749\n",
      "Epoch 11/20\n",
      "292548/292548 [==============================] - 10s 36us/step - loss: 0.8981 - acc: 0.5833\n",
      "Epoch 12/20\n",
      "292548/292548 [==============================] - 10s 36us/step - loss: 0.8845 - acc: 0.5915\n",
      "Epoch 13/20\n",
      "292548/292548 [==============================] - 10s 36us/step - loss: 0.8722 - acc: 0.5991\n",
      "Epoch 14/20\n",
      "292548/292548 [==============================] - 10s 36us/step - loss: 0.8566 - acc: 0.6081\n",
      "Epoch 15/20\n",
      "292548/292548 [==============================] - 10s 36us/step - loss: 0.8462 - acc: 0.6122\n",
      "Epoch 16/20\n",
      "292548/292548 [==============================] - 10s 35us/step - loss: 0.8327 - acc: 0.6202\n",
      "Epoch 17/20\n",
      "292548/292548 [==============================] - 10s 36us/step - loss: 0.8204 - acc: 0.6277\n",
      "Epoch 18/20\n",
      "292548/292548 [==============================] - 10s 36us/step - loss: 0.8092 - acc: 0.6327\n",
      "Epoch 19/20\n",
      "292548/292548 [==============================] - 10s 36us/step - loss: 0.7997 - acc: 0.6377\n",
      "Epoch 20/20\n",
      "292548/292548 [==============================] - 10s 36us/step - loss: 0.7890 - acc: 0.6426\n"
     ]
    }
   ],
   "source": [
    "first_train('1024_1',10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_6 (InputLayer)            (None, 4, 4, 10)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_31 (Conv2D)              (None, 4, 4, 128)    5248        input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_32 (Conv2D)              (None, 4, 4, 128)    5248        input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_33 (Conv2D)              (None, 4, 4, 128)    1408        input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_34 (Conv2D)              (None, 4, 4, 128)    5248        input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_35 (Conv2D)              (None, 4, 4, 128)    11648       input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_36 (Conv2D)              (None, 4, 4, 128)    20608       input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "flatten_31 (Flatten)            (None, 2048)         0           conv2d_31[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "flatten_32 (Flatten)            (None, 2048)         0           conv2d_32[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "flatten_33 (Flatten)            (None, 2048)         0           conv2d_33[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "flatten_34 (Flatten)            (None, 2048)         0           conv2d_34[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "flatten_35 (Flatten)            (None, 2048)         0           conv2d_35[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "flatten_36 (Flatten)            (None, 2048)         0           conv2d_36[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_6 (Concatenate)     (None, 12288)        0           flatten_31[0][0]                 \n",
      "                                                                 flatten_32[0][0]                 \n",
      "                                                                 flatten_33[0][0]                 \n",
      "                                                                 flatten_34[0][0]                 \n",
      "                                                                 flatten_35[0][0]                 \n",
      "                                                                 flatten_36[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_24 (Activation)      (None, 12288)        0           concatenate_6[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_24 (Dense)                (None, 512)          6291968     activation_24[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_25 (BatchNo (None, 512)          2048        dense_24[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_25 (Activation)      (None, 512)          0           batch_normalization_25[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_19 (Dropout)            (None, 512)          0           activation_25[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_25 (Dense)                (None, 128)          65664       dropout_19[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_26 (BatchNo (None, 128)          512         dense_25[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_26 (Activation)      (None, 128)          0           batch_normalization_26[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_20 (Dropout)            (None, 128)          0           activation_26[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_26 (Dense)                (None, 4)            516         dropout_20[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 6,410,116\n",
      "Trainable params: 6,408,836\n",
      "Non-trainable params: 1,280\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/20\n",
      "299972/299972 [==============================] - 13s 42us/step - loss: 1.1534 - acc: 0.4421\n",
      "Epoch 2/20\n",
      "299972/299972 [==============================] - 11s 36us/step - loss: 1.0661 - acc: 0.4812\n",
      "Epoch 3/20\n",
      "299972/299972 [==============================] - 11s 36us/step - loss: 1.0343 - acc: 0.5002\n",
      "Epoch 4/20\n",
      "299972/299972 [==============================] - 11s 36us/step - loss: 1.0097 - acc: 0.5152\n",
      "Epoch 5/20\n",
      "299972/299972 [==============================] - 11s 36us/step - loss: 0.9894 - acc: 0.5279\n",
      "Epoch 6/20\n",
      "299972/299972 [==============================] - 11s 36us/step - loss: 0.9707 - acc: 0.5393\n",
      "Epoch 7/20\n",
      "299972/299972 [==============================] - 11s 36us/step - loss: 0.9528 - acc: 0.5502\n",
      "Epoch 8/20\n",
      "299972/299972 [==============================] - 11s 36us/step - loss: 0.9344 - acc: 0.5615\n",
      "Epoch 9/20\n",
      "299972/299972 [==============================] - 11s 36us/step - loss: 0.9176 - acc: 0.5728\n",
      "Epoch 10/20\n",
      "299972/299972 [==============================] - 11s 36us/step - loss: 0.9008 - acc: 0.5810\n",
      "Epoch 11/20\n",
      "299972/299972 [==============================] - 11s 36us/step - loss: 0.8859 - acc: 0.5907\n",
      "Epoch 12/20\n",
      "299972/299972 [==============================] - 11s 36us/step - loss: 0.8702 - acc: 0.5990\n",
      "Epoch 13/20\n",
      "299972/299972 [==============================] - 11s 36us/step - loss: 0.8564 - acc: 0.6053\n",
      "Epoch 14/20\n",
      "299972/299972 [==============================] - 11s 36us/step - loss: 0.8404 - acc: 0.6139\n",
      "Epoch 15/20\n",
      "299972/299972 [==============================] - 11s 36us/step - loss: 0.8287 - acc: 0.6205\n",
      "Epoch 16/20\n",
      "299972/299972 [==============================] - 11s 36us/step - loss: 0.8149 - acc: 0.6285\n",
      "Epoch 17/20\n",
      "299972/299972 [==============================] - 11s 36us/step - loss: 0.8027 - acc: 0.6337\n",
      "Epoch 18/20\n",
      "299972/299972 [==============================] - 11s 36us/step - loss: 0.7907 - acc: 0.6398\n",
      "Epoch 19/20\n",
      "299972/299972 [==============================] - 11s 36us/step - loss: 0.7788 - acc: 0.6461\n",
      "Epoch 20/20\n",
      "299972/299972 [==============================] - 11s 36us/step - loss: 0.7673 - acc: 0.6513\n"
     ]
    }
   ],
   "source": [
    "first_train('1024_2',10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded expectmax lib for 2048: /home/ubuntu/2048-api/game2048/expectimax/bin/2048.so\n",
      "24 85\n",
      "Epoch 1/1\n",
      "6422/6422 [==============================] - 1s 195us/step - loss: 1.1387 - acc: 0.4998\n",
      "16 92\n",
      "Epoch 1/1\n",
      "6417/6417 [==============================] - 1s 175us/step - loss: 1.1501 - acc: 0.4982\n",
      "17 89\n",
      "Epoch 1/1\n",
      "6427/6427 [==============================] - 1s 178us/step - loss: 1.1127 - acc: 0.4985\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-f6cf8f2773db>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0msecond_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'1024_1'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m512\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1024\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m256\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m256\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-16-1c55cc7a138e>\u001b[0m in \u001b[0;36msecond_train\u001b[0;34m(s, cand, s1, e1, s2, e2, s3, e3)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ma1\u001b[0m\u001b[0;34m>=\u001b[0m\u001b[0ms1\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0ma1\u001b[0m\u001b[0;34m<\u001b[0m\u001b[0me1\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0ma2\u001b[0m\u001b[0;34m>=\u001b[0m\u001b[0ms2\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0ma2\u001b[0m\u001b[0;34m<\u001b[0m\u001b[0me2\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0ma3\u001b[0m\u001b[0;34m>=\u001b[0m\u001b[0ms3\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0ma3\u001b[0m\u001b[0;34m<\u001b[0m\u001b[0me3\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m                 \u001b[0mchoice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmove\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtmp\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m                 \u001b[0mtmp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtmp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m                 \u001b[0mx_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtmp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-16-1c55cc7a138e>\u001b[0m in \u001b[0;36mmove\u001b[0;34m(model, tm, cand)\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mtmp_list\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mtmp_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtmp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0mtmp_list\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtmp_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m     \u001b[0mtmp_pre\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtmp_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mdirection\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtmp_pre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtmp_pre\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps)\u001b[0m\n\u001b[1;32m   1167\u001b[0m                                             \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1168\u001b[0m                                             \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1169\u001b[0;31m                                             steps=steps)\n\u001b[0m\u001b[1;32m   1170\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1171\u001b[0m     def train_on_batch(self, x, y,\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mpredict_loop\u001b[0;34m(model, f, ins, batch_size, verbose, steps)\u001b[0m\n\u001b[1;32m    292\u001b[0m                 \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    293\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 294\u001b[0;31m             \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    295\u001b[0m             \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    296\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbatch_index\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "while 1:\n",
    "    second_train('1024_1',10,512,1024,0,256,0,256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_tensorflow_p36)",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
